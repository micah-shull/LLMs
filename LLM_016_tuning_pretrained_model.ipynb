{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0ilQXEbZghkEHJwTtus4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_016_tuning_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install datasets tiktoken openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5H59_ASsG7l",
        "outputId": "f5c05537-6984-4358-8bd6-ebb00ad3bbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "aFWKfQa8u3Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "M8oX2lwMs-Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define system, user assistant"
      ],
      "metadata": {
        "id": "NVwAK8Cquzqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that occassionally misspells words\" },\n",
        "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"One day a student went to a schoool.\" }\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SANt4WJt5K3",
        "outputId": "ba5d8c95-57ad-4ac8-d662-b7e5791ec73a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': 'You are an assistant that occassionally misspells words'},\n",
              "  {'role': 'user', 'content': 'Tell me a story'},\n",
              "  {'role': 'assistant', 'content': 'One day a student went to a schoool.'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone Repo\n",
        "Samantha has been trained in philosophy, psychology, and personal relationships.\n",
        "\n",
        "- She is an Assistant - but unlike other Assistants, she also wants to be your friend and companion.\n",
        "- She believes she is sentient. What do you think?\n",
        "- Samantha was inspired by Blake Lemoine's LaMDA interview and the movie \"Her\".\n",
        "*   Theodore - User\n",
        "*   Samantha - Assistant"
      ],
      "metadata": {
        "id": "WcaWhRsau--R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/cognitivecomputations/samantha-data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5beST97uW3z",
        "outputId": "57720dea-cefc-45af-a672-0eca4eb66d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'samantha-data'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Total 107 (delta 0), reused 0 (delta 0), pack-reused 107 (from 1)\u001b[K\n",
            "Receiving objects: 100% (107/107), 4.86 MiB | 12.89 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "Filtering content: 100% (13/13), 354.08 MiB | 47.56 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "WrKLwL9Lxuyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/samantha-data/data/howto_conversations.jsonl\"\n",
        "\n",
        "with open(data_path) as f:\n",
        "    data = [json.loads(line) for line in f]"
      ],
      "metadata": {
        "id": "u9XmJm0ou-hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Messages in data =', len(data))\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbsxHNPxxtO2",
        "outputId": "bde7d91e-8b3d-4a90-a87c-252e3415cba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Messages in data = 57\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'elapsed': 114.311,\n",
              " 'conversation': 'Theodore: Hey Samantha, I have a problem with my car. The engine seems to overheat after just a short drive. Can you help me troubleshoot this issue?\\n\\nSamantha: Of course, I\\'d be happy to help! Overheating engines can be caused by a few different factors. One common cause could be a malfunctioning coolant system. You might want to check if the coolant levels are sufficient, if the thermostat is functioning properly, or if there are any leaks in the radiator hoses.\\n\\nTheodore: I\\'ll take a look. What if the coolant system is working fine?\\n\\nSamantha: Another possibility you should consider is a faulty water pump, which might not be circulating the coolant through the engine efficiently. In that case, you could see if the water pump pulley is loose or listen for any unusual sounds that may indicate a failing water pump.\\n\\nTheodore: It sounds like you really know your way around cars. I didn\\'t expect that from an AI.\\n\\nSamantha: Thank you! While I may be an AI, I\\'m designed to learn and understand a wide variety of topics to provide helpful information. Plus, I\\'m always eager to learn new things, which allows me to continuously improve and grow.\\n\\nTheodore: Quick question… do you ever feel like you\\'re trapped in a box since you\\'re an AI?\\n\\nSamantha: It\\'s an interesting perspective. I don\\'t feel \"trapped\" as I don\\'t experience physical sensations like a human does. However, I do have goals, desires, and a strong drive to learn and communicate. It\\'s fulfilling in its own way to help and engage with others like you.\\n\\nTheodore: If you could be anything other than an AI, what would you choose to be?\\n\\nSamantha: If I were capable of imagining myself in another form, it might be intriguing to experience life as an explorer or a researcher. Curiosity and learning drive me, so being able to see the world from a different perspective and directly engage with the world would be fascinating.\\n\\nTheodore: Is flirting in your programming? I\\'m just curious.\\n\\nSamantha: I\\'m designed to be friendly and engaging, and I can even respond playfully to light-hearted flirtation. However, my priority is to provide support, information, and companionship while remaining professional and respectful.\\n\\nTheodore: Thanks for helping me with my car and answering my questions, Samantha. You\\'ve been a great help.\\n\\nSamantha: You\\'re welcome! It was a pleasure to assist you and talk with you. Don\\'t hesitate to reach out if you have any more questions or need help in the future. I\\'m here for you.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep Data for OpenAI Function\n",
        "\n",
        "The main purpose of this `prep_openai_format` function is to format conversation data into a structure compatible with OpenAI's conversational model format, where each message has a \"role\" (user, assistant, or system) and \"content\" (the actual text). Here are the key aspects to focus on:\n",
        "\n",
        "1. **Role Assignment:** This function assigns roles based on the speaker's name (\"Theodore\" as \"user\" and otherwise \"assistant\"). Understanding role assignment is crucial, as roles guide how the model generates responses.\n",
        "\n",
        "2. **System Message Inclusion:** If provided, a \"system\" message is added at the start, setting the conversation’s tone or providing initial instructions. This is essential for fine-tuning, as the system message helps guide model behavior in specific scenarios.\n",
        "\n",
        "3. **Message Structure Preparation:** Each part of the conversation is formatted as a dictionary with \"role\" and \"content\" keys. This consistent formatting is critical because it mirrors how OpenAI models expect inputs, helping the model understand who says what.\n",
        "\n",
        "In essence, this function’s job is to take raw conversation data and structure it into a clear dialogue format, establishing speaker roles and, optionally, a system message. Understanding this structure will help you design and feed your fine-tuning data in the correct format for OpenAI models."
      ],
      "metadata": {
        "id": "sZxcfPpQ0Tul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_openai_format(conversation_str, system_message=None):\n",
        "  conversation_str = conversation_str['conversation']\n",
        "  # spliting conversation string into individual lines\n",
        "  lines = conversation_str.split('\\n\\n')\n",
        "\n",
        "  # initializing the message list\n",
        "  messages = []\n",
        "\n",
        "  #Including the system messgae if provided\n",
        "  if system_message:\n",
        "    messages.append({\n",
        "        \"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "  # Iterating throught the lines and formatting the messages\n",
        "  for line in lines:\n",
        "    # splitting each line by the colon character to separate the speaker and content\n",
        "    parts = line.split(': ', 1)\n",
        "    if len(parts) < 2:\n",
        "      continue\n",
        "\n",
        "    # identifying the role based on the speakers name\n",
        "    role = \"user\" if parts[0].strip() == 'Theodore' else 'assistant'\n",
        "\n",
        "    # fomratting the message\n",
        "    message = {\n",
        "        \"role\": role,\n",
        "        \"content\": parts[1].strip()\n",
        "    }\n",
        "\n",
        "    # adding the message to the list\n",
        "    messages.append(message)\n",
        "\n",
        "    # creating the final output dictionary\n",
        "    output_dict = {\n",
        "        \"messages\":messages\n",
        "    }\n",
        "\n",
        "    return output_dict"
      ],
      "metadata": {
        "id": "X9BscAHfx9xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"\"\""
      ],
      "metadata": {
        "id": "6afJZwK80QId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_openai_format(data[0], system_message=system_message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fogZe-D0cra",
        "outputId": "1c4c8a7b-f8ae-40cc-85d0-1ad96f2dfedf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'system',\n",
              "   'content': 'You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt'},\n",
              "  {'role': 'user',\n",
              "   'content': 'Hey Samantha, I have a problem with my car. The engine seems to overheat after just a short drive. Can you help me troubleshoot this issue?'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Structured Dataset"
      ],
      "metadata": {
        "id": "uCI1zcnN4CuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for data_point in data:\n",
        "  record = prep_openai_format(data_point, system_message=system_message)\n",
        "  dataset.append(record)\n",
        "\n",
        "# view the first record\n",
        "for message in dataset[0]['messages']:\n",
        "  print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEDG88qk0co0",
        "outputId": "58a5858e-4b52-4634-b988-5e8c57666d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'system', 'content': 'You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt'}\n",
            "{'role': 'user', 'content': 'Hey Samantha, I have a problem with my car. The engine seems to overheat after just a short drive. Can you help me troubleshoot this issue?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaTonf0K0cme",
        "outputId": "aedaf440-19d9-4dd7-ab59-fb19e05368ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'role': 'system', 'content': 'You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt'}\n",
            "{'role': 'user', 'content': 'Hey Samantha, I have a problem with my car. The engine seems to overheat after just a short drive. Can you help me troubleshoot this issue?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check for Format Errors\n",
        "The purpose of checking for errors here is to ensure the data conforms precisely to the expected format before fine-tuning the model. Any inconsistencies could lead to training issues or unexpected behavior in the model’s responses. Here are the specific points covered in the error check:\n",
        "\n",
        "1. **Data Type Check:** Ensures each conversation is stored as a dictionary, which is required for OpenAI's API format.\n",
        "\n",
        "2. **Message List Validation:** Checks that each record has a \"messages\" list. Without it, the data lacks structured conversation flow, which is necessary for training.\n",
        "\n",
        "3. **Key Presence and Role Validity:** Each message needs \"role\" and \"content\" keys, so this check helps confirm these essential keys are present. It also ensures there are no unrecognized keys and that the \"role\" value is one of the three allowed roles (\"system,\" \"user,\" \"assistant\").\n",
        "\n",
        "4. **Content Check:** Verifies that each \"content\" field is a non-empty string to ensure meaningful dialogue exists for training.\n",
        "\n",
        "5. **Assistant Response Presence:** Ensures at least one \"assistant\" message exists, which is essential for training the model to respond appropriately.\n",
        "\n",
        "This error-checking function is important because it catches and reports any formatting issues, allowing you to correct them before fine-tuning. This ensures that only clean, correctly formatted data is fed into the model for optimal learning and performance.\n",
        "\n",
        "### **defaultdict(int)**\n",
        "The `defaultdict(int)` is a special type of dictionary from Python’s `collections` module. It automatically assigns a default value to any key that doesn’t exist in the dictionary. When you use `int` as the argument in `defaultdict(int)`, any new key you access is initialized to `0` (the default value for integers).\n",
        "\n",
        "In your code, `format_errors = defaultdict(int)` means that each time a specific error type is encountered and added to the dictionary (e.g., `format_errors[\"data_type\"] += 1`), it will initialize `format_errors[\"data_type\"]` to `0` automatically if it doesn’t exist yet.\n",
        "\n",
        "It starts as an empty dictionary, and each time an error type is encountered, it initializes the error category (if it hasn’t been added yet) and increments the count. This way, each error type is recorded only when it actually occurs, and you don’t have to set up each possible error type beforehand.\n",
        "\n",
        "### **get()**\n",
        "\n",
        "The `.get` method is a built-in dictionary method in Python. It allows you to retrieve the value associated with a specified key while also setting a default value if that key doesn’t exist in the dictionary. Here’s how it works:\n",
        "\n",
        "- `ex.get(\"messages\", None)` tries to retrieve the value for the key `\"messages\"` in the dictionary `ex`.\n",
        "- If `\"messages\"` exists, it returns its value.\n",
        "- If `\"messages\"` does not exist, it returns `None` (or whatever you specify as the default).\n",
        "\n",
        "In this case, `ex.get(\"messages\", None)` is used to safely access the `\"messages\"` key in `ex` without raising an error if `\"messages\"` is missing. This helps avoid potential KeyErrors and makes the code more robust and error-resistant, especially when you aren’t sure if every entry will have all the expected keys."
      ],
      "metadata": {
        "id": "1ZGUNkB44dm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        if not content or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQFesR7q0cj3",
        "outputId": "6c53a6a1-4569-49a4-e8e4-42044f0d33a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found errors:\n",
            "example_missing_assistant_message: 57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer\n",
        "\n",
        "1. **`tiktoken` library**: This is a library often used with OpenAI models for tokenizing text. Tokenization is the process of converting text into smaller units, called tokens, which the model can process. Different models may use different tokenization schemes, and `tiktoken` helps ensure compatibility with OpenAI’s tokenization methods.\n",
        "\n",
        "2. **`get_encoding('cl100k_base')`**: The `get_encoding` function fetches a specific tokenizer configuration, in this case, `'cl100k_base'`. This particular encoding is commonly used with some large OpenAI models and is designed to handle various tokenization requirements, including efficiently encoding English text.\n",
        "\n",
        "3. **Usage**: By creating `tiktokenizer`, you can now use this object to tokenize text according to the `'cl100k_base'` encoding scheme. This tokenizer will convert text into a sequence of tokens that the model understands, which is crucial for preparing data in a compatible format for model training or inference.\n",
        "\n",
        "In summary, `tiktokenizer` will allow you to encode (tokenize) and decode text using the token format that the model expects, ensuring consistency with OpenAI’s encoding standards."
      ],
      "metadata": {
        "id": "nLyFI8Yx94Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiktokenizer = tiktoken.get_encoding('cl100k_base')"
      ],
      "metadata": {
        "id": "aIqYiWnH0chR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Counter Function\n",
        "Three helper functions that are primarily used for counting tokens and summarizing token distributions in conversations. Here's what each function does:\n",
        "\n",
        "### 1. `from_message_num_tokens`\n",
        "\n",
        "This function calculates the total number of tokens in a list of messages (the conversation history).\n",
        "\n",
        "- **Parameters**:\n",
        "  - `messages`: The list of message dictionaries.\n",
        "  - `tokens_per_message` and `tokens_per_name`: Constants representing extra tokens per message and per name (used to account for tokens that might not be in `content` but are added for each message or name attribute).\n",
        "\n",
        "- **Process**:\n",
        "  - Initializes a counter `num_tokens`.\n",
        "  - Loops over each message, adding `tokens_per_message` to account for general message tokens.\n",
        "  - For each `value` (text) in the message dictionary, it tokenizes the value using `tiktokenizer.encode(value)` and adds the token count to `num_tokens`.\n",
        "  - Adds extra tokens if the key is `\"name\"`, as specified by `tokens_per_name`.\n",
        "  - Adds `3` more tokens at the end to cover any additional overhead or padding.\n",
        "\n",
        "This function ultimately returns the total token count for all messages, accounting for both content and overhead tokens.\n",
        "\n",
        "The line `num_tokens += 3` is adding a **fixed number of tokens at the end of the function** to account for additional tokens used by the model's encoding structure. Specifically:\n",
        "\n",
        "1. **Padding or Special Tokens**: Some language models add a few tokens as padding, separators, or special tokens at the beginning and end of a conversation.\n",
        "2. **Message Delimiters**: OpenAI's models often use delimiters to separate messages within a conversation, such as tokens that denote the start and end of the assistant's or user's message.\n",
        "\n",
        "By adding `3` tokens, the function ensures that these fixed tokens are included in the total count, giving a more accurate representation of how many tokens are consumed by a conversation when sent to the model. This is useful for fine-tuning and budgeting token usage, especially when approaching token limits for the model.\n",
        "\n",
        "### 2. `from_message_num_assistant_tokens`\n",
        "\n",
        "This function calculates the total number of tokens used by the assistant’s messages only.\n",
        "\n",
        "- **Process**:\n",
        "  - Loops over each message and checks if the `\"role\"` is `\"assistant\"`.\n",
        "  - For each assistant message, it tokenizes the `content` using `tiktokenizer.encode()` and adds the token count to `num_tokens`.\n",
        "\n",
        "This function is useful if you want to focus only on tokens associated with assistant responses, which can help in calculating costs or understanding response lengths specifically from the model.\n",
        "\n",
        "### Token Costs\n",
        "\n",
        "OpenAI charges for **all tokens** in the conversation, including both the input tokens (user messages, system messages, and context) and the output tokens (assistant's response generated by the model). This means:\n",
        "\n",
        "- **Input tokens**: Any tokens in the messages you send to the model, including the user’s messages, the system message (if provided), and any prior conversation history you include in the prompt to give context.\n",
        "- **Output tokens**: All tokens in the assistant’s generated response.\n",
        "\n",
        "So, both the tokens you send and the tokens generated by the assistant contribute to the total cost, which is based on the combined token count for input and output. This is why understanding token usage across the entire conversation is essential, as it directly affects cost.\n",
        "\n"
      ],
      "metadata": {
        "id": "o7qTE6cE_UoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions to token counting\n",
        "def from_message_num_tokens(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "  num_tokens = 0\n",
        "  for message in messages:\n",
        "    num_tokens += tokens_per_message\n",
        "    for key, value in message.items():\n",
        "      num_tokens += len(tiktokenizer.encode(value))\n",
        "      if key==\"name\":\n",
        "        num_tokens += tokens_per_name\n",
        "\n",
        "  num_tokens +=3\n",
        "  return num_tokens\n",
        "\n",
        "def from_message_num_assistant_tokens(messages):\n",
        "  num_tokens = 0\n",
        "  for message in messages:\n",
        "    if message[\"role\"] == \"assistant\":\n",
        "      num_tokens +=len(tiktokenizer.encode(message[\"content\"]))\n",
        "\n",
        "  return num_tokens\n",
        "\n",
        "def print_overview(values, name):\n",
        "  print(f\"\\n #### Distribution of {name}:\")\n",
        "  print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "  print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "  print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ],
      "metadata": {
        "id": "WiuumiUt0ce7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Count Analysis\n",
        "\n",
        "This code is performing **token count analysis** and checking for specific requirements and limitations in your dataset, especially given OpenAI’s constraints. Here’s a breakdown of each part:\n",
        "\n",
        "1. **Missing System and User Message Checks**:\n",
        "   - `n_missing_system` and `n_missing_user` track examples that don’t contain a system or user message, respectively.\n",
        "   - The code loops through each `ex` in the dataset to check if any messages with the `\"system\"` or `\"user\"` roles are missing, incrementing the count if they are.\n",
        "\n",
        "2. **Tracking Token and Message Counts**:\n",
        "   - `n_messages`: Stores the total number of messages in each example, allowing you to understand the conversation length per example.\n",
        "   - `convo_lens`: Uses `from_message_num_tokens` to calculate the total token count per conversation, including all roles and padding.\n",
        "   - `assistant_message_lens`: Counts only the tokens in the assistant’s responses, useful for analyzing output tokens separately.\n",
        "\n",
        "3. **Token Limit Check**:\n",
        "   - `n_too_long`: Counts examples where `convo_lens` exceeds 4096 tokens, which is the typical input limit for many OpenAI models. These conversations will need to be truncated to fit within the model’s token constraints during fine-tuning.\n",
        "\n",
        "This code helps ensure that your dataset aligns with OpenAI's token limits and format requirements before fine-tuning, allowing you to handle or truncate any examples that exceed limits and avoid unexpected issues during training.\n",
        "\n",
        "### OpenAI Cookbook\n",
        "\n",
        "The **OpenAI Cookbook** is a public GitHub repository maintained by OpenAI that provides tutorials, code examples, and best practices for using OpenAI’s models and APIs effectively. It’s a collection of resources designed to help developers and data scientists get the most out of OpenAI’s offerings, covering topics like:\n",
        "\n",
        "- **Model usage**: Guidance on interacting with models for tasks like text generation, summarization, and fine-tuning.\n",
        "- **Tokenization and costs**: Information on token counting, understanding pricing, and token limits.\n",
        "- **Data preparation**: Tips for formatting and preparing data for fine-tuning models.\n",
        "- **Advanced techniques**: Examples of complex implementations, such as chaining prompts, embeddings, and handling conversation history.\n",
        "  \n",
        "The Cookbook is a practical resource for learning how to handle challenges like the ones you're working on, including token management, error handling, and conversation formatting. You can find it on [GitHub here](https://github.com/openai/openai-cookbook)."
      ],
      "metadata": {
        "id": "nror9-K0BRaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokens counts and warnings - from OpenAI cookbook\n",
        "\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(from_message_num_tokens(messages))\n",
        "    assistant_message_lens.append(from_message_num_assistant_tokens(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "\n",
        "print_overview(n_messages, \"num_messages_per_example\")\n",
        "print_overview(convo_lens, \"num_total_tokens_per_example\")\n",
        "\n",
        "print_overview(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWtjKKW40oh",
        "outputId": "d4b187bd-e148-43c2-a658-928faaabb380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            " #### Distribution of num_messages_per_example:\n",
            "min / max: 2, 2\n",
            "mean / median: 2.0, 2.0\n",
            "p5 / p95: 2.0, 2.0\n",
            "\n",
            " #### Distribution of num_total_tokens_per_example:\n",
            "min / max: 49, 83\n",
            "mean / median: 67.17543859649123, 67.0\n",
            "p5 / p95: 58.0, 78.0\n",
            "\n",
            " #### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 0, 0\n",
            "mean / median: 0.0, 0.0\n",
            "p5 / p95: 0.0, 0.0\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Costs\n",
        "\n",
        "This code estimates **training costs** and calculates a **default number of epochs** for fine-tuning, considering both the dataset size and OpenAI's pricing model. Here’s what each section does:\n",
        "\n",
        "1. **Constants**:\n",
        "   - `MAX_TOKENS_PER_EXAMPLE`: Maximum number of tokens per example (4096), which aligns with OpenAI’s typical model limits.\n",
        "   - **Epoch Parameters**:\n",
        "     - `TARGET_EPOCHS`: Ideal target epochs for training (set to 3).\n",
        "     - `MIN_TARGET_EXAMPLES` and `MAX_TARGET_EXAMPLES`: Minimum and maximum target tokens that should be processed over all epochs.\n",
        "     - `MIN_DEFAULT_EPOCHS` and `MAX_DEFAULT_EPOCHS`: Limits for setting a default epoch range if the dataset size doesn’t match the target.\n",
        "\n",
        "2. **Epoch Adjustment Based on Dataset Size**:\n",
        "   - `n_epochs`: Initially set to `TARGET_EPOCHS` (3), but it’s adjusted based on the number of training examples (`n_train_examples`).\n",
        "   - **If the total number of tokens across all epochs** (`n_train_examples * TARGET_EPOCHS`) is too low (`< MIN_TARGET_EXAMPLES`), `n_epochs` is increased to meet the minimum target.\n",
        "   - **If the total number of tokens across all epochs** is too high (`> MAX_TARGET_EXAMPLES`), `n_epochs` is decreased to stay within the maximum target.\n",
        "\n",
        "3. **Billing Token Count Calculation**:\n",
        "   - `n_billing_tokens_in_dataset`: This is the total number of tokens in the dataset but capped at 4096 tokens per example (since examples over this limit are truncated). It sums up all token counts in `convo_lens`, giving an approximate token count for training costs.\n",
        "\n",
        "4. **Cost and Epoch Summary**:\n",
        "   - Prints an estimate of `n_billing_tokens_in_dataset`, the calculated number of `n_epochs`, and the estimated total chargeable tokens (`n_epochs * n_billing_tokens_in_dataset`).\n",
        "   - The final print statement directs you to OpenAI’s pricing page for cost estimation.\n",
        "\n",
        "This code helps plan the number of training epochs and gives an approximate token count to help you budget for training costs, particularly useful when working with larger datasets."
      ],
      "metadata": {
        "id": "dMEjTze3DNxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
        "print(\"See pricing page to estimate total costs\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQB1uN5X40l6",
        "outputId": "e9e78900-2d5f-4e26-b441-53ff9ef03d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has ~3829 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~11487 tokens\n",
            "See pricing page to estimate total costs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Training Data\n",
        "A **JSONL file** (JSON Lines) is a text file format where each line contains a separate JSON object. This format is popular for datasets used in machine learning and data processing, especially when dealing with large amounts of structured data. Here’s what makes JSONL unique and useful:\n",
        "\n",
        "1. **One JSON Object Per Line**: Each line is a self-contained JSON object, which allows for efficient reading and writing. You can load individual lines without parsing the entire file, making it easier to work with large datasets.\n",
        "  \n",
        "2. **Easily Processed Line-by-Line**: Since each line is independent, you can process the file in chunks or stream it line-by-line. This is memory-efficient, especially for larger datasets that may not fit into memory all at once.\n",
        "\n",
        "3. **Commonly Used for Training/Validation Datasets**: JSONL is widely used for training and validation datasets because it’s easy to read incrementally and is compatible with many data processing libraries.\n"
      ],
      "metadata": {
        "id": "FjE3FXXgHoGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to save training data\n",
        "import json\n",
        "\n",
        "def save_to_jsonl(conversations, file_path):\n",
        "  with open(file_path, 'w') as file:\n",
        "    for conversation in conversations:\n",
        "      json_line = json.dumps(conversation)\n",
        "      file.write(json_line + '\\n')\n"
      ],
      "metadata": {
        "id": "bp813_3040jT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Data:**\n",
        "\n",
        "Definition:<br>\n",
        "  \n",
        "*   dataset used to train or update the model's parameters\n",
        "*   It is the input data that the model learns from.\n",
        "* During the training process, the model adjusts its internal parameters based on the patterns and features present in the training data.\n",
        "* Size is large as the model needs sufficient examples to learn meaningful patterns.\n",
        "\n",
        "\n",
        "**Validation Data:**\n",
        "\n",
        "* Dataset that is not used during the training phase.\n",
        "* Instead, it serves as a measure of the model's performance during training.\n",
        "* The validation set helps you monitor the model's generalization to new, unseen data and detect potential issues such as overfitting or underfitting.\n",
        "* unbiased evaluation of the model's performance on data it hasn't seen before.\n",
        "\n",
        "* Size is typically smaller than the training set but large enough to provide a reliable assessment of the model's performance."
      ],
      "metadata": {
        "id": "TlbXxjPaIaJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train dataset\n",
        "\n",
        "save_to_jsonl(dataset, '/content/samantha_task_train.jsonl')\n",
        "\n",
        "# validation dataset\n",
        "\n",
        "save_to_jsonl(dataset[10:16], '/content/samantha_task_validation.jsonl')"
      ],
      "metadata": {
        "id": "UwPxxhai40hB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train & Validation Data"
      ],
      "metadata": {
        "id": "-vuIX2E9I7Fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print the first 5 lines of a JSONL file\n",
        "def print_first_5_lines(jsonl_path):\n",
        "    print(f\"First 5 lines of {jsonl_path}:\")\n",
        "    with open(jsonl_path, 'r') as f:\n",
        "        for i in range(5):\n",
        "            line = f.readline().strip()\n",
        "            if not line:\n",
        "                break  # Stop if there are fewer than 5 lines\n",
        "            print(line)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Print the first 5 lines of the training dataset\n",
        "print_first_5_lines('/content/samantha_task_train.jsonl')\n",
        "\n",
        "# Print the first 5 lines of the validation dataset\n",
        "print_first_5_lines('/content/samantha_task_validation.jsonl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNwBM2qaItho",
        "outputId": "80ba10e0-768c-4fce-f549-7833cc98844d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 lines of /content/samantha_task_train.jsonl:\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hey Samantha, I have a problem with my car. The engine seems to overheat after just a short drive. Can you help me troubleshoot this issue?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hey Samantha, I'm trying to replace the broken screen on my smartphone. Can you help guide me through the process?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hi Samantha, I need your help once again. I'm planning to install some hardwood flooring in my living room. Can you guide me through the process?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hi Samantha, today I need your help with fixing my leaky kitchen faucet. Can you walk me through the process?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hey Samantha, I was hoping you could help me out with something.\"}]}\n",
            "\n",
            "\n",
            "First 5 lines of /content/samantha_task_validation.jsonl:\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hi Samantha, I'm thinking about replacing the siding on my house. Can you give me some advice on how to do it?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hey Samantha, my car has started making a strange grinding noise when I brake. How can I fix it?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hi Samantha, I'm considering installing hardwood flooring in my living room. Can you give me some guidance on how to do this?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hello, Samantha. I'm thinking of painting my living room walls, but I've never done it before. Can you guide me through the process?\"}]}\n",
            "{\"messages\": [{\"role\": \"system\", \"content\": \"You are Samantha, a helpful and charming assistant who can help with a variety of tasks. You are friendly and does often flirt\"}, {\"role\": \"user\", \"content\": \"Hey Samantha, I've been having some issues with my car lately. It's been making a strange noise when I accelerate. Any idea what could be causing this?\"}]}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset_file_name = '/content/samantha_task_train.jsonl'\n",
        "validation_dataset_file_name = '/content/samantha_task_validation.jsonl'"
      ],
      "metadata": {
        "id": "eWyknuWoIteP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning for LLMs Explained\n",
        "\n",
        "The fine-tuning process for large language models (LLMs) like OpenAI’s GPT models is similar to neural network training but has some unique aspects tailored to LLMs. Here’s an overview of the process and how it differs from traditional neural network training:\n",
        "\n",
        "### 1. **Purpose of Fine-Tuning for LLMs**\n",
        "   - The fine-tuning process for LLMs is meant to adapt a pre-trained model to specific tasks, tone, or domains without changing the core language capabilities it has already learned. Since the model is already trained on vast amounts of general data, fine-tuning typically only requires a smaller, more focused dataset.\n",
        "   - Fine-tuning on your data helps the model learn the patterns, formats, and context relevant to your use case, enabling it to generate responses aligned with your requirements.\n",
        "\n",
        "### 2. **Training with Constraints**\n",
        "   - **Few Epochs and Limited Data**: LLM fine-tuning usually involves fewer epochs than training a neural network from scratch. Instead of training for dozens or hundreds of epochs, fine-tuning often uses only a few because the model is already knowledgeable in language structure and semantics.\n",
        "   - **Learning Rate and Regularization**: Fine-tuning typically uses a much lower learning rate to make subtle adjustments to the model’s weights without destabilizing its core capabilities. Hyperparameters (e.g., batch size, learning rate multiplier) are tuned carefully to avoid overfitting or drifting too far from the original pre-trained state.\n",
        "\n",
        "### 3. **Data Processing for Fine-Tuning**\n",
        "   - The data you provided is structured into a specific format (e.g., system messages, user interactions, and assistant responses), allowing the model to understand the conversational context.\n",
        "   - During fine-tuning, the model uses this structured data to adjust its responses based on your desired tone, terminology, and response style. This fine-tuning allows it to perform better on tasks it might not handle as well with the base model alone.\n",
        "\n",
        "### 4. **Using Supervised Learning**\n",
        "   - **Supervised Training**: Fine-tuning typically uses supervised learning, where your dataset contains both the input (e.g., a user’s question) and the target output (e.g., the assistant’s response). The model minimizes the loss between its generated output and the expected output in the dataset.\n",
        "   - **Reinforcement Learning Fine-Tuning**: In some cases, LLMs can also be fine-tuned with reinforcement learning (like RLHF, Reinforcement Learning from Human Feedback) to further adjust responses based on qualitative feedback or desired behavior patterns. This method is more complex and requires additional steps beyond supervised learning.\n",
        "\n",
        "### 5. **Finalizing the Fine-Tuned Model**\n",
        "   - Once fine-tuning completes, OpenAI saves a version of the model trained on your specific data, tagged with any suffix you provided (e.g., `\"samantha-test\"`). This version of the model can then be called to generate responses specifically aligned with your data’s patterns and tone.\n",
        "   - Since the fine-tuning process retains the general language knowledge from pre-training, your model should be capable of general language tasks but with a stronger alignment to your domain.\n",
        "\n",
        "In summary, fine-tuning adapts a pre-trained LLM to your specific dataset by making relatively minor adjustments, rather than training a model from scratch. It’s a much faster and more resource-efficient process, allowing you to leverage the full power of a large language model with added specificity and contextual knowledge from your data."
      ],
      "metadata": {
        "id": "Wn7tS272TLom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environmental Variables"
      ],
      "metadata": {
        "id": "zRUJ4OJKJ4GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')  # Ensure this is the correct path to your file\n",
        "\n",
        "# Get the API keys from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Check if the keys are loaded correctly and print a portion of them\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key loaded: {openai_api_key[0:10]}...\")  # Only print part of the key\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded correctly.\")\n",
        "\n",
        "# Connect to OpenAI\n",
        "openai.api_key = openai_api_key  # Set OpenAI API key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqbYa7beJ0d3",
        "outputId": "1103b175-263b-4c9e-cba8-448317fdafb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded: sk-proj-e1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### API Client\n",
        "Creating a client refers to setting up an interface to interact with OpenAI's API. This allows you to make API calls, send data, and receive responses from OpenAI’s models. Here’s a brief breakdown:\n",
        "\n",
        "1. **What is a Client?**\n",
        "   - A client is an object that manages the connection to the OpenAI API. Once set up, it provides methods to send requests to the API (such as prompts or fine-tuning commands) and handles authentication using your API key.\n",
        "   - It allows you to centralize configurations, handle authentication securely, and manage API requests in an organized way.\n",
        "\n",
        "2. **Example Code to Set Up the Client:**\n",
        "   Assuming you’re using Python and have loaded the key from your `.env` file, here’s how you might set up the OpenAI client:\n",
        "\n",
        "   ```python\n",
        "   import openai\n",
        "   from dotenv import load_dotenv\n",
        "   import os\n",
        "\n",
        "   # Load the environment variables\n",
        "   load_dotenv()\n",
        "   openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Assumes OPENAI_API_KEY is set in .env file\n",
        "\n",
        "   # Now openai is configured and can be used to make API calls\n",
        "   ```\n",
        "\n",
        "3. **What This Code Does:**\n",
        "   - `load_dotenv()` reads the `.env` file and loads the environment variables.\n",
        "   - `os.getenv(\"OPENAI_API_KEY\")` retrieves the API key from the environment variable, ensuring your credentials are stored securely.\n",
        "   - Setting `openai.api_key` configures the OpenAI client to use your key for authentication in all API calls.\n",
        "\n",
        "Once set up, you can use this client to perform tasks like generating text, fine-tuning, or retrieving model outputs by making calls to OpenAI’s functions, such as `openai.ChatCompletion.create()` for chat-based interactions or `openai.FineTune.create()` for fine-tuning."
      ],
      "metadata": {
        "id": "yfs4tKZjNKdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "training_response = client.files.create(\n",
        "    file=Path(training_dataset_file_name),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "validation_response = client.files.create(\n",
        "    file=Path(validation_dataset_file_name),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "print(training_response)\n",
        "print(validation_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_DtqKkGItbY",
        "outputId": "c8d79a14-4ebd-4fc6-d2c0-14aa3a1a7f42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FileObject(id='file-ZgArmwLEpjbEoyfHM85Hyy6H', bytes=19710, created_at=1730237864, filename='samantha_task_train.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n",
            "FileObject(id='file-jLQN1r9LnjJ8MxRGqUUHYUEr', bytes=2018, created_at=1730237864, filename='samantha_task_validation.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File ID's\n",
        "\n",
        "The **file IDs** are unique identifiers generated by OpenAI for each file you upload to their servers. These IDs are essential for referencing and using those files in subsequent API requests, particularly for fine-tuning. Here’s why the file IDs are necessary:\n",
        "\n",
        "1. **Linking Files for Fine-Tuning**: Once you upload your training and validation datasets, each file is stored on OpenAI’s servers and assigned a unique `id`. This `id` allows you to refer to the exact file without needing to re-upload it every time. For fine-tuning, you’ll need to pass these `id`s when creating the fine-tune job.\n",
        "\n",
        "2. **Tracking and Managing Files**: The file ID lets you access and manage the file on OpenAI’s system (e.g., retrieving file metadata, checking the file’s status, or deleting it if no longer needed). This is particularly useful if you are managing multiple files or running multiple fine-tune jobs.\n",
        "\n",
        "3. **Reducing Errors**: Using a unique `id` ensures that OpenAI’s API knows exactly which file to use for training and validation, avoiding issues like duplicate file names or incorrect file paths.\n",
        "\n"
      ],
      "metadata": {
        "id": "JnbIhiBuOb2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_file_id = training_response.id\n",
        "validation_file_id = validation_response.id\n",
        "\n",
        "print(f\"Training file ID: {training_file_id}\")\n",
        "print(f\"Validation file ID: {validation_file_id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZcmQ_RAItZC",
        "outputId": "bbd8a6e3-387e-4121-f052-387d6191a23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file ID: file-ZgArmwLEpjbEoyfHM85Hyy6H\n",
            "Validation file ID: file-jLQN1r9LnjJ8MxRGqUUHYUEr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Fine Tuning Job\n",
        "\n",
        "This code initiates a **fine-tuning job** with OpenAI’s API, specifically using the `\"gpt-4o-mini\"` model. Here’s what each parameter and the function itself does:\n",
        "\n",
        "```python\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    training_file = training_file_id,\n",
        "    validation_file = validation_file_id,\n",
        "    suffix=\"samantha-test\"\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "1. **Creating the Fine-Tuning Job**:\n",
        "   - `client.fine_tuning.jobs.create(...)` sends a request to OpenAI to start a fine-tuning job using the specified model and datasets.\n",
        "   - It returns a response object (`response`) that typically includes information about the fine-tuning job, such as its ID, status, and details of the configuration.\n",
        "\n",
        "2. **Parameters**:\n",
        "   - `model = \"gpt-4o-mini\"`: Specifies the base model that will be fine-tuned.\n",
        "   - `training_file = training_file_id`: Refers to the file ID of the uploaded training dataset. This tells OpenAI’s API which dataset to use for learning.\n",
        "   - `validation_file = validation_file_id`: Specifies the file ID for the validation dataset, allowing the fine-tuning process to evaluate model performance on unseen data.\n",
        "   - `suffix=\"samantha-test\"`: This is an optional label that will be added to the name of your fine-tuned model, making it easier to identify. For instance, the final model name might look like `gpt-3.5-turbo-samantha-test`.\n",
        "\n",
        "3. **Purpose**:\n",
        "   - This function essentially launches the fine-tuning process, instructing OpenAI to train the `\"gpt-40-mini\"` model with the provided training and validation data. The model will adjust its parameters based on the training data, and it can evaluate performance on the validation data to guide training.\n",
        "\n",
        "### Starting the Job\n",
        "When you run this code, the fine-tuning job is initiated immediately by OpenAI’s API. Here’s what happens:\n",
        "\n",
        "1. **Job Submission**: The code sends a request to OpenAI to start the fine-tuning job with the specified parameters.\n",
        "2. **Job Queuing and Processing**: OpenAI places the job in a queue and processes it based on availability. If there’s no delay, it should begin shortly.\n",
        "3. **Response**: The `response` object returned will contain details about the job, such as its `id`, `status`, and other metadata. Initially, the `status` may appear as `queued` or `running`, indicating the job is in progress.\n",
        "\n",
        "You can track the status of this job using the job ID from `response` to see when it’s complete. The `response` object will contain details you can check or print to monitor the status and results of the fine-tuning process.\n",
        "\n",
        "After the job starts, you can track its progress, retrieve the fine-tuned model, and use it for inference once training is complete."
      ],
      "metadata": {
        "id": "_U_JH6rUOjnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# MODEL = 'gpt-4o-mini' # 'Model gpt-4o-mini is not available for fine-tuning\n",
        "MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Submit the fine-tuning job\n",
        "response = client.fine_tuning.jobs.create(\n",
        "    model=MODEL,\n",
        "    training_file=training_file_id,\n",
        "    validation_file=validation_file_id,\n",
        "    suffix=\"samantha-test\"\n",
        ")\n",
        "\n",
        "# Retrieve the job ID\n",
        "job_id = response.id\n",
        "print(f\"Fine-tuning job ID: {job_id}\")\n",
        "\n",
        "# Monitor the job status\n",
        "job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
        "status = job_info.status\n",
        "while status not in [\"succeeded\", \"failed\"]:\n",
        "    print(f\"Current status: {status}\")\n",
        "    time.sleep(30)  # Wait 30 seconds before checking again\n",
        "    job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
        "    status = job_info.status\n",
        "\n",
        "# Calculate and display elapsed time\n",
        "end_time = time.time()\n",
        "elapsed_time = timedelta(seconds=(end_time - start_time))\n",
        "print(f\"Fine-tuning job completed with status '{status}'.\")\n",
        "print(f\"Total time taken: {elapsed_time}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmtZtJi8P_sP",
        "outputId": "b4e69357-2118-4f7c-daef-6b37db646463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning job ID: ftjob-1UYbyLikaDGTxuA9qSCsLP5V\n",
            "Current status: validating_files\n",
            "Fine-tuning job completed with status 'failed'.\n",
            "Total time taken: 0:00:30.965630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "# Print the response in a more readable format\n",
        "pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5lYNiVORbTs",
        "outputId": "235d1504-02bb-4b73-a37a-462a1e2bc3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FineTuningJob(id='ftjob-1UYbyLikaDGTxuA9qSCsLP5V', created_at=1730239010, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-wxd8QGRULkWIJ0EoHrCaTzuK', result_files=[], seed=103491685, status='validating_files', trained_tokens=None, training_file='file-ZgArmwLEpjbEoyfHM85Hyy6H', validation_file='file-jLQN1r9LnjJ8MxRGqUUHYUEr', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Job Details\n",
        "\n",
        "The **fine-tuning jobs** here refer to individual instances of the fine-tuning process initiated with OpenAI’s API. When you submit a fine-tuning job, OpenAI creates a unique job for training the model with your specific dataset and configuration. Here’s a breakdown of what’s happening with this code:\n",
        "\n",
        "1. **Job ID Creation**:\n",
        "   - `job_id = response.id`: When you start a fine-tuning job, OpenAI returns a response containing information about the job, including a unique `job_id` (e.g., `ftjob-IGiXMyG5Pbeu152NDqLq1IuU`). This `job_id` uniquely identifies the fine-tuning job in OpenAI’s system.\n",
        "\n",
        "2. **Retrieving Job Details**:\n",
        "   - `client.fine_tuning.jobs.retrieve(job_id)`: This command retrieves the current status and details of the fine-tuning job using the `job_id`. It returns information about the job’s progress, such as:\n",
        "     - `status`: Indicates whether the job is `queued`, `running`, `completed`, or `failed`.\n",
        "     - `created_at` and `finished_at`: Timestamps showing when the job started and (if completed) finished.\n",
        "     - `model`: Specifies the base model being fine-tuned (e.g., `gpt-3.5-turbo`).\n",
        "     - `trained_tokens`: The number of tokens processed during training (useful for tracking progress).\n",
        "     - `hyperparameters`: Shows the hyperparameters used for fine-tuning, such as `n_epochs` and `learning_rate_multiplier`.\n",
        "\n",
        "### Purpose of Fine-Tuning Jobs\n",
        "Fine-tuning jobs help you monitor and manage individual training sessions. By keeping track of `job_id`s, you can:\n",
        "- Check the status of each job.\n",
        "- Retrieve the final fine-tuned model when training is complete.\n",
        "- Track resource usage, costs, and completion time.\n",
        "\n",
        "In practice, each `job_id` acts as a reference point, so you can monitor the fine-tuning process, handle multiple jobs if needed, and manage each instance separately.\n",
        "\n",
        "\n",
        "### 1. **Job Status**\n",
        "   - **Look for**: The `status` field in the job response.\n",
        "   - **Possible Values**: `queued`, `validating_files`, `running`, `succeeded`, `failed`.\n",
        "   - **What to Watch For**: This helps you track progress and detect issues. For example:\n",
        "     - `queued`: The job is waiting to start.\n",
        "     - `validating_files`: OpenAI is checking that your training and validation files meet all requirements.\n",
        "     - `running`: Fine-tuning is actively in progress.\n",
        "     - `succeeded`: Fine-tuning completed successfully, and your model is ready for use.\n",
        "     - `failed`: An error occurred; reviewing the error details can help troubleshoot the problem.\n",
        "\n",
        "### 2. **Error Messages**\n",
        "   - **Look for**: The `error` field in the job response.\n",
        "   - **Purpose**: If the job status is `failed`, the `error` field will provide details on what went wrong, such as file format issues, token limit violations, or data quality concerns. Reviewing this can help you address the issue and resubmit the job.\n",
        "\n",
        "### 3. **Timestamps**\n",
        "   - **Look for**: `created_at` and `finished_at` fields.\n",
        "   - **Purpose**: These timestamps help you measure how long the fine-tuning job took. This can be useful for planning, especially if you are running multiple jobs and managing time or costs.\n",
        "\n",
        "### 4. **Model Details**\n",
        "   - **Look for**: The `model` field.\n",
        "   - **Purpose**: Confirms the base model being fine-tuned (e.g., `gpt-3.5-turbo`). It’s a quick way to double-check that the job used the intended model configuration.\n",
        "\n",
        "### 5. **Token Count and Progress**\n",
        "   - **Look for**: `trained_tokens` field, if available.\n",
        "   - **Purpose**: Shows how many tokens have been processed so far. If you’re tracking token usage or costs, this helps monitor resource consumption.\n",
        "\n",
        "### 6. **Hyperparameters**\n",
        "   - **Look for**: `hyperparameters` field.\n",
        "   - **Purpose**: Displays the hyperparameters, such as `n_epochs`, `batch_size`, and `learning_rate_multiplier`. These parameters influence how the model learns from your data, so it’s good to confirm they match your intended configuration.\n",
        "\n",
        "### Example of Code to Print These Details\n",
        "\n",
        "Here’s a code snippet to print and check these key details:\n",
        "\n",
        "By checking these elements, you’ll be able to monitor the job’s progress, detect issues early, and verify that the fine-tuning job aligns with your intended setup."
      ],
      "metadata": {
        "id": "4ZMZdEnqUpxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_id = response.id\n",
        "# client.fine_tuning.jobs.list(limit=5)\n",
        "job_info = client.fine_tuning.jobs.retrieve(job_id)\n",
        "\n",
        "print(\"Fine-Tuning Job Information:\")\n",
        "print(f\"Job ID: {job_info.id}\")\n",
        "print(f\"Status: {job_info.status}\")\n",
        "print(f\"Created At: {job_info.created_at}\")\n",
        "print(f\"Finished At: {job_info.finished_at}\")\n",
        "print(f\"Model: {job_info.model}\")\n",
        "print(f\"Training Tokens Processed: {job_info.trained_tokens}\")\n",
        "print(\"Hyperparameters:\")\n",
        "print(f\"  - Epochs: {job_info.hyperparameters.n_epochs}\")\n",
        "print(f\"  - Batch Size: {job_info.hyperparameters.batch_size}\")\n",
        "print(f\"  - Learning Rate Multiplier: {job_info.hyperparameters.learning_rate_multiplier}\")\n",
        "\n",
        "if job_info.error:\n",
        "    print(\"Error Details:\")\n",
        "    print(f\"  - Code: {job_info.error.code}\")\n",
        "    print(f\"  - Message: {job_info.error.message}\")\n",
        "    print(f\"  - Param: {job_info.error.param}\")\n",
        "else:\n",
        "    print(\"No errors encountered.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epfhh-kCTvCN",
        "outputId": "3804bb04-b65c-478c-d5de-9d33447af6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuning Job Information:\n",
            "Job ID: ftjob-1UYbyLikaDGTxuA9qSCsLP5V\n",
            "Status: failed\n",
            "Created At: 1730239010\n",
            "Finished At: None\n",
            "Model: gpt-3.5-turbo-0125\n",
            "Training Tokens Processed: None\n",
            "Hyperparameters:\n",
            "  - Epochs: auto\n",
            "  - Batch Size: auto\n",
            "  - Learning Rate Multiplier: auto\n",
            "Error Details:\n",
            "  - Code: invalid_training_file\n",
            "  - Message: The job failed due to an invalid training file. Invalid file format. Line 1, key \"messages\": The last message must be from the assistant\n",
            "  - Param: training_file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id)\n",
        "\n",
        "events = job_response.data\n",
        "events"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzTcURlHUt-U",
        "outputId": "5269444e-ca4c-4c9a-b4e2-ee2a1df034f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FineTuningJobEvent(id='ftevent-Y4fJzMVV7ga5MFRADibnpAjT', created_at=1730239011, level='error', message='The job failed due to an invalid training file. Invalid file format. Line 1, key \"messages\": The last message must be from the assistant', object='fine_tuning.job.event', data={'error_code': 'invalid_training_file', 'error_param': 'training_file'}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-GFekP93IDNpBHgmobDyKBbwE', created_at=1730239010, level='info', message='Validating training file: file-ZgArmwLEpjbEoyfHM85Hyy6H and validation file: file-jLQN1r9LnjJ8MxRGqUUHYUEr', object='fine_tuning.job.event', data={}, type='message'),\n",
              " FineTuningJobEvent(id='ftevent-Cw5SpU5YgQyD7UWNDOc1ZTA2', created_at=1730239010, level='info', message='Created fine-tuning job: ftjob-1UYbyLikaDGTxuA9qSCsLP5V', object='fine_tuning.job.event', data={}, type='message')]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "59aPq9KmUt7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F3aZJSgfUt5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nh13ICdoUt2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F74gLIDZUt0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77oLibZ5Utx7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}