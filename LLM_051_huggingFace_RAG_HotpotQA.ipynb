{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMvBrmFO6bOdfmZM0qCllf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_051_huggingFace_RAG_HotpotQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Step 1: Install Required Libraries\n",
        "\n",
        "\n",
        "| Package              | Purpose |\n",
        "|----------------------|---------|\n",
        "| `transformers`       | LLMs like FLAN-T5 |\n",
        "| `datasets`           | Load and manage `HotpotQA` |\n",
        "| `sentence-transformers` | Create vector embeddings |\n",
        "| `faiss-cpu`          | Fast vector similarity search |\n",
        "| `python-dotenv`      | Manage your Hugging Face token securely |\n",
        "| `huggingface_hub`    | Login and download models/data |\n",
        "| `tqdm`               | Pretty progress bars |\n",
        "\n"
      ],
      "metadata": {
        "id": "2kyzhzFLJSTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mfCMV-r9I2c9"
      },
      "outputs": [],
      "source": [
        "# # üîß Environment + model access\n",
        "# !pip install python-dotenv\n",
        "# !pip install huggingface_hub\n",
        "# !pip install transformers\n",
        "\n",
        "# # üì¶ Hugging Face datasets\n",
        "# !pip install datasets\n",
        "\n",
        "# # üí¨ Sentence embeddings\n",
        "# !pip install sentence-transformers\n",
        "\n",
        "# # üîç Vector search\n",
        "# !pip install faiss-cpu  # Use faiss-gpu if on GPU runtime\n",
        "\n",
        "# # üìÑ Optional: Progress bars and data cleaning\n",
        "# !pip install tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Step 2: Load Environment Variables + Login to Hugging Face"
      ],
      "metadata": {
        "id": "KMtFuquS9LuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the .env file containing your token\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])"
      ],
      "metadata": {
        "id": "stvetTtn9JSI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìö Step 3: Load the HotpotQA Dataset\n"
      ],
      "metadata": {
        "id": "NKDbS20F7Goj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "from termcolor import colored\n",
        "\n",
        "# Load the HotpotQA dataset (fullwiki version includes large context)\n",
        "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", trust_remote_code=True)\n",
        "\n",
        "# Get a sample\n",
        "example = dataset[\"train\"][0]\n",
        "\n",
        "# what is it?\n",
        "# print(type(example[\"context\"]))\n",
        "# print(example[\"context\"])\n",
        "\n",
        "example = dataset[\"train\"][0]\n",
        "\n",
        "# Question and answer\n",
        "print(colored(\"üìå Question:\", \"cyan\", attrs=[\"bold\"]))\n",
        "print(example[\"question\"], \"\\n\")\n",
        "\n",
        "print(colored(\"‚úÖ Answer:\", \"green\", attrs=[\"bold\"]))\n",
        "print(example[\"answer\"], \"\\n\")\n",
        "\n",
        "# Updated context printer\n",
        "print(colored(\"üìö Context (Title ‚Üí Paragraph):\", \"yellow\", attrs=[\"bold\"]))\n",
        "titles = example[\"context\"][\"title\"]\n",
        "sentences = example[\"context\"][\"sentences\"]\n",
        "\n",
        "for title, sentence_list in zip(titles, sentences):\n",
        "    paragraph = \" \".join(sentence_list)\n",
        "    print(f\"üìù {title}:\\n{paragraph}\\n\")\n",
        "\n",
        "# Supporting facts\n",
        "print(colored(\"üîé Supporting Facts:\", \"magenta\", attrs=[\"bold\"]))\n",
        "print(example[\"supporting_facts\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "hSaeg-49JGat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e552a84-9a72-4ce1-a70a-0008fe6391e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Question:\n",
            "Which magazine was started first Arthur's Magazine or First for Women? \n",
            "\n",
            "‚úÖ Answer:\n",
            "Arthur's Magazine \n",
            "\n",
            "üìö Context (Title ‚Üí Paragraph):\n",
            "üìù Radio City (Indian radio station):\n",
            "Radio City is India's first private FM radio station and was started on 3 July 2001.  It broadcasts on 91.1 (earlier 91.0 in most cities) megahertz from Mumbai (where it was started in 2004), Bengaluru (started first in 2001), Lucknow and New Delhi (since 2003).  It plays Hindi, English and regional songs.  It was launched in Hyderabad in March 2006, in Chennai on 7 July 2006 and in Visakhapatnam October 2007.  Radio City recently forayed into New Media in May 2008 with the launch of a music portal - PlanetRadiocity.com that offers music related news, videos, songs, and other music-related features.  The Radio station currently plays a mix of Hindi and Regional music.  Abraham Thomas is the CEO of the company.\n",
            "\n",
            "üìù History of Albanian football:\n",
            "Football in Albania existed before the Albanian Football Federation (FSHF) was created.  This was evidenced by the team's registration at the Balkan Cup tournament during 1929-1931, which started in 1929 (although Albania eventually had pressure from the teams because of competition, competition started first and was strong enough in the duels) .  Albanian National Team was founded on June 6, 1930, but Albania had to wait 16 years to play its first international match and then defeated Yugoslavia in 1946.  In 1932, Albania joined FIFA (during the 12‚Äì16 June convention ) And in 1954 she was one of the founding members of UEFA.\n",
            "\n",
            "üìù Echosmith:\n",
            "Echosmith is an American, Corporate indie pop band formed in February 2009 in Chino, California.  Originally formed as a quartet of siblings, the band currently consists of Sydney, Noah and Graham Sierota, following the departure of eldest sibling Jamie in late 2016.  Echosmith started first as \"Ready Set Go!\"  until they signed to Warner Bros.  Records in May 2012.  They are best known for their hit song \"Cool Kids\", which reached number 13 on the \"Billboard\" Hot 100 and was certified double platinum by the RIAA with over 1,200,000 sales in the United States and also double platinum by ARIA in Australia.  The song was Warner Bros.  Records' fifth-biggest-selling-digital song of 2014, with 1.3 million downloads sold.  The band's debut album, \"Talking Dreams\", was released on October 8, 2013.\n",
            "\n",
            "üìù Women's colleges in the Southern United States:\n",
            "Women's colleges in the Southern United States refers to undergraduate, bachelor's degree‚Äìgranting institutions, often liberal arts colleges, whose student populations consist exclusively or almost exclusively of women, located in the Southern United States.  Many started first as girls' seminaries or academies.  Salem College is the oldest female educational institution in the South and Wesleyan College is the first that was established specifically as a college for women.  Some schools, such as Mary Baldwin University and Salem College, offer coeducational courses at the graduate level.\n",
            "\n",
            "üìù First Arthur County Courthouse and Jail:\n",
            "The First Arthur County Courthouse and Jail, was perhaps the smallest court house in the United States, and serves now as a museum.\n",
            "\n",
            "üìù Arthur's Magazine:\n",
            "Arthur's Magazine (1844‚Äì1846) was an American literary periodical published in Philadelphia in the 19th century.  Edited by T.S. Arthur, it featured work by Edgar A. Poe, J.H. Ingraham, Sarah Josepha Hale, Thomas G. Spear, and others.  In May 1846 it was merged into \"Godey's Lady's Book\".\n",
            "\n",
            "üìù 2014‚Äì15 Ukrainian Hockey Championship:\n",
            "The 2014‚Äì15 Ukrainian Hockey Championship was the 23rd season of the Ukrainian Hockey Championship.  Only four teams participated in the league this season, because of the instability in Ukraine and that most of the clubs had economical issues.  Generals Kiev was the only team that participated in the league the previous season, and the season started first after the year-end of 2014.  The regular season included just 12 rounds, where all the teams went to the semifinals.  In the final, ATEK Kiev defeated the regular season winner HK Kremenchuk.\n",
            "\n",
            "üìù First for Women:\n",
            "First for Women is a woman's magazine published by Bauer Media Group in the USA.  The magazine was started in 1989.  It is based in Englewood Cliffs, New Jersey.  In 2011 the circulation of the magazine was 1,310,696 copies.\n",
            "\n",
            "üìù Freeway Complex Fire:\n",
            "The Freeway Complex Fire was a 2008 wildfire in the Santa Ana Canyon area of Orange County, California.  The fire started as two separate fires on November 15, 2008.  The \"Freeway Fire\" started first shortly after 9am with the \"Landfill Fire\" igniting approximately 2 hours later.  These two separate fires merged a day later and ultimately destroyed 314 residences in Anaheim Hills and Yorba Linda.\n",
            "\n",
            "üìù William Rast:\n",
            "William Rast is an American clothing line founded by Justin Timberlake and Trace Ayala.  It is most known for their premium jeans.  On October 17, 2006, Justin Timberlake and Trace Ayala put on their first fashion show to launch their new William Rast clothing line.  The label also produces other clothing items such as jackets and tops.  The company started first as a denim line, later evolving into a men‚Äôs and women‚Äôs clothing line.\n",
            "\n",
            "üîé Supporting Facts:\n",
            "{'title': [\"Arthur's Magazine\", 'First for Women'], 'sent_id': [0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üßπ Step 4: Extract Contexts and Build a Document Store\n",
        "\n",
        "### üîç **What This Step Does**\n",
        "This step prepares the **retrieval database** for RAG. We extract individual **context paragraphs** from the HotpotQA dataset so they can be embedded and indexed for semantic search.\n",
        "\n",
        "Instead of relying on a static knowledge base, we dynamically build a **document store** from the dataset. This allows the model to later retrieve relevant information based on similarity to a user‚Äôs question.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Why It's Important**\n",
        "In a RAG pipeline:\n",
        "- The **retriever** fetches relevant passages (in this case, context paragraphs)\n",
        "- The **generator** uses them to answer the question\n",
        "\n",
        "If the documents aren‚Äôt structured well, the retriever won‚Äôt perform effectively.\n"
      ],
      "metadata": {
        "id": "xxzTjeUy9lFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of all documents (context paragraphs) for retrieval\n",
        "docs = []\n",
        "metadata = []\n",
        "\n",
        "max_docs = 1000\n",
        "processed = 0\n",
        "skipped = 0\n",
        "\n",
        "for example in dataset[\"train\"]:\n",
        "    if processed >= max_docs:\n",
        "        break\n",
        "\n",
        "    context = example.get(\"context\", {})\n",
        "    titles = context.get(\"title\")\n",
        "    sentences = context.get(\"sentences\")\n",
        "\n",
        "    if isinstance(titles, list) and isinstance(sentences, list):\n",
        "        for title, sentence_list in zip(titles, sentences):\n",
        "            paragraph = \" \".join(sentence_list)\n",
        "            docs.append(paragraph)\n",
        "            metadata.append({\"title\": title})\n",
        "        processed += 1\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f\"‚úÖ Collected {len(docs)} paragraphs from {processed} examples. Skipped: {skipped}\")\n"
      ],
      "metadata": {
        "id": "wKiLH5nxJGX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de62d76b-c9e5-4b4c-a0a3-7aa3c6b5a912"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Collected 9930 paragraphs from 1000 examples. Skipped: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely ‚Äî this is one of the most powerful ideas behind RAG and modern search systems. Let's break it down in a clean, intuitive way for your documentation (and understanding):\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What Does It Mean to \"Embed and Index for Semantic Search\"?\n",
        "\n",
        "### üß† 1. **Embedding**: Turn Text Into Meaningful Vectors\n",
        "\n",
        "In traditional search (like CTRL+F or SQL `LIKE`), we match words **exactly**.\n",
        "\n",
        "In **semantic search**, we want to match **meaning**, even if the words are different.\n",
        "\n",
        "So we use **embedding models** like [`sentence-transformers`](https://www.sbert.net/) to convert each paragraph (or question) into a **vector** ‚Äî a list of numbers that captures its meaning in multi-dimensional space.\n",
        "\n",
        "Example:\n",
        "\n",
        "| Text | Embedding |\n",
        "|------|-----------|\n",
        "| `\"Tesla was born in Smiljan.\"` | `[0.12, -0.45, 0.88, ...]` |\n",
        "| `\"Where was Nikola Tesla born?\"` | `[0.14, -0.42, 0.91, ...]` |\n",
        "\n",
        "The closer these vectors are in space, the more similar the **meaning**.\n",
        "\n",
        "---\n",
        "\n",
        "### üóÇÔ∏è 2. **Indexing**: Make Searching Fast\n",
        "\n",
        "Once we have all those vectors for your documents, we need to **store and organize** them so we can:\n",
        "- Quickly compare any question vector to all document vectors\n",
        "- Retrieve the **most similar matches** (i.e., relevant knowledge)\n",
        "\n",
        "This is where **FAISS** (Facebook AI Similarity Search) comes in:\n",
        "- It builds a **vector index**\n",
        "- It supports **fast nearest neighbor search**\n",
        "- You can ask: _\"What are the 5 most similar paragraphs to this question?\"_\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ In a RAG Pipeline\n",
        "\n",
        "| Step | What Happens |\n",
        "|------|--------------|\n",
        "| üîπ Embed your documents | Each doc is transformed into a vector |\n",
        "| üîπ Index them with FAISS | FAISS stores and organizes them for fast lookup |\n",
        "| üîπ Embed a new question | Transform the question into a vector |\n",
        "| üîπ Query FAISS | Find top-k matching document vectors |\n",
        "| üîπ Feed those docs to a generator | A model like FLAN-T5 uses them to answer the question |\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Tools You‚Äôre Using\n",
        "\n",
        "| Tool | Role |\n",
        "|------|------|\n",
        "| `sentence-transformers` | Create semantic embeddings |\n",
        "| `faiss` | Search over embeddings |\n",
        "| `transformers` | Generate answers based on the retrieved context |\n",
        "\n"
      ],
      "metadata": {
        "id": "iLzILuxlB1sm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QZJ3eU_JGVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mhZmofwNJGS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove Widgets from Notebook to save to Github"
      ],
      "metadata": {
        "id": "JcqxoHFcJLer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your current notebook file (adjust if different)\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_051_huggingFace_Zero-Shot_RAG_Classification_YELP_Reviews.ipynb\"\n",
        "\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove the widget metadata if it exists\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook metadata cleaned. Try saving to GitHub again.\")\n"
      ],
      "metadata": {
        "id": "yno3jJDaJGQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}