{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_009_chatbots_yield_vs_return_response.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
      "metadata": {
        "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2"
      },
      "source": [
        "#Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "xYtTpEjjA8hD"
      },
      "id": "xYtTpEjjA8hD"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "# !pip install google-generativeai\n",
        "# !pip install anthropic\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "ifw0J2E4A2Ex"
      },
      "id": "ifw0J2E4A2Ex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "m7TZSjRQA517"
      },
      "id": "m7TZSjRQA517"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
      "metadata": {
        "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "from typing import List\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "33QZ6MgqBGbd"
      },
      "id": "33QZ6MgqBGbd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')  # Ensure this is the correct path to your file\n",
        "\n",
        "# Get the API keys from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Check if the keys are loaded correctly and print a portion of them\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key loaded: {openai_api_key[0:10]}...\")  # Only print part of the key\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded correctly.\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key loaded: {anthropic_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Anthropic API key not loaded correctly.\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key loaded: {google_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Google API key not loaded correctly.\")"
      ],
      "metadata": {
        "id": "IFJKU1REww17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459ab8e3-e243-4da2-95a5-3eb899864d69"
      },
      "id": "IFJKU1REww17",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded: sk-proj-e1...\n",
            "Anthropic API Key loaded: sk-ant-api...\n",
            "Google API Key loaded: AIzaSyDh3a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai\n",
        "\n",
        "# Connect to OpenAI\n",
        "openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "\n",
        "# Connect to Anthropic (Claude)\n",
        "claude = anthropic.Anthropic(api_key=anthropic_api_key)  # Set Anthropic API key\n",
        "\n",
        "# Connect to Google Generative AI\n",
        "google.generativeai.configure(api_key=google_api_key)  # Set Google API key"
      ],
      "metadata": {
        "id": "_tvSBQa3BSGB"
      },
      "id": "_tvSBQa3BSGB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Customer Service Chatbot\n",
        "\n",
        "Let's start building a **customer service chatbot** for Stanley's popular store, focusing on handling FAQ-style conversations. The chatbot will be able to handle common customer queries like product availability, returns, shipping, and possibly escalate issues to human support.\n",
        "\n",
        "### Strategy:\n",
        "- **Handle common questions**: FAQs about products, returns, shipping times, etc.\n",
        "- **Escalation**: Detect when a user needs human support.\n",
        "- **Sentiment analysis** (optional, advanced feature): Detect frustration and automatically offer human support or apologies.\n",
        "\n",
        "### Step 1: Define Common Queries (FAQ)\n",
        "We’ll build the chatbot to handle the following common queries:\n",
        "- **Product Availability**: Is the Stanley Cup available in certain colors or sizes?\n",
        "- **Returns**: How to return a product?\n",
        "- **Shipping Times**: How long does shipping take?\n",
        "- **Order Tracking**: Where is my order?\n",
        "- **Escalation**: When the user asks for human support or gets frustrated.\n",
        "\n",
        "### Step 2: Implement the Chatbot Functionality\n",
        "We’ll create a basic chatbot function that can answer these queries based on the user's input. Later, we can enhance it with sentiment analysis and multilingual support.\n",
        "\n",
        "### How It Works:\n",
        "1. **Predefined FAQs**: The chatbot has built-in answers for common customer service queries like product availability, returns, shipping, and order tracking. If the user's message contains keywords related to those topics, it will return a pre-programmed response.\n",
        "   \n",
        "2. **Handling Complex Queries**: If the user asks a question that doesn't match the predefined FAQs, the chatbot passes the query to OpenAI's GPT model to generate a response.\n",
        "\n",
        "3. **Escalation to Human Support**: If the user mentions wanting to speak with a human representative, the chatbot will offer an escalation message, informing the user that a human can be connected.\n",
        "\n",
        "### Next Steps:\n",
        "1. **Sentiment Analysis**: We can integrate a basic sentiment analysis function to detect frustration (e.g., if the user is unhappy with the bot's answers) and automatically suggest human assistance.\n",
        "   \n",
        "2. **Multilingual Support**: Later, we can add language detection and handle conversations in multiple languages.\n",
        "\n",
        "3. **API Integration**: For order tracking or real-time availability, we can integrate actual APIs from Stanley’s website or database to provide more personalized responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "_v-qD7JZO6oY"
      },
      "id": "_v-qD7JZO6oY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Return Response"
      ],
      "metadata": {
        "id": "bifGHTWj7sAi"
      },
      "id": "bifGHTWj7sAi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035",
      "metadata": {
        "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "69f90ff2-fccb-480c-a912-46a8449e7ed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://668330e9b473570394.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://668330e9b473570394.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful customer service assistant for the Stanley store. You assist customers with queries about products, returns, shipping, and orders.\"\n",
        "history_file = \"chat_history.json\"\n",
        "\n",
        "# Function to save chat history to a JSON file\n",
        "def save_chat_history(history, file_path):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(history, file, indent=4)\n",
        "\n",
        "# Sample FAQs for the Stanley store\n",
        "faq_responses = {\n",
        "    \"availability\": \"The Stanley Cup is available in multiple colors and sizes, but some items may be out of stock due to high demand. You can check the latest availability on our website.\",\n",
        "    \"returns\": \"You can return any Stanley product within 30 days of purchase. Just visit our Returns page for more details and to start the process.\",\n",
        "    \"shipping\": \"Shipping typically takes between 3-5 business days for standard delivery. Expedited options are also available.\",\n",
        "    \"order tracking\": \"You can track your order by visiting the 'Order Tracking' page on our website. Just enter your order number to get the latest updates.\",\n",
        "    \"human support\": \"I can help you with most questions, but if you need further assistance, I can connect you to a human representative.\"\n",
        "}\n",
        "\n",
        "# Chat function\n",
        "def stanley_chat(message, history):\n",
        "    # System message at the start\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Add the conversation history\n",
        "    for msg in history:\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the current user message\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Check for common FAQs\n",
        "    response = \"\"\n",
        "    if \"availability\" in message.lower():\n",
        "        response = faq_responses[\"availability\"]\n",
        "    elif \"return\" in message.lower():\n",
        "        response = faq_responses[\"returns\"]\n",
        "    elif \"shipping\" in message.lower():\n",
        "        response = faq_responses[\"shipping\"]\n",
        "    elif \"track\" in message.lower():\n",
        "        response = faq_responses[\"order tracking\"]\n",
        "    elif \"human\" in message.lower() or \"representative\" in message.lower():\n",
        "        response = faq_responses[\"human support\"]\n",
        "    else:\n",
        "        # Use the AI to handle more complex or unknown questions\n",
        "        stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "        for chunk in stream:\n",
        "            response += chunk.choices[0].delta.content or ''\n",
        "\n",
        "    return response\n",
        "\n",
        "    # Add the current user message and assistant response to the history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Save updated history to a JSON file\n",
        "    save_chat_history(history, history_file)\n",
        "\n",
        "# Gradio interface\n",
        "gr.ChatInterface(fn=stanley_chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Chat Log"
      ],
      "metadata": {
        "id": "nDBICK2X5P6C"
      },
      "id": "nDBICK2X5P6C"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def print_chat_history(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"r\") as file:\n",
        "            history = json.load(file)\n",
        "\n",
        "        print(\"\\nChat History:\\n\" + \"=\" * 50)\n",
        "        turn = 1\n",
        "        for msg in history:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                # print(f\"Turn {turn}:\")\n",
        "                print(f\"User: {msg['content']}\")\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                print(f\"Assistant: {msg['content']}\")\n",
        "                print(\"-\" * 50)\n",
        "                turn += 1\n",
        "    else:\n",
        "        print(\"No chat history found.\")\n",
        "\n",
        "print_chat_history(\"/content/chat_history.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gQgXOIU4Lny",
        "outputId": "3d4d1e7e-c1d8-42e9-91a6-f6b2b80f2cac"
      },
      "id": "8gQgXOIU4Lny",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat History:\n",
            "==================================================\n",
            "User: hello\n",
            "Assistant: Hello! How can I assist you today?\n",
            "--------------------------------------------------\n",
            "User: i need some help with a return\n",
            "Assistant: You can return any Stanley product within 30 days of purchase. Just visit our Returns page for more details and to start the process.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yield (Stream) Response\n",
        "\n",
        "You can modify the function to consistently use `yield` even when an FAQ response is returned. Now, if the FAQ condition is met, the response is yielded directly, maintaining a consistent generator flow in both cases.\n",
        "\n",
        "```python\n",
        "  # If the question is not in FAQs, generate response from the AI\n",
        "  if not response:\n",
        "      stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "      for chunk in stream:\n",
        "          response += chunk.choices[0].delta.content or ''\n",
        "          yield response\n",
        "  else:\n",
        "      # Yield the FAQ response as a single output\n",
        "      yield response\n",
        "```\n"
      ],
      "metadata": {
        "id": "H2em9dVP2ODw"
      },
      "id": "H2em9dVP2ODw"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import openai\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful customer service assistant for the Stanley store. You assist customers with queries about products, returns, shipping, and orders.\"\n",
        "history_file = \"chat_history.json\"\n",
        "\n",
        "# Function to save chat history to a JSON file\n",
        "def save_chat_history(history, file_path):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(history, file, indent=4)\n",
        "\n",
        "# Sample FAQs for the Stanley store\n",
        "faq_responses = {\n",
        "    \"availability\": \"The Stanley Cup is available in multiple colors and sizes, but some items may be out of stock due to high demand. You can check the latest availability on our website.\",\n",
        "    \"returns\": \"You can return any Stanley product within 30 days of purchase. Just visit our Returns page for more details and to start the process.\",\n",
        "    \"shipping\": \"Shipping typically takes between 3-5 business days for standard delivery. Expedited options are also available.\",\n",
        "    \"order tracking\": \"You can track your order by visiting the 'Order Tracking' page on our website. Just enter your order number to get the latest updates.\",\n",
        "    \"human support\": \"I can help you with most questions, but if you need further assistance, I can connect you to a human representative.\"\n",
        "}\n",
        "\n",
        "# Chat function\n",
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding user and assistant messages using OpenAI-style format\n",
        "    for msg in history:\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Check for common FAQs first\n",
        "    response = \"\"\n",
        "    if \"availability\" in message.lower():\n",
        "        response = faq_responses[\"availability\"]\n",
        "    elif \"return\" in message.lower():\n",
        "        response = faq_responses[\"returns\"]\n",
        "    elif \"shipping\" in message.lower():\n",
        "        response = faq_responses[\"shipping\"]\n",
        "    elif \"track\" in message.lower():\n",
        "        response = faq_responses[\"order tracking\"]\n",
        "    elif \"human\" in message.lower() or \"representative\" in message.lower():\n",
        "        response = faq_responses[\"human support\"]\n",
        "\n",
        "    # If the question is not in FAQs, generate response from the AI\n",
        "    if not response:\n",
        "        stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "        for chunk in stream:\n",
        "            response += chunk.choices[0].delta.content or ''\n",
        "            yield response\n",
        "    else:\n",
        "        # Yield the FAQ response as a single output\n",
        "        yield response\n",
        "\n",
        "\n",
        "    # Add the current user message and assistant response to the history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Save updated history to a JSON file\n",
        "    save_chat_history(history, history_file)\n",
        "\n",
        "# Gradio interface\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "M5joRB_UvzrF",
        "outputId": "1c4e71e1-0b71-485f-98e9-8e94a80a4008"
      },
      "id": "M5joRB_UvzrF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://bdda640b6f5d6228cf.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bdda640b6f5d6228cf.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7866 <> https://bdda640b6f5d6228cf.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Chat Log"
      ],
      "metadata": {
        "id": "Z9GY622y02EL"
      },
      "id": "Z9GY622y02EL"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def print_chat_history(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"r\") as file:\n",
        "            history = json.load(file)\n",
        "\n",
        "        print(\"\\nChat History:\\n\" + \"=\" * 50)\n",
        "        turn = 1\n",
        "        for msg in history:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                # print(f\"Turn {turn}:\")\n",
        "                print(f\"User: {msg['content']}\")\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                print(f\"Assistant: {msg['content']}\")\n",
        "                print(\"-\" * 50)\n",
        "                turn += 1\n",
        "    else:\n",
        "        print(\"No chat history found.\")\n",
        "\n",
        "print_chat_history(\"/content/chat_history.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeZNGDiy0zSS",
        "outputId": "b224e85b-810d-46ad-f1d4-f7eb5c3dc524"
      },
      "id": "yeZNGDiy0zSS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat History:\n",
            "==================================================\n",
            "User: hello\n",
            "Assistant: Hello! How can I assist you today?\n",
            "--------------------------------------------------\n",
            "User: I need to make a return\n",
            "Assistant: You can return any Stanley product within 30 days of purchase. Just visit our Returns page for more details and to start the process.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}