{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHlIhzfwfqBUVk4JZov/5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_025_tuning_LLM_imdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTfkqykweKCW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a structured lesson plan to guide you through fine-tuning a large language model (LLM), focusing on essential concepts and practical steps. This plan will break down the core components and provide you with a solid understanding of the process before diving into code.\n",
        "\n",
        "---\n",
        "\n",
        "## Fine-Tuning a Large Language Model (LLM)\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Introduction to Fine-Tuning Large Language Models**\n",
        "   - **Goal**: Understand the purpose of fine-tuning and when to apply it to LLMs.\n",
        "   - **Key Concepts**:\n",
        "     - **Pre-trained Model**: Understand that pre-trained models, like GPT, BERT, etc., have learned language patterns and structures but may need further tuning for specific tasks (e.g., sentiment analysis, summarization).\n",
        "     - **Fine-Tuning vs. Pre-training**: Know that pre-training is the initial phase, involving large datasets, while fine-tuning adapts the model to a narrower, task-specific dataset.\n",
        "   - **Takeaway**: Fine-tuning leverages the model’s learned language structures for specific applications, reducing the need for vast compute resources.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Preparing Data for Fine-Tuning**\n",
        "   - **Goal**: Learn data selection and preparation techniques for effective fine-tuning.\n",
        "   - **Key Concepts**:\n",
        "     - **Data Relevance**: Choosing data that closely matches your target task. For example, for a sentiment analysis task, choose a dataset with labeled sentiments.\n",
        "     - **Data Formatting**: Structuring data in formats that align with LLMs (usually text paired with labels, sometimes in JSON/CSV format).\n",
        "     - **Tokenization**: Prepare text data by breaking it down into tokens. Key concepts here include `padding`, `truncation`, and the importance of uniform input sizes.\n",
        "   - **Takeaway**: Properly prepared data is essential for smooth fine-tuning and effective task adaptation.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Understanding and Setting Hyperparameters**\n",
        "   - **Goal**: Familiarize yourself with tuning parameters that affect training speed, accuracy, and generalization.\n",
        "   - **Key Concepts**:\n",
        "     - **Learning Rate**: A critical hyperparameter that controls the model’s adjustments at each step; setting this value too high or low can hinder performance.\n",
        "     - **Batch Size**: Impacts memory usage and stability; large models typically require smaller batch sizes.\n",
        "     - **Epochs**: Number of times the model goes through the entire dataset. Too many epochs can lead to overfitting; too few may underfit.\n",
        "   - **Takeaway**: Understanding these hyperparameters is crucial for optimizing performance without over- or under-training the model.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Choosing a Fine-Tuning Method**\n",
        "   - **Goal**: Explore different techniques for fine-tuning, focusing on the most popular ones.\n",
        "   - **Key Concepts**:\n",
        "     - **Standard Fine-Tuning**: Updating all parameters of the model, typically more resource-intensive.\n",
        "     - **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like LoRA, Adapter Layers, and Prompt Tuning that update only a subset of parameters or add small, trainable modules to the model.\n",
        "   - **Takeaway**: Choose an appropriate method based on your compute resources and task complexity; PEFT methods like LoRA are useful for large LLMs with constrained resources.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Training Process**\n",
        "   - **Goal**: Execute the fine-tuning process and understand each step.\n",
        "   - **Key Concepts**:\n",
        "     - **Forward Propagation**: Understand how the model makes predictions by passing inputs forward through the network.\n",
        "     - **Loss Calculation**: Measure the error of predictions, often with cross-entropy loss for classification tasks.\n",
        "     - **Backward Propagation**: Update model weights based on loss gradients to minimize error. Learn about **gradients** and **optimizers** (e.g., AdamW).\n",
        "   - **Takeaway**: Each training step adjusts the model to better match your task-specific dataset by minimizing the error.\n",
        "\n",
        "---\n",
        "\n",
        "#### **6. Evaluation and Metrics**\n",
        "   - **Goal**: Assess the model’s performance with relevant metrics.\n",
        "   - **Key Concepts**:\n",
        "     - **Accuracy, Precision, Recall, F1-Score**: Understand these metrics for classification tasks.\n",
        "     - **Validation and Test Sets**: The importance of keeping test data separate to get an unbiased measure of the model’s real-world performance.\n",
        "   - **Takeaway**: Effective evaluation ensures that the model performs well on unseen data and provides insight into areas for further tuning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **7. Saving, Loading, and Deploying the Model**\n",
        "   - **Goal**: Make your fine-tuned model reusable and deployable.\n",
        "   - **Key Concepts**:\n",
        "     - **Model Serialization**: Saving the model and tokenizer so they can be loaded later without retraining.\n",
        "     - **Deploying with APIs**: Briefly introduce options for deploying the model using APIs or integrating with web apps.\n",
        "   - **Takeaway**: Saved models can be reused or further fine-tuned, and deployment enables real-world applications.\n",
        "\n",
        "---\n",
        "\n",
        "#### **8. Advanced Techniques (Optional)**\n",
        "   - **Goal**: Explore advanced fine-tuning concepts if you’re comfortable with the basics.\n",
        "   - **Key Concepts**:\n",
        "     - **Hyperparameter Tuning**: Use tools like grid search or Bayesian optimization to find the best hyperparameters.\n",
        "     - **Ensemble Methods**: Combine models to improve accuracy or robustness.\n",
        "     - **Knowledge Distillation**: Compress large models into smaller, faster versions while preserving performance.\n",
        "   - **Takeaway**: Advanced techniques can further optimize and refine your model’s performance, but they require a strong grasp of fine-tuning fundamentals.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "This lesson plan covers the essential steps and concepts you need to know to fine-tune a large language model. Each step builds on the last, reinforcing core knowledge and ensuring you’re equipped with both the theoretical background and practical skills to fine-tune LLMs effectively.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q6CZOeA6eShL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers datasets\n",
        "# !pip install python-dotenv"
      ],
      "metadata": {
        "id": "qhAJ0syxexol"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libaries"
      ],
      "metadata": {
        "id": "t_SUysLF-HSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv('/content/huggingface_api_key.env')\n",
        "api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "os.environ[\"HF_TOKEN\"] = api_key\n",
        "\n",
        "# Load the Amazon Reviews dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Split data into documents and labels for training and testing\n",
        "documents = dataset[\"train\"][\"text\"]\n",
        "labels = [\"pos\" if label == 1 else \"neg\" for label in dataset[\"train\"][\"label\"]]\n",
        "\n",
        "# Quick preview of dataset\n",
        "print(\"Sample IMDb Review Text:\", documents[0])\n",
        "print(\"Sample IMDb Review Label:\", labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL8soXQrfAcN",
        "outputId": "7872d123-0ed8-4d26-b18a-04768ac119de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample IMDb Review Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
            "Sample IMDb Review Label: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few entries in the training dataset\n",
        "for i in range(3):\n",
        "    print(f\"Review {i+1}:\")\n",
        "    print(\"Text:\", dataset[\"train\"][\"text\"][i])\n",
        "    print(\"Label:\", \"Positive\" if dataset[\"train\"][\"label\"][i] == 1 else \"Negative\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcNiFzPVhGxf",
        "outputId": "aa169ac5-0bc0-45c1-a716-8eb89a7b9cd8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1:\n",
            "Text: I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
            "Label: Negative\n",
            "==================================================\n",
            "Review 2:\n",
            "Text: \"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.\n",
            "Label: Negative\n",
            "==================================================\n",
            "Review 3:\n",
            "Text: If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\n",
            "Label: Negative\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Data"
      ],
      "metadata": {
        "id": "FADcS_FH-LEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract labels and documents from the IMDb dataset\n",
        "documents = dataset[\"train\"][\"text\"]\n",
        "labels = [\"pos\" if label == 1 else \"neg\" for label in dataset[\"train\"][\"label\"]]\n",
        "\n",
        "# Calculate and print value counts for labels\n",
        "label_counts = pd.Series(labels).value_counts()\n",
        "\n",
        "# Calculate review lengths\n",
        "review_lengths = [len(review.split()) for review in documents]\n",
        "\n",
        "# Create a 1x2 grid for the plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Label Distribution\n",
        "axes[0].bar(label_counts.index, label_counts.values, color=['skyblue', 'salmon'])\n",
        "axes[0].set_title(\"IMDb Reviews Sentiment Distribution\")\n",
        "axes[0].set_xlabel(\"Sentiment\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "axes[0].set_xticks(range(len(label_counts.index)))\n",
        "axes[0].set_xticklabels([\"Positive\", \"Negative\"])\n",
        "\n",
        "# Plot 2: Review Length Distribution\n",
        "axes[1].hist(review_lengths, bins=30, color='gold', alpha=0.7)\n",
        "axes[1].set_title(\"Distribution of Review Lengths in IMDb Reviews\")\n",
        "axes[1].set_xlabel(\"Number of Words\")\n",
        "axes[1].set_ylabel(\"Frequency\")\n",
        "\n",
        "# Adjust layout and show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"Review Length Statistics:\")\n",
        "print(\"Mean Length:\", sum(review_lengths) / len(review_lengths))\n",
        "print(\"Max Length:\", max(review_lengths))\n",
        "print(\"Min Length:\", min(review_lengths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "YNPXANZphYnW",
        "outputId": "b08e1ba1-dd42-4943-890d-5f979912d99c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAHqCAYAAADrpwd3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACTOklEQVR4nOzdf3zN9f//8fuZ2Q9jm1/bLHvPQn7nZzFMZBmWEoooPxKlKT+K0g8/iwzz+0f6gUShIlEyP4pYfizzm1R+FduEbYiZ7fX9w/e8Po7tzMw4s27Xy+Vc6jyfj/N6Pl7nOOc8z2PP8zwWwzAMAQAAAAAAAACATJwcnQAAAAAAAAAAAPkVRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQADjV37lxZLBZt377d0anctOHDh8tisTg6jbvWjz/+KIvFoh9//NHRqTicxWLR8OHDb/s4Wd3nTZs2VfXq1W/72JJ05MgRWSwWzZ07946MBwBAbtzJOV7Tpk3VtGlT87r1vfrLL7+8I+N3795d5cqVuyNj5db58+f1/PPPy8/PTxaLRf3793d0SlkqV66cunfv7ug07lrW590///xz28YoaI/RnbjPbpe74bUHuB5FdOA/LqsitvXN2MnJScePH890m5SUFLm7u8tisahv375mu7VAZr0ULlxYpUqVUsOGDfXmm2/q2LFjeZ6/NddrxyxXrpxeeeUVJSUl5fl4d6vz589r2LBhql69ujw8PFSyZEnVqlVL/fr104kTJ27r2DNmzLiri6YLFy7UpEmTchxfrlw589+jk5OTvL29VaNGDfXu3VtbtmxxWF53Un7ODQDw32Kd61ovbm5u8vf3V1hYmKZMmaJz587lyTgnTpzQ8OHDFRcXlyfHy0v5ObecGD16tObOnas+ffpo/vz5evbZZ+3GXjsPs1gs8vDw0IMPPqhPP/30DmbsONbPY+PHj3d0KnaNHj1ay5Ytc3QaeSKrInb37t1lsVjk6empixcvZrrNoUOHzH+f1z5O1j+gWS+urq7y9fVV06ZNNXr0aJ06dSrP87fmeu2Y9913n4YOHapLly7l+XjA3c7Z0QkAyL9cXV31+eefa/DgwTbtX3/9dba3e/rpp9W6dWtlZGTo7Nmz2rZtmyZNmqTJkyfr448/VqdOnfI815kzZ6po0aK6cOGC1q5dq6lTp+rXX3/Vzz//nOdjWb399tt64403btvx80paWpqaNGmiAwcOqFu3bnr55Zd1/vx57d27VwsXLtQTTzwhf3//2zb+jBkzVKpUqUyrPpo0aaKLFy/KxcXlto2dFxYuXKg9e/bc1KqnWrVq6dVXX5UknTt3Tvv379eSJUv04YcfasCAAYqKirKJv3jxopydb+4tOTd53an73F5ugYGBunjxogoXLnxbxwcA4HojR45UUFCQ0tLSFB8frx9//FH9+/dXVFSUli9frvvvv9+Mzc0c78SJExoxYoTKlSunWrVq5fh2q1evvqlxciO73D788ENlZGTc9hxuxbp169SgQQMNGzYsR/HXzsNOnjypjz76SN26dVNqaqp69ep12/I8ePCgnJxYp3gjo0ePVocOHdS2bds7PvadeoycnZ3177//6ttvv9VTTz1l07dgwQK5ubnZLVK/8soreuCBB5Senq5Tp05p8+bNGjZsmKKiorR48WI9/PDDeZqrq6urPvroI0lScnKyvvnmG40aNUp//PGHFixYkKdjXetueO0BrkcRHYBdrVu3zrKIvnDhQoWHh+urr77K8nZ16tTRM888Y9N29OhRtWjRQt26dVOVKlVUs2bNPM21Q4cOKlWqlCTphRdeUKdOnbRo0SJt3bpVDz74YJ6OZeXs7HzThU9HWLZsmXbs2KEFCxaoc+fONn2XLl3S5cuXHZKXk5OT3NzcHDL27XbPPfdkeg6MHTtWnTt31sSJE1WxYkX16dPH7Lvd98OlS5fk4uLi8PvcugIQAIA7rVWrVqpXr555fciQIVq3bp0effRRPfbYY9q/f7/c3d0l3Zk53r///qsiRYo4fDHB3fCH7cTERFWtWjXH8dfPw7p37657771XEydOvK1FdFdX19t2bOSNO/UYubq6qlGjRvr8888zFdFv9Fk6JCREHTp0sGnbuXOnWrRoofbt22vfvn0qU6ZMnuXq7Oxs83x56aWX1LBhQ33++eeKioqSr69vno11rbvhtQe4Hn8mBWBX586dFRcXpwMHDpht8fHxWrduXaZi7I0EBgZq7ty5unz5siIjIzP1//vvv3rhhRdUsmRJeXp6qmvXrjp79myucw8JCZEk/fHHHzbtW7ZsUcuWLeXl5aUiRYrooYce0qZNm8z+L7/8UhaLRT/99FOmY37wwQeyWCzas2ePJPv7ZX722WeqW7eu3N3dVaJECXXq1MlmW5wpU6aoUKFCNtvNTJgwQRaLRQMHDjTb0tPTVaxYMb3++utm2xdffKG6deuqWLFi8vT0VI0aNTR58uRs7wvrfdCoUaNMfW5ubvL09LRpO3DggDp06KASJUrIzc1N9erV0/Lly21irF+N3rRpkwYOHKjSpUvLw8NDTzzxhM1XDcuVK6e9e/fqp59+Mr8maN33M7v9uXft2qWHHnpIRYoUUYUKFcx9QX/66SfVr19f7u7uqlSpktasWZPpnP7++28999xz8vX1laurq6pVq6ZPPvnEJsY69uLFi/Xee++pbNmycnNzU/PmzfX777/b5LNy5UodPXrUzD+3e/e5u7tr/vz5KlGihN577z0ZhmH2Xb8n+rlz59S/f3+VK1dOrq6u8vHx0SOPPKJff/31hnlZz+2LL77Q22+/rXvuuUdFihRRSkpKtvvQx8bGqmHDhnJ3d1dQUJBmzZpl0299zI8cOZLlfWk9Zna52dsTfd26dQoJCZGHh4e8vb31+OOPa//+/TYx1ufb77//ru7du8vb21teXl7q0aOH/v3335w9CAAAXOPhhx/WO++8o6NHj+qzzz4z27Oa40VHR6tx48by9vZW0aJFValSJb355puSrr4XPvDAA5KkHj16mO9/1vc76/wmNjZWTZo0UZEiRczbXr8nulV6errefPNN+fn5ycPDQ4899limbRbt7e987TFvlFtW+xJfuHBBr776qgICAuTq6qpKlSpp/PjxNnMXSebWjsuWLVP16tXNedeqVauyvsOvk5iYqJ49e8rX11dubm6qWbOm5s2bZ/Zb5xiHDx/WypUrzdyvn4vcSOnSpVW5cuVMnwsyMjI0adIkVatWTW5ubvL19dULL7xg8xnk0Ucf1b333pvlcYODg23+MJPV45GUlKT+/fub92WFChU0duxYmxW4derUUbt27WxuV6NGDVksFu3atctsW7RokSwWS6Y5Um6kpqZq2LBhqlChglxdXRUQEKDBgwcrNTXVJu5mHuMff/xR9erVk5ubm8qXL68PPvgg03PJYrHowoULmjdvnvl4ZnWf3Wiul93zMTvXP0Y5/UyTG507d9b3339v85lv27ZtOnTo0E1/lq5Zs6YmTZqkpKQkTZs2LVP/P//8o6eeekqenp4qWbKk+vXrl+vtWCwWixo3bizDMPTnn3/a9H3//ffmnL1YsWIKDw/X3r17zf7x48fLYrHo6NGjmY47ZMgQubi4mM+vrF57cvKcHDhwoEqWLGnzevTyyy/LYrFoypQpZltCQoIsFotmzpxptk2dOlXVqlVTkSJFVLx4cdWrV08LFy7M1f2E/yaK6ADsatKkicqWLWvzxrJo0SIVLVpU4eHhN3284OBglS9fXtHR0Zn6+vbtq/3792v48OHq2rWrFixYoLZt22aarOeUdXJdvHhxs23dunVq0qSJUlJSNGzYMI0ePVpJSUl6+OGHtXXrVklSeHi4ihYtqsWLF2c65qJFi1StWrVsf4TxvffeU9euXVWxYkVFRUWpf//+Wrt2rZo0aWJOoEJCQpSRkWGz1czGjRvl5OSkjRs3mm07duzQ+fPn1aRJE0lXJ4tPP/20ihcvrrFjx+r9999X06ZNbf4IkJXAwEBJ0qeffnrD+3Pv3r1q0KCB9u/frzfeeEMTJkyQh4eH2rZtq6VLl2aKf/nll7Vz504NGzZMffr00bfffmuzT/6kSZNUtmxZVa5cWfPnz9f8+fP11ltvZZvD2bNn9eijj6p+/fqKjIyUq6ur+c2CTp06qXXr1nr//fd14cIFdejQwWYv04SEBDVo0EBr1qxR3759NXnyZFWoUEE9e/bMco/u999/X0uXLtVrr72mIUOG6JdfflGXLl3M/rfeeku1atVSqVKlzPxvZa/vokWL6oknntDff/+tffv22Y178cUXNXPmTLVv314zZszQa6+9Jnd3d/NDU07yGjVqlFauXKnXXntNo0ePznal29mzZ9W6dWvVrVtXkZGRKlu2rPr06ZPpjw85cbP32Zo1axQWFqbExEQNHz5cAwcO1ObNm9WoUaMsPyQ/9dRTOnfunMaMGaOnnnpKc+fO1YgRI246TwAAJJn7a2e3rcrevXv16KOPKjU1VSNHjtSECRP02GOPmXOwKlWqaOTIkZKk3r17m+9/1jmcJJ0+fVqtWrVSrVq1NGnSJDVr1izbvN577z2tXLlSr7/+ul555RVFR0crNDQ0yz2Ws5OT3K5lGIYee+wxTZw4US1btlRUVJQqVaqkQYMG2Sz2sPr555/10ksvqVOnToqMjNSlS5fUvn17nT59Otu8Ll68qKZNm2r+/Pnq0qWLxo0bJy8vL3Xv3t1cIFKlShXNnz9fpUqVUq1atczcS5cufVP3wZUrV/TXX3/ZfC6Qrn57ddCgQWrUqJEmT56sHj16aMGCBQoLC1NaWpokqWPHjjp8+LC2bdtmc9ujR4/ql19+yXabyn///VcPPfSQPvvsM3Xt2lVTpkxRo0aNNGTIEJv7MiQkxOZzwZkzZ7R3795Mnw02btyo0qVLq0qVKjd1/tfLyMjQY489pvHjx6tNmzaaOnWq2rZtq4kTJ6pjx46Z4nPyGO/YsUMtW7bU6dOnNWLECPXs2VMjR47MtPf5/Pnz5erqqpCQEPPxfOGFF2xibjTXu9HzMTdu9JkmN9q1ayeLxWKzFerChQtVuXJl1alT56aP16FDB7m7u2f5WvXUU0/p0qVLGjNmjFq3bq0pU6aod+/euc49q8/S8+fPNz8rjx07Vu+884727dunxo0bm/FPPfWUuVDpeosXL1aLFi0yPQ+vlZPnZEhIiPkcscrqs7T1/62vdR9++KFeeeUVVa1aVZMmTdKIESNUq1atPP3NKvwHGAD+0+bMmWNIMrZt22a2DRs2zJBknDp1ynjttdeMChUqmH0PPPCA0aNHD8MwDEOSERERYfYdPnzYkGSMGzfO7niPP/64IclITk62Gb9u3brG5cuXzbjIyEhDkvHNN99km78114MHDxqnTp0yjhw5YnzyySeGu7u7Ubp0aePChQuGYRhGRkaGUbFiRSMsLMzIyMgwb//vv/8aQUFBxiOPPGK2Pf3004aPj49x5coVs+3kyZOGk5OTMXLkyExjWx05csQoVKiQ8d5779nkuHv3bsPZ2dlsT09PNzw9PY3BgwebuZUsWdJ48sknjUKFChnnzp0zDMMwoqKiDCcnJ+Ps2bOGYRhGv379DE9PT5u8cuLff/81KlWqZEgyAgMDje7duxsff/yxkZCQkCm2efPmRo0aNYxLly6ZbRkZGUbDhg2NihUrmm3Wxy00NNTm/hwwYIBRqFAhIykpyWyrVq2a8dBDD2Uaa/369YYkY/369WbbQw89ZEgyFi5caLYdOHDAkGQ4OTkZv/zyi9n+ww8/GJKMOXPmmG09e/Y0ypQpY/zzzz82Y3Xq1Mnw8vIy/v33X5uxq1SpYqSmpppxkydPNiQZu3fvNtvCw8ONwMDATPnbExgYaISHh9vtnzhxYqZ/25KMYcOGmde9vLxsnltZsZeX9dzuvfde83yv78vqPp8wYYLZlpqaatSqVcvw8fExn5fWx/zw4cM3PKa93KyvEdc+ZtZxTp8+bbbt3LnTcHJyMrp27Wq2WZ9vzz33nM0xn3jiCaNkyZKZxgIAwDCynutez8vLy6hdu7Z5/fo5nvW9+9SpU3aPsW3btkzvcVbW99pZs2Zl2XftPMn6vnrPPfcYKSkpZvvixYsNScbkyZPNtsDAQKNbt243PGZ2uXXr1s3mPXvZsmWGJOPdd9+1ievQoYNhsViM33//3WyTZLi4uNi07dy505BkTJ06NdNY15o0aZIhyfjss8/MtsuXLxvBwcFG0aJFbc79RnOrawUGBhotWrQwTp06ZZw6dcrYvXu38eyzz2b63LJx40ZDkrFgwQKb269atcqmPTk52XB1dTVeffVVm7jIyEjDYrEYR48etRn72sdj1KhRhoeHh/Hbb7/Z3PaNN94wChUqZBw7dswwDMNYsmSJIcnYt2+fYRiGsXz5csPV1dV47LHHjI4dO5q3u//++40nnngi2/PPyeex+fPnG05OTsbGjRtt2mfNmmVIMjZt2mS25fQxbtOmjVGkSBHj77//NtsOHTpkODs72zyXDMMwPDw8svx3m9O5Xk6ej/Zc/xjdzGearFz7udmqW7duhoeHh2EYV583zZs3Nwzj6mdAPz8/Y8SIEVk+Ttbn/pIlS+yOV7NmTaN48eKZxn/sscds4l566SVDkrFz585s87fman2+/P7778b48eMNi8ViVK9e3bxPzp07Z3h7exu9evWyuX18fLzh5eVl0x4cHGzUrVvXJm7r1q2GJOPTTz+1Gfva156cPicTExMNScaMGTMMwzCMpKQkw8nJyXjyyScNX19f83avvPKKUaJECfMcHn/8caNatWrZ3h/AjbASHUC2OnfurN9//13btm0z/3uzXz+7VtGiRSXJZvWwdHVlzLX7ovXp00fOzs767rvvcnTcSpUqqXTp0ipXrpyee+45VahQQd9//72KFCkiSYqLizO/Onf69Gn9888/+ueff3ThwgU1b95cGzZsML9W2bFjRyUmJtpsefHll18qIyMjy9UZVl9//bUyMjL01FNPmcf/559/5Ofnp4oVK2r9+vWSru4F3rBhQ23YsEGStH//fp0+fVpvvPGGDMNQTEyMpKt/Pa9evbq8vb0lSd7e3rpw4UKWK/mz4+7uri1btmjQoEGSrn5tsWfPnipTpoxefvll82ubZ86c0bp168zVH9b8T58+rbCwMB06dEh///23zbF79+5t8xXNkJAQpaenZ/kVvpwqWrSozaqeSpUqydvbW1WqVFH9+vXNduv/W79maBiGvvrqK7Vp00aGYdg8BmFhYUpOTja3Q7Hq0aOHzQpt6zZA1391MS/Zew5cy9vbW1u2bNGJEydyPU63bt3MvV1vxNnZ2WYVkIuLi1544QUlJiYqNjY21zncyMmTJxUXF6fu3burRIkSZvv999+vRx55JMvn/4svvmhzPSQkRKdPn1ZKSsptyxMAULAVLVr0hu/LkvTNN9/k+ofwXF1d1aNHjxzHd+3aVcWKFTOvd+jQQWXKlMnx3Di3vvvuOxUqVEivvPKKTfurr74qwzD0/fff27SHhoaqfPny5vX7779fnp6eN5xLfffdd/Lz89PTTz9tthUuXFivvPKKzp8/n+XWijm1evVqlS5dWqVLl1aNGjU0f/589ejRQ+PGjTNjlixZIi8vLz3yyCM2c8a6deuqaNGi5rzd09NTrVq10uLFi22+0blo0SI1aNBA//vf/+zmsWTJEoWEhKh48eI2Y4SGhio9Pd38LGCdf1qvb9y4UQ888IAeeeQRczVtUlKS9uzZY8beiiVLlqhKlSqqXLmyTV7WH6y0nrvVjR7j9PR0rVmzRm3btpW/v78ZV6FCBbVq1eqm87vRXC8vno/Xux2faaSrn6V//PFHc0vU+Pj4W/4sndVrVUREhM31l19+WZJy9Hpx4cIF8/lSoUIFvfbaa2rUqJG++eYb8z6Jjo5WUlKSnn76aZt/M4UKFVL9+vVt/s107NhRsbGxNtsnLVq0SK6urnr88cft5pHT56R1eybr82XTpk0qVKiQBg0apISEBB06dEjS1edR48aNzXPw9vbWX3/9lelbJcDNoIgOIFu1a9dW5cqVtXDhQi1YsEB+fn639Ivg58+flySbDwWSVLFiRZvrRYsWVZkyZXK85+FXX32l6OhoLVy4UA0aNFBiYqJNAdH6ZtqtWzdzkmC9fPTRR0pNTVVycrIkmXumL1q0yLz9okWLVKtWLd133312czh06JAMw1DFihUzjbF//34lJiaasSEhIYqNjdXFixe1ceNGlSlTRnXq1FHNmjXNyfLPP/9sM1F+6aWXdN9996lVq1YqW7asnnvuuRzvOenl5aXIyEgdOXJER44c0ccff6xKlSpp2rRpGjVqlCTp999/l2EYeueddzLlP2zYMEmyOQdJmT44WL+edyv72ZctWzbTPqReXl4KCAjI1HbtWKdOnVJSUpJmz56dKX/rh9Y7kf+N2HsOXCsyMlJ79uxRQECAHnzwQQ0fPvymC/tBQUE5jvX395eHh4dNm/Xf+s3uO3ozrB9MKlWqlKmvSpUq5h+6ruWIxwwAULCdP38+2/fljh07qlGjRnr++efl6+urTp06afHixTdVwLvnnntu6kdEr58bWywWVahQ4ba+L0tX35v9/f0z3R/WLUSuLypmVUQuXrz4Dd+Xjx49qooVK8rJybYkYW+cm1G/fn1FR0dr1apVGj9+vLy9vXX27Fmb+//QoUNKTk6Wj49Ppnnj+fPnbeaMHTt21PHjx82FLn/88YdiY2OzXVxjHWPVqlWZjh8aGirp/+alvr6+qlixovkZYOPGjQoJCVGTJk104sQJ/fnnn9q0aZMyMjLypIh+6NAh7d27N1Ne1rnfjebLku1jnJiYqIsXL6pChQqZ4rJqu5EbzfXy4vl4s2PmVuvWrVWsWDEtWrRICxYs0AMPPJCr+8TK3mvV9a8X5cuXl5OTU45eL9zc3BQdHa3o6GjNmTNHVapUsftZ+uGHH87072b16tU2/2aefPJJOTk5mZ+lDcPQkiVL1KpVq0y/xXWtm3lOhoSE2Dxf6tWrp3r16qlEiRLauHGjUlJStHPnTpvny+uvv66iRYvqwQcfVMWKFRUREXFLWwDhv+n2/uQ4gAKhc+fOmjlzpooVK6aOHTtmmuzejD179sjHxyfbN9DcaNKkiUqVKiVJatOmjWrUqKEuXbooNjZWTk5O5qRq3LhxqlWrVpbHsK4QdnV1NfcAnzFjhhISErRp0yaNHj062xwyMjJksVj0/fffq1ChQnaPL0mNGzdWWlqaYmJizImy9H8TggMHDujUqVM2b/w+Pj6Ki4vTDz/8oO+//17ff/+95syZo65du9r8CNONBAYG6rnnntMTTzyhe++9VwsWLNC7775r3kevvfaawsLCsrzt9ZO+rM5TUq73ss/umDcay5r/M888o27dumUZe//999/UMW8H6w/TZjeBfuqppxQSEqKlS5dq9erVGjdunMaOHauvv/46xyt6croKPaey+hFd6erqozvJEY8ZAKDg+uuvv5ScnJzt+7K7u7s2bNig9evXa+XKlVq1apUWLVqkhx9+WKtXr7b73nT9MfJadu/NOckpL+TH9+VSpUqZheqwsDBVrlxZjz76qCZPnmzuRZ6RkSEfHx8tWLAgy2Ncu+96mzZtVKRIES1evFgNGzbU4sWL5eTkpCeffDLbPDIyMvTII49o8ODBWfZfuzincePGWrt2rS5evKjY2FgNHTrU/Ebqxo0btX//fhUtWlS1a9e+qfvCXl41atRQVFRUlv3XL1y504/xjcbLi+fjzY6ZW66urmrXrp3mzZunP//8U8OHD8/1sdLS0vTbb79l+/tcVvZeG7JSqFAh8/ki/d9z5oUXXtDy5csl/d/nrPnz58vPzy/TMZyd/6+06O/vr5CQEC1evFhvvvmmfvnlFx07dkxjx47NNo+beU42btxYH374of7880/zs7T1B1E3btwof3//TH90qlKlig4ePKgVK1Zo1apV+uqrrzRjxgwNHTqU31dCjlFEB3BDnTt31tChQ3Xy5EnNnz8/18eJiYnRH3/8oWeeeSZT36FDh2x+YOn8+fM6efKkWrdufdPjFC1aVMOGDVOPHj20ePFiderUyfwKoqenp80kwZ6OHTtq3rx5Wrt2rfbv3y/DMG642qR8+fIyDENBQUHZrliXpAcffFAuLi7auHGjNm7caG610qRJE3344Ydau3atef1aLi4uatOmjdq0aaOMjAy99NJL+uCDD/TOO+/c9KqG4sWLq3z58mZR995775V09au0ObmPcupmJnG3onTp0ipWrJjS09Pzbf7nz5/X0qVLFRAQcMMfhSpTpoxeeuklvfTSS0pMTFSdOnX03nvvmUX0vMzrxIkTunDhgs1q9N9++02SVK5cOUn/tyLH+gO5VlmtFMtpbtYfvT148GCmvgMHDqhUqVKZVsgDAJCXrHNbewsIrJycnNS8eXM1b95cUVFRGj16tN566y2tX79eoaGheT7fsa78tDIMQ7///rvNgoDixYtnel+Wrr43W+d10s3NGQIDA7VmzRqdO3fOZsXrgQMHzP68EBgYqF27dikjI8NmgU5ejyNJ4eHheuihhzR69Gi98MIL8vDwUPny5bVmzRo1atTohn/g8PDw0KOPPqolS5YoKipKixYtUkhIiM3WJVkpX768zp8/n6N5aUhIiObMmaMvvvhC6enpatiwoZycnMyi4P79+9WwYcM8+eNI+fLltXPnTjVv3jxP/t36+PjIzc1Nv//+e6a+rNryYswbPR/zk86dO+uTTz6Rk5NTtj9EeyNffvmlLl68mOVr1aFDh2y+hfr7778rIyPDnMffjDJlymjAgAEaMWKEfvnlFzVo0MD8LO3j45Pjz9IvvfSSDh48qEWLFqlIkSJq06ZNtre5meektTgeHR2tbdu26Y033pB09bPzzJkzzW/Z1q1b1+Z2Hh4e6tixozp27KjLly+rXbt2eu+99zRkyBC5ubnd8LwAtnMBcEPly5fXpEmTNGbMGD344IO5OsbRo0fVvXt3ubi4mAXja82ePdv8xW1Jmjlzpq5cuZKrffQkqUuXLipbtqz5F++6deuqfPnyGj9+vLmdxrVOnTplcz00NFQlSpTQokWLtGjRIj344IM33B6jXbt2KlSokEaMGJFp1YJhGDa/YO/m5qYHHnhAn3/+uY4dO2azEv3ixYuaMmWKypcvrzJlypi3ufb20tXJo/WDlHVf86zs3LlT//zzT6b2o0ePat++feZWGj4+PmratKk++OADnTx5MlP89fdRTnl4eGT5AS+vFSpUSO3bt9dXX31l/mHgWreSv3Wrn1tx8eJFPfvsszpz5ozeeuutbFePXT+ej4+P/P39bR7nvMpLkq5cuaIPPvjAvH758mV98MEHKl26tDn5tE6erfsPWnOdPXt2puPlNLcyZcqoVq1amjdvns2/kT179mj16tW5+iMaAAA5tW7dOo0aNUpBQUHq0qWL3bgzZ85karN+s9H63mz9o29ezXk+/fRTm72Pv/zyS508edJmbly+fHn98ssvunz5stm2YsUKHT9+3OZYN5Nb69atlZ6ermnTptm0T5w4URaLJddz86zGiY+Pt9k+8cqVK5o6daqKFi2qhx56KE/GsXr99dd1+vRpffjhh5KufusvPT3d3NbwWleuXMl0X3Xs2FEnTpzQRx99pJ07d95wcY11jJiYGP3www+Z+pKSknTlyhXzuvWzwNixY3X//feb2xaGhIRo7dq12r59e55s5WLN6++//zbvi2tdvHgx01Z6N2Jdybxs2TKb3/P5/fffM+2hL936Z4OcPB/zk2bNmmnUqFGaNm1alqu4c2Lnzp3q37+/ihcvnmn/c0maPn26zfWpU6dKUq6fry+//LKKFCmi999/X9LVPzJ6enpq9OjRNp/Zra7/nNW+fXsVKlRIn3/+uZYsWaJHH330hgtjbuY5GRQUpHvuuUcTJ05UWlqaGjVqJOnq8+WPP/7Ql19+qQYNGtiskL/+s7SLi4uqVq0qwzCyPCcgK6xEB5Aj/fr1y3Hsr7/+qs8++0wZGRlKSkrStm3b9NVXX8lisWj+/PmZttSQrhbtmjdvrqeeekoHDx7UjBkz1LhxYz322GO5yrdw4cLq16+fBg0apFWrVqlly5b66KOP1KpVK1WrVk09evTQPffco7///lvr16+Xp6envv32W5vbt2vXTl988YUuXLig8ePH33DM8uXL691339WQIUN05MgRtW3bVsWKFdPhw4e1dOlS9e7dW6+99poZHxISovfff19eXl6qUaOGpKvF0kqVKungwYPq3r27zfGff/55nTlzRg8//LDKli2ro0ePaurUqapVq1a2q5qjo6M1bNgwPfbYY2rQoIGKFi2qP//8U5988olSU1NtvlY4ffp0NW7cWDVq1FCvXr107733KiEhQTExMfrrr7+0c+fOHD4C/6du3bqaOXOm3n33XVWoUEE+Pj63tK9+dt5//32tX79e9evXV69evVS1alWdOXNGv/76q9asWZPlpPtG6tatq0WLFmngwIF64IEHVLRo0RuupPj777/12WefSbq6+nzfvn1asmSJ4uPj9eqrr9r8iOf1zp07p7Jly6pDhw6qWbOmihYtqjVr1mjbtm2aMGHCLeVlj7+/v8aOHasjR47ovvvu06JFixQXF6fZs2ebP/hbrVo1NWjQQEOGDNGZM2dUokQJffHFFzYfAHOT27hx49SqVSsFBwerZ8+eunjxoqZOnSovL69b+sorAADX+v7773XgwAFduXJFCQkJWrdunaKjoxUYGKjly5dnuwpx5MiR2rBhg8LDwxUYGKjExETNmDFDZcuWVePGjSVdnQd6e3tr1qxZKlasmDw8PFS/fv2b+o2Sa5UoUUKNGzdWjx49lJCQoEmTJqlChQrq1auXGfP888/ryy+/VMuWLfXUU0/pjz/+0GeffWbzI5A3m1ubNm3UrFkzvfXWWzpy5Ihq1qyp1atX65tvvlH//v0zHTu3evfurQ8++EDdu3dXbGysypUrpy+//FKbNm3SpEmTst2jPjdatWql6tWrKyoqShEREXrooYf0wgsvaMyYMYqLi1OLFi1UuHBhHTp0SEuWLNHkyZPVoUMH8/bWva1fe+01c+HGjQwaNEjLly/Xo48+qu7du6tu3bq6cOGCdu/erS+//FJHjhwxt6OsUKGC/Pz8dPDgQfNHIaWrK2tff/11SbqpIvratWt16dKlTO1t27bVs88+q8WLF+vFF1/U+vXr1ahRI6Wnp+vAgQNavHixfvjhB9WrVy/HY0nS8OHDtXr1ajVq1Eh9+vQx/xBTvXp1xcXF2cTWrVtXa9asUVRUlPz9/RUUFKT69evneKycPB/zEycnJ7399ts5jt+4caMuXbqk9PR0nT59Wps2bdLy5cvl5eWlpUuXZlmIP3z4sB577DG1bNlSMTEx+uyzz9S5c2fVrFkzVzmXLFlSPXr00IwZM7R//35VqVJFM2fO1LPPPqs6deqoU6dOKl26tI4dO6aVK1eqUaNGNn948/HxUbNmzRQVFaVz587l6I9ON/ucDAkJ0RdffKEaNWqY35itU6eOPDw89Ntvv2X6AdcWLVrIz89PjRo1kq+vr/bv369p06YpPDw8z19vUIAZAP7T5syZY0gytm3bZrYNGzbMkGScOnUq29tKMiIiIszrhw8fNiSZF2dnZ6NEiRJG/fr1jSFDhhhHjx61O/5PP/1k9O7d2yhevLhRtGhRo0uXLsbp06dvmH92uSYnJxteXl7GQw89ZLbt2LHDaNeunVGyZEnD1dXVCAwMNJ566ilj7dq1mW4fHR1tSDIsFotx/Phxu2Nf76uvvjIaN25seHh4GB4eHkblypWNiIgI4+DBgzZxK1euNCQZrVq1sml//vnnDUnGxx9/bNP+5ZdfGi1atDB8fHwMFxcX43//+5/xwgsvGCdPnsz2Pvrzzz+NoUOHGg0aNDB8fHwMZ2dno3Tp0kZ4eLixbt26TPF//PGH0bVrV8PPz88oXLiwcc899xiPPvqo8eWXX5oxWf27MQzDWL9+vSHJWL9+vdkWHx9vhIeHG8WKFTMkmY9HVrEPPfSQUa1atUw5BQYGGuHh4Znar/83aBiGkZCQYERERBgBAQFG4cKFDT8/P6N58+bG7NmzM+W5ZMkSm9ta/w3PmTPHbDt//rzRuXNnw9vb25BkBAYGZsrj+lytzwGLxWJ4enoa1apVM3r16mVs2bIly9tIMoYNG2YYhmGkpqYagwYNMmrWrGkUK1bM8PDwMGrWrGnMmDHD5jb28rJ3btf2ZXWfb9++3QgODjbc3NyMwMBAY9q0aZlu/8cffxihoaGGq6ur4evra7z55pvm8+TaY9rLLav71zAMY82aNUajRo0Md3d3w9PT02jTpo2xb98+mxh7z3Xrv8XDhw9ned8CAP7brO8T1ouLi4vh5+dnPPLII8bkyZONlJSUTLe5fo63du1a4/HHHzf8/f0NFxcXw9/f33j66aeN3377zeZ233zzjVG1alXD2dnZ5v3O3vzG2nftXNX6Xv35558bQ4YMMXx8fAx3d3cjPDw8y7n0hAkTjHvuucdwdXU1GjVqZGzfvj3TMbPLrVu3bpnmNufOnTMGDBhg+Pv7G4ULFzYqVqxojBs3zsjIyLCJy2oeZhhX50LdunXL8nyvlZCQYPTo0cMoVaqU4eLiYtSoUSPTHMF6vKzmgVnJLnbu3LmZ5iGzZ8826tata7i7uxvFihUzatSoYQwePNg4ceJEptt36dLFkGSEhobaHfv68z537pwxZMgQo0KFCoaLi4tRqlQpo2HDhsb48eONy5cv28Q++eSThiRj0aJFZtvly5eNIkWKGC4uLsbFixdveP7Xfx67/jJ//nzzuGPHjjWqVatmuLq6GsWLFzfq1q1rjBgxwkhOTjaPdzOP8dq1a43atWsbLi4uRvny5Y2PPvrIePXVVw03NzebuAMHDhhNmjQx3N3dDUnmcXI618vp8zEr1+d9M59pspJVzt26dTM8PDyyvZ31cRo3blymMa2XwoULG6VLlzaaNGlivPfee0ZiYqLd8fft22d06NDBKFasmFG8eHGjb9++Ofr3kl2uf/zxh1GoUCGb+2v9+vVGWFiY4eXlZbi5uRnly5c3unfvbmzfvj3T7T/88ENDklGsWLEsc8nqtccwcv6cnD59uiHJ6NOnj017aGioISnT5/sPPvjAaNKkiVkHKF++vDFo0CCbf+/AjVgMg1/iAgAAAAAAQN5p27at9u7dm2mPfwC4G7EnOgAAAAAAAHLt4sWLNtcPHTqk7777Tk2bNnVMQgCQx1iJDgAAAAAAgFwrU6aMunfvrnvvvVdHjx7VzJkzlZqaqh07dqhixYqOTg8Abhk/LAoAAAAAAIBca9mypT7//HPFx8fL1dVVwcHBGj16NAV0AAUGK9EBAAAAAAAAALCDPdEBAAAAAAAAALCDIjoAAAAAAAAAAHawJ3oeycjI0IkTJ1SsWDFZLBZHpwMAAIB8zDAMnTt3Tv7+/nJyYl1LXmNuDgAAgJzI6bycInoeOXHihAICAhydBgAAAO4ix48fV9myZR2dRoHD3BwAAAA340bzcoroeaRYsWKSrt7hnp6eDs4GAAAA+VlKSooCAgLMOSTyFnNzAAAA5ERO5+UU0fOI9Wuinp6eTNQBAACQI2w1cnswNwcAAMDNuNG8nA0YAQAAAAAAAACwgyI6AAAAAAAAAAB2UEQHAAAAAAAAAMAOiugAAAAAAAAAANhBER0AAAAAAAAAADsoogMAAAAAAAAAYAdFdAAAAAAAAAAA7KCIDgAAAAAAAACAHRTRAQAAAGjDhg1q06aN/P39ZbFYtGzZMpt+wzA0dOhQlSlTRu7u7goNDdWhQ4dsYs6cOaMuXbrI09NT3t7e6tmzp86fP28Ts2vXLoWEhMjNzU0BAQGKjIzMlMuSJUtUuXJlubm5qUaNGvruu+/y/HwBAACAnKKIDgAAAEAXLlxQzZo1NX369Cz7IyMjNWXKFM2aNUtbtmyRh4eHwsLCdOnSJTOmS5cu2rt3r6Kjo7VixQpt2LBBvXv3NvtTUlLUokULBQYGKjY2VuPGjdPw4cM1e/ZsM2bz5s16+umn1bNnT+3YsUNt27ZV27ZttWfPntt38gAAAEA2LIZhGI5OoiBISUmRl5eXkpOT5enp6eh0AAAAkI/l97mjxWLR0qVL1bZtW0lXV6H7+/vr1Vdf1WuvvSZJSk5Olq+vr+bOnatOnTpp//79qlq1qrZt26Z69epJklatWqXWrVvrr7/+kr+/v2bOnKm33npL8fHxcnFxkSS98cYbWrZsmQ4cOCBJ6tixoy5cuKAVK1aY+TRo0EC1atXSrFmzcpR/fr9/AQAAkD/kdN7ISnQAAAAA2Tp8+LDi4+MVGhpqtnl5eal+/fqKiYmRJMXExMjb29ssoEtSaGionJyctGXLFjOmSZMmZgFdksLCwnTw4EGdPXvWjLl2HGuMdRwAAADgTnN2dAIAAAAA8rf4+HhJkq+vr027r6+v2RcfHy8fHx+bfmdnZ5UoUcImJigoKNMxrH3FixdXfHx8tuNkJTU1Vampqeb1lJSUmzk9AAAAIFusRAcAAABwVxszZoy8vLzMS0BAgKNTAgAAQAFCER0AAABAtvz8/CRJCQkJNu0JCQlmn5+fnxITE236r1y5ojNnztjEZHWMa8ewF2Ptz8qQIUOUnJxsXo4fP36zpwgAAADYRREdAAAAQLaCgoLk5+entWvXmm0pKSnasmWLgoODJUnBwcFKSkpSbGysGbNu3TplZGSofv36ZsyGDRuUlpZmxkRHR6tSpUoqXry4GXPtONYY6zhZcXV1laenp80FAAAAyCsU0QEAAADo/PnziouLU1xcnKSrPyYaFxenY8eOyWKxqH///nr33Xe1fPly7d69W127dpW/v7/atm0rSapSpYpatmypXr16aevWrdq0aZP69u2rTp06yd/fX5LUuXNnubi4qGfPntq7d68WLVqkyZMna+DAgWYe/fr106pVqzRhwgQdOHBAw4cP1/bt29W3b987fZcAAAAAkvhh0QLh/R3/ODoFAPnUG7VLOTqFfCFtxKuOTgFAPlZ42ARHp5AvbN++Xc2aNTOvWwvb3bp109y5czV48GBduHBBvXv3VlJSkho3bqxVq1bJzc3NvM2CBQvUt29fNW/eXE5OTmrfvr2mTJli9nt5eWn16tWKiIhQ3bp1VapUKQ0dOlS9e/c2Yxo2bKiFCxfq7bff1ptvvqmKFStq2bJlql69+h24F/KZ423y9ngB3+bt8QAAAP4jLIZhGI5OoiBISUmRl5eXkpOT7/jXRymiA7CHIvpVFNEBZMcRRXRHzh3/CwrM/UsRHQAA4LbK6byR7VwAAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdDi2ib9iwQW3atJG/v78sFouWLVtm9qWlpen1119XjRo15OHhIX9/f3Xt2lUnTpywOcaZM2fUpUsXeXp6ytvbWz179tT58+dtYnbt2qWQkBC5ubkpICBAkZGRmXJZsmSJKleuLDc3N9WoUUPffffdbTlnAAAAAAAAAMDdw6FF9AsXLqhmzZqaPn16pr5///1Xv/76q9555x39+uuv+vrrr3Xw4EE99thjNnFdunTR3r17FR0drRUrVmjDhg3q3bu32Z+SkqIWLVooMDBQsbGxGjdunIYPH67Zs2ebMZs3b9bTTz+tnj17aseOHWrbtq3atm2rPXv23L6TBwAAAAAAAADkexbDMAxHJyFJFotFS5cuVdu2be3GbNu2TQ8++KCOHj2q//3vf9q/f7+qVq2qbdu2qV69epKkVatWqXXr1vrrr7/k7++vmTNn6q233lJ8fLxcXFwkSW+88YaWLVumAwcOSJI6duyoCxcuaMWKFeZYDRo0UK1atTRr1qwc5Z+SkiIvLy8lJyfL09Mzl/dC7ry/4587Oh6Au8cbtUs5OoV8IW3Eq45OAUA+VnjYhDs+piPnjv8FBeb+Pd4mb48X8G3eHg8AAOAul9N54121J3pycrIsFou8vb0lSTExMfL29jYL6JIUGhoqJycnbdmyxYxp0qSJWUCXpLCwMB08eFBnz541Y0JDQ23GCgsLU0xMzG0+IwAAAAAAAABAfubs6ARy6tKlS3r99df19NNPm38ViI+Pl4+Pj02cs7OzSpQoofj4eDMmKCjIJsbX19fsK168uOLj4822a2Osx8hKamqqUlNTzespKSm5PzkAAAAAAAAAQL50V6xET0tL01NPPSXDMDRz5kxHpyNJGjNmjLy8vMxLQECAo1MCAAAAAAAAAOSxfF9EtxbQjx49qujoaJu9afz8/JSYmGgTf+XKFZ05c0Z+fn5mTEJCgk2M9fqNYqz9WRkyZIiSk5PNy/Hjx3N/kgAAAAAAAACAfClfF9GtBfRDhw5pzZo1KlmypE1/cHCwkpKSFBsba7atW7dOGRkZql+/vhmzYcMGpaWlmTHR0dGqVKmSihcvbsasXbvW5tjR0dEKDg62m5urq6s8PT1tLgAAAAAAAACAgsWhRfTz588rLi5OcXFxkqTDhw8rLi5Ox44dU1pamjp06KDt27drwYIFSk9PV3x8vOLj43X58mVJUpUqVdSyZUv16tVLW7du1aZNm9S3b1916tRJ/v7+kqTOnTvLxcVFPXv21N69e7Vo0SJNnjxZAwcONPPo16+fVq1apQkTJujAgQMaPny4tm/frr59+97x+wQAAAAAAAAAkH84tIi+fft21a5dW7Vr15YkDRw4ULVr19bQoUP1999/a/ny5frrr79Uq1YtlSlTxrxs3rzZPMaCBQtUuXJlNW/eXK1bt1bjxo01e/Zss9/Ly0urV6/W4cOHVbduXb366qsaOnSoevfubcY0bNhQCxcu1OzZs1WzZk19+eWXWrZsmapXr37n7gwAAAAAAAAAQL7j7MjBmzZtKsMw7PZn12dVokQJLVy4MNuY+++/Xxs3bsw25sknn9STTz55w/EAAAAAAAAAAP8d+XpPdAAAAAAAAAAAHIkiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAADcUHp6ut555x0FBQXJ3d1d5cuX16hRo2QYhhljGIaGDh2qMmXKyN3dXaGhoTp06JDNcc6cOaMuXbrI09NT3t7e6tmzp86fP28Ts2vXLoWEhMjNzU0BAQGKjIy8I+cIAAAAZIUiOgAAAIAbGjt2rGbOnKlp06Zp//79Gjt2rCIjIzV16lQzJjIyUlOmTNGsWbO0ZcsWeXh4KCwsTJcuXTJjunTpor179yo6OlorVqzQhg0b1Lt3b7M/JSVFLVq0UGBgoGJjYzVu3DgNHz5cs2fPvqPnCwAAAFg5OzoBAAAAAPnf5s2b9fjjjys8PFySVK5cOX3++efaunWrpKur0CdNmqS3335bjz/+uCTp008/la+vr5YtW6ZOnTpp//79WrVqlbZt26Z69epJkqZOnarWrVtr/Pjx8vf314IFC3T58mV98skncnFxUbVq1RQXF6eoqCibYjsAAABwpzh0JfqGDRvUpk0b+fv7y2KxaNmyZTb9d/LroEuWLFHlypXl5uamGjVq6Lvvvsvz8wUAAADuVg0bNtTatWv122+/SZJ27typn3/+Wa1atZIkHT58WPHx8QoNDTVv4+Xlpfr16ysmJkaSFBMTI29vb7OALkmhoaFycnLSli1bzJgmTZrIxcXFjAkLC9PBgwd19uzZ236eAAAAwPUcWkS/cOGCatasqenTp2fZf6e+Drp582Y9/fTT6tmzp3bs2KG2bduqbdu22rNnz+07eQAAAOAu8sYbb6hTp06qXLmyChcurNq1a6t///7q0qWLJCk+Pl6S5Ovra3M7X19fsy8+Pl4+Pj42/c7OzipRooRNTFbHuHaM66WmpiolJcXmAgAAAOQVh27n0qpVK3PlyvXu5NdBJ0+erJYtW2rQoEGSpFGjRik6OlrTpk3TrFmz7sA9AQAAAORvixcv1oIFC7Rw4UJzTt2/f3/5+/urW7duDs1tzJgxGjFihENzAAAAQMGVb39Y9E5+HTQmJsZmHGuMdRwAAADgv27QoEHmavQaNWro2Wef1YABAzRmzBhJkp+fnyQpISHB5nYJCQlmn5+fnxITE236r1y5ojNnztjEZHWMa8e43pAhQ5ScnGxejh8/fotnCwAAAPyffFtEv5NfB7UXY+/rohJfGQUAAMB/y7///isnJ9uPD4UKFVJGRoYkKSgoSH5+flq7dq3Zn5KSoi1btig4OFiSFBwcrKSkJMXGxpox69atU0ZGhurXr2/GbNiwQWlpaWZMdHS0KlWqpOLFi2eZm6urqzw9PW0uAAAAQF7Jt0X0/G7MmDHy8vIyLwEBAY5OCQAAALht2rRpo/fee08rV67UkSNHtHTpUkVFRemJJ56QJFksFvXv31/vvvuuli9frt27d6tr167y9/dX27ZtJUlVqlRRy5Yt1atXL23dulWbNm1S37591alTJ/n7+0uSOnfuLBcXF/Xs2VN79+7VokWLNHnyZA0cONBRpw4AAID/OIfuiZ6da78OWqZMGbM9ISFBtWrVMmPy4uug9mLsfV1UuvqV0Wsn8ikpKRTSAQAAUGBNnTpV77zzjl566SUlJibK399fL7zwgoYOHWrGDB48WBcuXFDv3r2VlJSkxo0ba9WqVXJzczNjFixYoL59+6p58+ZycnJS+/btNWXKFLPfy8tLq1evVkREhOrWratSpUpp6NCh5u8ZAQAAAHdavi2iX/t1UGvR3Pp10D59+kiy/Tpo3bp1JWX9ddC33npLaWlpKly4sKTMXwcNDg7W2rVr1b9/f3P86Oho82unWXF1dZWrq2tenzYAAACQLxUrVkyTJk3SpEmT7MZYLBaNHDlSI0eOtBtTokQJLVy4MNux7r//fm3cuDG3qQIAAAB5yqHbuZw/f15xcXGKi4uTdPXHROPi4nTs2LE7+nXQfv36adWqVZowYYIOHDig4cOHa/v27erbt++dvksAAAAAAAAAAPmIQ1eib9++Xc2aNTOvWwvb3bp109y5c+/Y10EbNmyohQsX6u2339abb76pihUratmyZapevfoduBcAAAAAAAAAAPmVxTAMw9FJFAQpKSny8vJScnKyPD097+jY7+/4546OB+Du8UbtUo5OIV9IG/Gqo1MAkI8VHjbhjo/pyLnjf0GBuX+Pt8nb4wV8m7fHAwAAuMvldN7o0O1cAAAAAAAAAADIzyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsyNdF9PT0dL3zzjsKCgqSu7u7ypcvr1GjRskwDDPGMAwNHTpUZcqUkbu7u0JDQ3Xo0CGb45w5c0ZdunSRp6envL291bNnT50/f94mZteuXQoJCZGbm5sCAgIUGRl5R84RAAAAAAAAAJB/5esi+tixYzVz5kxNmzZN+/fv19ixYxUZGampU6eaMZGRkZoyZYpmzZqlLVu2yMPDQ2FhYbp06ZIZ06VLF+3du1fR0dFasWKFNmzYoN69e5v9KSkpatGihQIDAxUbG6tx48Zp+PDhmj179h09XwAAAAAAAABA/uLs6ASys3nzZj3++OMKDw+XJJUrV06ff/65tm7dKunqKvRJkybp7bff1uOPPy5J+vTTT+Xr66tly5apU6dO2r9/v1atWqVt27apXr16kqSpU6eqdevWGj9+vPz9/bVgwQJdvnxZn3zyiVxcXFStWjXFxcUpKirKptgOAAAAAAAAAPhvydcr0Rs2bKi1a9fqt99+kyTt3LlTP//8s1q1aiVJOnz4sOLj4xUaGmrexsvLS/Xr11dMTIwkKSYmRt7e3mYBXZJCQ0Pl5OSkLVu2mDFNmjSRi4uLGRMWFqaDBw/q7NmzWeaWmpqqlJQUmwsAAAAAAAAAoGDJ1yvR33jjDaWkpKhy5coqVKiQ0tPT9d5776lLly6SpPj4eEmSr6+vze18fX3Nvvj4ePn4+Nj0Ozs7q0SJEjYxQUFBmY5h7StevHim3MaMGaMRI0bkwVkCAAAAAAAAAPKrfL0SffHixVqwYIEWLlyoX3/9VfPmzdP48eM1b948R6emIUOGKDk52bwcP37c0SkBAAAAAAAAAPJYvl6JPmjQIL3xxhvq1KmTJKlGjRo6evSoxowZo27dusnPz0+SlJCQoDJlypi3S0hIUK1atSRJfn5+SkxMtDnulStXdObMGfP2fn5+SkhIsImxXrfGXM/V1VWurq63fpIAAAAAAAAAgHwrX69E//fff+XkZJtioUKFlJGRIUkKCgqSn5+f1q5da/anpKRoy5YtCg4OliQFBwcrKSlJsbGxZsy6deuUkZGh+vXrmzEbNmxQWlqaGRMdHa1KlSpluZULAAAAAAAAAOC/IV8X0du0aaP33ntPK1eu1JEjR7R06VJFRUXpiSeekCRZLBb1799f7777rpYvX67du3era9eu8vf3V9u2bSVJVapUUcuWLdWrVy9t3bpVmzZtUt++fdWpUyf5+/tLkjp37iwXFxf17NlTe/fu1aJFizR58mQNHDjQUacOAAAAAAAAAMgH8vV2LlOnTtU777yjl156SYmJifL399cLL7ygoUOHmjGDBw/WhQsX1Lt3byUlJalx48ZatWqV3NzczJgFCxaob9++at68uZycnNS+fXtNmTLF7Pfy8tLq1asVERGhunXrqlSpUho6dKh69+59R88XAAAAAAAAAJC/WAzDMBydREGQkpIiLy8vJScny9PT846O/f6Of+7oeADuHm/ULuXoFPKFtBGvOjoFAPlY4WET7viYjpw7/hcUmPv3eJu8PV7At3l7PAAAgLtcTueN+Xo7FwAAAAAAAAAAHIkiOgAAAAAAAAAAdlBEBwAAAJAjf//9t5555hmVLFlS7u7uqlGjhrZv3272G4ahoUOHqkyZMnJ3d1doaKgOHTpkc4wzZ86oS5cu8vT0lLe3t3r27Knz58/bxOzatUshISFyc3NTQECAIiMj78j5AQAAAFmhiA4AAADghs6ePatGjRqpcOHC+v7777Vv3z5NmDBBxYsXN2MiIyM1ZcoUzZo1S1u2bJGHh4fCwsJ06dIlM6ZLly7au3evoqOjtWLFCm3YsEG9e/c2+1NSUtSiRQsFBgYqNjZW48aN0/DhwzV79uw7er4AAACAlbOjEwAAAACQe3/++afuvffe2z7O2LFjFRAQoDlz5phtQUFB5v8bhqFJkybp7bff1uOPPy5J+vTTT+Xr66tly5apU6dO2r9/v1atWqVt27apXr16kqSpU6eqdevWGj9+vPz9/bVgwQJdvnxZn3zyiVxcXFStWjXFxcUpKirKptgOAAAA3CmsRAcAAADuYhUqVFCzZs302Wef2az4zmvLly9XvXr19OSTT8rHx0e1a9fWhx9+aPYfPnxY8fHxCg0NNdu8vLxUv359xcTESJJiYmLk7e1tFtAlKTQ0VE5OTtqyZYsZ06RJE7m4uJgxYWFhOnjwoM6ePZtlbqmpqUpJSbG5AAAAAHmFIjoAAABwF/v11191//33a+DAgfLz89MLL7ygrVu35vk4f/75p2bOnKmKFSvqhx9+UJ8+ffTKK69o3rx5kqT4+HhJkq+vr83tfH19zb74+Hj5+PjY9Ds7O6tEiRI2MVkd49oxrjdmzBh5eXmZl4CAgFs8WwAAAOD/UEQHAAAA7mK1atXS5MmTdeLECX3yySc6efKkGjdurOrVqysqKkqnTp3Kk3EyMjJUp04djR49WrVr11bv3r3Vq1cvzZo1K0+OfyuGDBmi5ORk83L8+HFHpwQAAIAChCI6AAAAUAA4OzurXbt2WrJkicaOHavff/9dr732mgICAtS1a1edPHnylo5fpkwZVa1a1aatSpUqOnbsmCTJz89PkpSQkGATk5CQYPb5+fkpMTHRpv/KlSs6c+aMTUxWx7h2jOu5urrK09PT5gIAAADkFYroAAAAQAGwfft2vfTSSypTpoyioqL02muv6Y8//lB0dLROnDhh/thnbjVq1EgHDx60afvtt98UGBgo6eqPjPr5+Wnt2rVmf0pKirZs2aLg4GBJUnBwsJKSkhQbG2vGrFu3ThkZGapfv74Zs2HDBqWlpZkx0dHRqlSpkooXL35L5wAAAADkBkV0AAAA4C4WFRWlGjVqqGHDhjpx4oQ+/fRTHT16VO+++66CgoIUEhKiuXPn6tdff72lcQYMGKBffvlFo0eP1u+//66FCxdq9uzZioiIkCRZLBb1799f7777rpYvX67du3era9eu8vf3V9u2bSVdXbnesmVL9erVS1u3btWmTZvUt29fderUSf7+/pKkzp07y8XFRT179tTevXu1aNEiTZ48WQMHDryl/AEAAIDccnZ0AgAAAAByb+bMmXruuefUvXt3lSlTJssYHx8fffzxx7c0zgMPPKClS5dqyJAhGjlypIKCgjRp0iR16dLFjBk8eLAuXLig3r17KykpSY0bN9aqVavk5uZmxixYsEB9+/ZV8+bN5eTkpPbt22vKlClmv5eXl1avXq2IiAjVrVtXpUqV0tChQ9W7d+9byh8AAADILYthGIajkygIUlJS5OXlpeTk5Du+B+P7O/65o+MBuHu8UbuUo1PIF9JGvOroFADkY4WHTbjjYzpy7vhfUGDu3+Nt8vZ4Ad/m7fEAAADucjmdN7KdCwAAAHAXmzNnjpYsWZKpfcmSJZo3b54DMgIAAAAKForoAAAAwF1szJgxKlUq8zePfHx8NHr0aAdkBAAAABQsFNEBAACAu9ixY8cUFBSUqT0wMFDHjh1zQEYAAABAwUIRHQAAALiL+fj4aNeuXZnad+7cqZIlSzogIwAAAKBgoYgOAAAA3MWefvppvfLKK1q/fr3S09OVnp6udevWqV+/furUqZOj0wMAAADues6OTgAAAABA7o0aNUpHjhxR8+bN5ex8dXqfkZGhrl27sic6AAAAkAdytRL93nvv1enTpzO1JyUl6d57773lpAAAAADkjIuLixYtWqQDBw5owYIF+vrrr/XHH3/ok08+kYuLi6PTAwAAAO56uVqJfuTIEaWnp2dqT01N1d9//33LSQEAAAC4Offdd5/uu+8+R6cBAAAAFDg3VURfvny5+f8//PCDvLy8zOvp6elau3atypUrl2fJAQAAAMheenq65s6dq7Vr1yoxMVEZGRk2/evWrXNQZgAAAEDBcFNF9LZt20qSLBaLunXrZtNXuHBhlStXThMmTMiz5AAAAABkr1+/fpo7d67Cw8NVvXp1WSwWR6cEAAAAFCg3VUS3rmoJCgrStm3bVKpUqduSFAAAAICc+eKLL7R48WK1bt3a0akAAAAABVKu9kQ/fPhwXucBAAAAIBdcXFxUoUIFR6cBAAAAFFi5KqJL0tq1a+3uu/jJJ5/ccmIAAAAAbuzVV1/V5MmTNW3aNLZyAQAAAG6DXBXRR4wYoZEjR6pevXoqU6YMk3UAAADAQX7++WetX79e33//vapVq6bChQvb9H/99dcOygwAAAAoGHJVRJ81a5bmzp2rZ599Nq/zAQAAAHATvL299cQTTzg6DQAAAKDAylUR/fLly2rYsGFe5wIAAADgJs2ZM8fRKQAAAAAFmlNubvT8889r4cKFeZ0LAAAAgFy4cuWK1qxZow8++EDnzp2TJJ04cULnz593cGYAAADA3S9XK9EvXbqk2bNna82aNbr//vsz7bsYFRWVJ8kBAAAAyN7Ro0fVsmVLHTt2TKmpqXrkkUdUrFgxjR07VqmpqZo1a5ajUwQAAADuarkqou/atUu1atWSJO3Zs8emjx8ZBQAAAO6cfv36qV69etq5c6dKlixptj/xxBPq1auXAzMDAAAACoZcFdHXr1+f13kAAAAAyIWNGzdq8+bNcnFxsWkvV66c/v77bwdlBQAAABQcudoTHQAAAED+kJGRofT09Eztf/31l4oVK+aAjAAAAICCJVcr0Zs1a5btti3r1q3LdUIAAAAAcq5FixaaNGmSZs+eLenq9ornz5/XsGHD1Lp1awdnBwAAANz9clVEt+6HbpWWlqa4uDjt2bNH3bp1y4u8AAAAAOTAhAkTFBYWpqpVq+rSpUvq3LmzDh06pFKlSunzzz93dHoAAADAXS9XRfSJEydm2T58+HCdP3/+lhICAAAAkHNly5bVzp079cUXX2jXrl06f/68evbsqS5dusjd3d3R6QEAAAB3vVwV0e155pln9OCDD2r8+PF5eVgAAAAA2XB2dtYzzzzj6DSQ3x1vk7fHC/g2b48HAACQT+VpET0mJkZubm55eUgAAAAA2fj000+z7e/atesdygQAAAAomHJVRG/Xrp3NdcMwdPLkSW3fvl3vvPNOniQGAAAA4Mb69etncz0tLU3//vuvXFxcVKRIEYroAAAAwC3KVRHdy8vL5rqTk5MqVaqkkSNHqkWLFnmSGAAAAIAbO3v2bKa2Q4cOqU+fPho0aJADMgIAAAAKllwV0efMmZPXeQAAAADIIxUrVtT777+vZ555RgcOHHB0OgAAAMBd7Zb2RI+NjdX+/fslSdWqVVPt2rXzJCkAAAAAt8bZ2VknTpxwdBoAAADAXS9XRfTExER16tRJP/74o7y9vSVJSUlJatasmb744guVLl06L3MEAAAAYMfy5cttrlt/r2jatGlq1KiRg7ICAAAACo5cFdFffvllnTt3Tnv37lWVKlUkSfv27VO3bt30yiuv6PPPP8/TJAEAAABkrW3btjbXLRaLSpcurYcfflgTJkxwTFIAAABAAZKrIvqqVau0Zs0as4AuSVWrVtX06dP5YVEAAADgDsrIyHB0CgAAAECB5pSbG2VkZKhw4cKZ2gsXLswkHgAAAAAAAABQYORqJfrDDz+sfv366fPPP5e/v78k6e+//9aAAQPUvHnzPE0QAAAAgH0DBw7McWxUVNRtzAQAAAAomHJVRJ82bZoee+wxlStXTgEBAZKk48ePq3r16vrss8/yNEEAAAAA9u3YsUM7duxQWlqaKlWqJEn67bffVKhQIdWpU8eMs1gsjkoRAAAAuKvlqogeEBCgX3/9VWvWrNGBAwckSVWqVFFoaGieJgcAAAAge23atFGxYsU0b948FS9eXJJ09uxZ9ejRQyEhIXr11VcdnCEAAABwd7upPdHXrVunqlWrKiUlRRaLRY888ohefvllvfzyy3rggQdUrVo1bdy4MU8T/Pvvv/XMM8+oZMmScnd3V40aNbR9+3az3zAMDR06VGXKlJG7u7tCQ0N16NAhm2OcOXNGXbp0kaenp7y9vdWzZ0+dP3/eJmbXrl0KCQmRm5ubAgICFBkZmafnAQAAANwOEyZM0JgxY8wCuiQVL15c7777riZMmODAzAAAAICC4aaK6JMmTVKvXr3k6emZqc/Ly0svvPBCnu6zePbsWTVq1EiFCxfW999/r3379mnChAk2HxAiIyM1ZcoUzZo1S1u2bJGHh4fCwsJ06dIlM6ZLly7au3evoqOjtWLFCm3YsEG9e/c2+1NSUtSiRQsFBgYqNjZW48aN0/DhwzV79uw8OxcAAADgdkhJSdGpU6cytZ86dUrnzp1zQEYAAABAwXJT27ns3LlTY8eOtdvfokULjR8//paTsho7dqwCAgI0Z84csy0oKMj8f8MwNGnSJL399tt6/PHHJUmffvqpfH19tWzZMnXq1En79+/XqlWrtG3bNtWrV0+SNHXqVLVu3Vrjx4+Xv7+/FixYoMuXL+uTTz6Ri4uLqlWrpri4OEVFRdkU2wEAAID85oknnlCPHj00YcIEPfjgg5KkLVu2aNCgQWrXrp2DswMAAADufje1Ej0hIUGFCxe22+/s7JzlKpjcWr58uerVq6cnn3xSPj4+ql27tj788EOz//Dhw4qPj7fZi93Ly0v169dXTEyMJCkmJkbe3t5mAV2SQkND5eTkpC1btpgxTZo0kYuLixkTFhamgwcP6uzZs3l2PgAAAEBemzVrllq1aqXOnTsrMDBQgYGB6ty5s1q2bKkZM2Y4Oj0AAADgrndTRfR77rlHe/bssdu/a9culSlT5paTsvrzzz81c+ZMVaxYUT/88IP69OmjV155RfPmzZMkxcfHS5J8fX1tbufr62v2xcfHy8fHx6bf2dlZJUqUsInJ6hjXjnG91NRUpaSk2FwAAACAO61IkSKaMWOGTp8+rR07dmjHjh06c+aMZsyYIQ8PD0enBwAAANz1bqqI3rp1a73zzjs2+41bXbx4UcOGDdOjjz6aZ8llZGSoTp06Gj16tGrXrq3evXurV69emjVrVp6NkVtjxoyRl5eXeQkICHB0SgAAAPgPO3nypE6ePKmKFSvKw8NDhmE4OiUAAACgQLipIvrbb7+tM2fO6L777lNkZKS++eYbffPNNxo7dqwqVaqkM2fO6K233sqz5MqUKaOqVavatFWpUkXHjh2TJPn5+Um6us3MtRISEsw+Pz8/JSYm2vRfuXJFZ86csYnJ6hjXjnG9IUOGKDk52bwcP348N6cIAAAA3JLTp0+refPmuu+++9S6dWudPHlSktSzZ0+9+uqrDs4OAAAAuPvdVBHd19dXmzdvVvXq1TVkyBA98cQTeuKJJ/Tmm2+qevXq+vnnnzNti3IrGjVqpIMHD9q0/fbbbwoMDJR09UdG/fz8tHbtWrM/JSVFW7ZsUXBwsCQpODhYSUlJio2NNWPWrVunjIwM1a9f34zZsGGD0tLSzJjo6GhVqlRJxYsXzzI3V1dXeXp62lwAAACAO23AgAEqXLiwjh07piJFipjtHTt21KpVqxyYGQAAAFAwON/sDQIDA/Xdd9/p7Nmz+v3332UYhipWrGi32HwrBgwYoIYNG2r06NF66qmntHXrVs2ePVuzZ8+WJFksFvXv31/vvvuuKlasqKCgIL3zzjvy9/dX27ZtJV1dud6yZUtzG5i0tDT17dtXnTp1kr+/vySpc+fOGjFihHr27KnXX39de/bs0eTJkzVx4sQ8PycAAAAgL61evVo//PCDypYta9NesWJFHT161EFZAQAAAAXHTRfRrYoXL64HHnggL3PJ5IEHHtDSpUs1ZMgQjRw5UkFBQZo0aZK6dOlixgwePFgXLlxQ7969lZSUpMaNG2vVqlVyc3MzYxYsWKC+ffuqefPmcnJyUvv27TVlyhSz38vLS6tXr1ZERITq1q2rUqVKaejQoerdu/dtPT8AAADgVl24cMFmBbrVmTNn5Orq6oCMAAAAgIIl10X0O+XRRx/N9sdKLRaLRo4cqZEjR9qNKVGihBYuXJjtOPfff782btyY6zwBAAAARwgJCdGnn36qUaNGSbo6P87IyFBkZKSaNWvm4OwAAACAu1++L6IDAAAAsC8yMlLNmzfX9u3bdfnyZQ0ePFh79+7VmTNntGnTJkenBwAAANz1buqHRQEAAADkL9WrV9dvv/2mxo0b6/HHH9eFCxfUrl077dixQ+XLl3d0egAAAMBdj5XoAAAAwF0qLS1NLVu21KxZs/TWW285Oh0AAACgQGIlOgAAAHCXKly4sHbt2uXoNAAAAIACjSI6AAAAcBd75pln9PHHHzs6DQAAAKDAYjsXAAAA4C525coVffLJJ1qzZo3q1q0rDw8Pm/6oqCgHZQYAAAAUDBTRAQAAgLvQn3/+qXLlymnPnj2qU6eOJOm3336zibFYLI5IDQAAAChQKKIDAAAAd6GKFSvq5MmTWr9+vSSpY8eOmjJlinx9fR2cGQAAAFCwsCc6AAAAcBcyDMPm+vfff68LFy44KBsAAACg4KKIDgAAABQA1xfVAQAAAOQNiugAAADAXchisWTa85w90AEAAIC8x57oAAAAwF3IMAx1795drq6ukqRLly7pxRdflIeHh03c119/7Yj0AAAAgAKDIjoAAABwF+rWrZvN9WeeecZBmQAAAAAFG0V0AAAA4C40Z84cR6cAAAAA/CewJzoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAADft/fffl8ViUf/+/c22S5cuKSIiQiVLllTRokXVvn17JSQk2Nzu2LFjCg8PV5EiReTj46NBgwbpypUrNjE//vij6tSpI1dXV1WoUEFz5869A2cEAAAAZI0iOgAAAICbsm3bNn3wwQe6//77bdoHDBigb7/9VkuWLNFPP/2kEydOqF27dmZ/enq6wsPDdfnyZW3evFnz5s3T3LlzNXToUDPm8OHDCg8PV7NmzRQXF6f+/fvr+eef1w8//HDHzg8AAAC4FkV0AAAAADl2/vx5denSRR9++KGKFy9uticnJ+vjjz9WVFSUHn74YdWtW1dz5szR5s2b9csvv0iSVq9erX379umzzz5TrVq11KpVK40aNUrTp0/X5cuXJUmzZs1SUFCQJkyYoCpVqqhv377q0KGDJk6c6JDzBQAAACiiAwAAAMixiIgIhYeHKzQ01KY9NjZWaWlpNu2VK1fW//73P8XExEiSYmJiVKNGDfn6+poxYWFhSklJ0d69e82Y648dFhZmHiMrqampSklJsbkAAAAAecXZ0QkAAAAAuDt88cUX+vXXX7Vt27ZMffHx8XJxcZG3t7dNu6+vr+Lj482Yawvo1n5rX3YxKSkpunjxotzd3TONPWbMGI0YMSLX5wUAAABkh5XoAAAAAG7o+PHj6tevnxYsWCA3NzdHp2NjyJAhSk5ONi/Hjx93dEoAAAAoQCiiAwAAALih2NhYJSYmqk6dOnJ2dpazs7N++uknTZkyRc7OzvL19dXly5eVlJRkc7uEhAT5+flJkvz8/JSQkJCp39qXXYynp2eWq9AlydXVVZ6enjYXAAAAIK9QRAcAAABwQ82bN9fu3bsVFxdnXurVq6cuXbqY/1+4cGGtXbvWvM3Bgwd17NgxBQcHS5KCg4O1e/duJSYmmjHR0dHy9PRU1apVzZhrj2GNsR4DAAAAuNPYEx0AAADADRUrVkzVq1e3afPw8FDJkiXN9p49e2rgwIEqUaKEPD099fLLLys4OFgNGjSQJLVo0UJVq1bVs88+q8jISMXHx+vtt99WRESEXF1dJUkvvviipk2bpsGDB+u5557TunXrtHjxYq1cufLOnjAAAADw/1FEBwAAAJAnJk6cKCcnJ7Vv316pqakKCwvTjBkzzP5ChQppxYoV6tOnj4KDg+Xh4aFu3bpp5MiRZkxQUJBWrlypAQMGaPLkySpbtqw++ugjhYWFOeKUAAAAAIroAAAAAHLnxx9/tLnu5uam6dOna/r06XZvExgYqO+++y7b4zZt2lQ7duzIixQBAACAW8ae6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdd1UR/f3335fFYlH//v3NtkuXLikiIkIlS5ZU0aJF1b59eyUkJNjc7tixYwoPD1eRIkXk4+OjQYMG6cqVKzYxP/74o+rUqSNXV1dVqFBBc+fOvQNnBAAAAAAAAADIz+6aIvq2bdv0wQcf6P7777dpHzBggL799lstWbJEP/30k06cOKF27dqZ/enp6QoPD9fly5e1efNmzZs3T3PnztXQoUPNmMOHDys8PFzNmjVTXFyc+vfvr+eff14//PDDHTs/AAAAAAAAAED+c1cU0c+fP68uXbroww8/VPHixc325ORkffzxx4qKitLDDz+sunXras6cOdq8ebN++eUXSdLq1au1b98+ffbZZ6pVq5ZatWqlUaNGafr06bp8+bIkadasWQoKCtKECRNUpUoV9e3bVx06dNDEiRMdcr4AAAAAAAAAgPzhriiiR0REKDw8XKGhoTbtsbGxSktLs2mvXLmy/ve//ykmJkaSFBMToxo1asjX19eMCQsLU0pKivbu3WvGXH/ssLAw8xhZSU1NVUpKis0FAAAAAAAAAFCwODs6gRv54osv9Ouvv2rbtm2Z+uLj4+Xi4iJvb2+bdl9fX8XHx5sx1xbQrf3WvuxiUlJSdPHiRbm7u2cae8yYMRoxYkSuzwsAAAAAAAAAkP/l65Xox48fV79+/bRgwQK5ubk5Oh0bQ4YMUXJysnk5fvy4o1MCAAAAAAAAAOSxfF1Ej42NVWJiourUqSNnZ2c5Ozvrp59+0pQpU+Ts7CxfX19dvnxZSUlJNrdLSEiQn5+fJMnPz08JCQmZ+q192cV4enpmuQpdklxdXeXp6WlzAQAAAAAAAAAULPm6iN68eXPt3r1bcXFx5qVevXrq0qWL+f+FCxfW2rVrzdscPHhQx44dU3BwsCQpODhYu3fvVmJiohkTHR0tT09PVa1a1Yy59hjWGOsxAAAAAAAAAAD/Tfl6T/RixYqpevXqNm0eHh4qWbKk2d6zZ08NHDhQJUqUkKenp15++WUFBwerQYMGkqQWLVqoatWqevbZZxUZGan4+Hi9/fbbioiIkKurqyTpxRdf1LRp0zR48GA999xzWrdunRYvXqyVK1fe2RMGAAAAAAAAAOQr+bqInhMTJ06Uk5OT2rdvr9TUVIWFhWnGjBlmf6FChbRixQr16dNHwcHB8vDwULdu3TRy5EgzJigoSCtXrtSAAQM0efJklS1bVh999JHCwsIccUoAAAAAAAAAgHziriui//jjjzbX3dzcNH36dE2fPt3ubQIDA/Xdd99le9ymTZtqx44deZEiAAAAAAAAAKCAyNd7ogMAAAAAAAAA4EgU0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADY4ezoBAAAAAAAd6HjbfL2eAHf5u3xAAAA8ggr0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYwQ+LAgAAAEBeyOsf2gQAAEC+wEp0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAAAAAAABgB0V0AAAAAAAAAADsoIgOAAAAAAAAAIAdFNEBAAAAAAAAALCDIjoAAAAAAAAAAHZQRAcAAAAAAAAAwA6K6AAAAAAAAAAA2EERHQAAAAAAAAAAOyiiAwAAALihMWPG6IEHHlCxYsXk4+Ojtm3b6uDBgzYxly5dUkREhEqWLKmiRYuqffv2SkhIsIk5duyYwsPDVaRIEfn4+GjQoEG6cuWKTcyPP/6oOnXqyNXVVRUqVNDcuXNv9+kBAAAAdlFEBwAAAHBDP/30kyIiIvTLL78oOjpaaWlpatGihS5cuGDGDBgwQN9++62WLFmin376SSdOnFC7du3M/vT0dIWHh+vy5cvavHmz5s2bp7lz52ro0KFmzOHDhxUeHq5mzZopLi5O/fv31/PPP68ffvjhjp4vAAAAYOXs6AQAAAAA5H+rVq2yuT537lz5+PgoNjZWTZo0UXJysj7++GMtXLhQDz/8sCRpzpw5qlKlin755Rc1aNBAq1ev1r59+7RmzRr5+vqqVq1aGjVqlF5//XUNHz5cLi4umjVrloKCgjRhwgRJUpUqVfTzzz9r4sSJCgsLu+PnDQAAALASHQAAAMBNS05OliSVKFFCkhQbG6u0tDSFhoaaMZUrV9b//vc/xcTESJJiYmJUo0YN+fr6mjFhYWFKSUnR3r17zZhrj2GNsR4DAAAAuNNYiQ4AAADgpmRkZKh///5q1KiRqlevLkmKj4+Xi4uLvL29bWJ9fX0VHx9vxlxbQLf2W/uyi0lJSdHFixfl7u6eKZ/U1FSlpqaa11NSUm7tBAEAAIBrsBIdAAAAwE2JiIjQnj179MUXXzg6FUlXf/TUy8vLvAQEBDg6JQAAABQgFNEBAAAA5Fjfvn21YsUKrV+/XmXLljXb/fz8dPnyZSUlJdnEJyQkyM/Pz4xJSEjI1G/tyy7G09Mzy1XokjRkyBAlJyebl+PHj9/SOQIAAADXoogOAAAA4IYMw1Dfvn21dOlSrVu3TkFBQTb9devWVeHChbV27Vqz7eDBgzp27JiCg4MlScHBwdq9e7cSExPNmOjoaHl6eqpq1apmzLXHsMZYj5EVV1dXeXp62lwAAACAvMKe6AAAAABuKCIiQgsXLtQ333yjYsWKmXuYe3l5yd3dXV5eXurZs6cGDhyoEiVKyNPTUy+//LKCg4PVoEEDSVKLFi1UtWpVPfvss4qMjFR8fLzefvttRUREyNXVVZL04osvatq0aRo8eLCee+45rVu3TosXL9bKlSsddu4AAAD4b2MlOgAAAIAbmjlzppKTk9W0aVOVKVPGvCxatMiMmThxoh599FG1b99eTZo0kZ+fn77++muzv1ChQlqxYoUKFSqk4OBgPfPMM+ratatGjhxpxgQFBWnlypWKjo5WzZo1NWHCBH300UcKCwu7o+cLAAAAWLESHQAAAMANGYZxwxg3NzdNnz5d06dPtxsTGBio7777LtvjNG3aVDt27LjpHAEAAIDbgZXoAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsCNfF9HHjBmjBx54QMWKFZOPj4/atm2rgwcP2sRcunRJERERKlmypIoWLar27dsrISHBJubYsWMKDw9XkSJF5OPjo0GDBunKlSs2MT/++KPq1KkjV1dXVahQQXPnzr3dpwcAAAAAAAAAyOfydRH9p59+UkREhH755RdFR0crLS1NLVq00IULF8yYAQMG6Ntvv9WSJUv0008/6cSJE2rXrp3Zn56ervDwcF2+fFmbN2/WvHnzNHfuXA0dOtSMOXz4sMLDw9WsWTPFxcWpf//+ev755/XDDz/c0fMFAAAAAAAAAOQvzo5OIDurVq2yuT537lz5+PgoNjZWTZo0UXJysj7++GMtXLhQDz/8sCRpzpw5qlKlin755Rc1aNBAq1ev1r59+7RmzRr5+vqqVq1aGjVqlF5//XUNHz5cLi4umjVrloKCgjRhwgRJUpUqVfTzzz9r4sSJCgsLu+PnDQAAAAAAAADIH/J1Ef16ycnJkqQSJUpIkmJjY5WWlqbQ0FAzpnLlyvrf//6nmJgYNWjQQDExMapRo4Z8fX3NmLCwMPXp00d79+5V7dq1FRMTY3MMa0z//v3t5pKamqrU1FTzekpKSl6cIgAAAAD8Nx1vk7fHC/g2b48HAAD+s/L1di7XysjIUP/+/dWoUSNVr15dkhQfHy8XFxd5e3vbxPr6+io+Pt6MubaAbu239mUXk5KSoosXL2aZz5gxY+Tl5WVeAgICbvkcAQAAAAAAAAD5y11TRI+IiNCePXv0xRdfODoVSdKQIUOUnJxsXo4fP+7olAAAAAAAAAAAeeyu2M6lb9++WrFihTZs2KCyZcua7X5+frp8+bKSkpJsVqMnJCTIz8/PjNm6davN8RISEsw+63+tbdfGeHp6yt3dPcucXF1d5erqesvnBgAAAAAAAADIv/L1SnTDMNS3b18tXbpU69atU1BQkE1/3bp1VbhwYa1du9ZsO3jwoI4dO6bg4GBJUnBwsHbv3q3ExEQzJjo6Wp6enqpataoZc+0xrDHWYwAAAAAAAAAA/pvy9Ur0iIgILVy4UN98842KFStm7mHu5eUld3d3eXl5qWfPnho4cKBKlCghT09PvfzyywoODlaDBg0kSS1atFDVqlX17LPPKjIyUvHx8Xr77bcVERFhriR/8cUXNW3aNA0ePFjPPfec1q1bp8WLF2vlypUOO3cAAAAAAAAAgOPl65XoM2fOVHJyspo2baoyZcqYl0WLFpkxEydO1KOPPqr27durSZMm8vPz09dff232FypUSCtWrFChQoUUHBysZ555Rl27dtXIkSPNmKCgIK1cuVLR0dGqWbOmJkyYoI8++khhYWF39HwBAAAAAAAAAPlLvl6JbhjGDWPc3Nw0ffp0TZ8+3W5MYGCgvvvuu2yP07RpU+3YseOmcwQAAAAAAAAAFFz5eiU6AAAAAAAAAACORBEdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsMPZ0QkAAAAAAJDnjrfJu2MFfJt3xwIAAHcdVqIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADmdHJwAAAAAAQL52vE3eHi/g27w9HgAAuK1YiQ4AAAAAAAAAgB0U0QEAAAAAAAAAsIMiOgAAAAAAAAAAdlBEBwAAAAAAAADADoroAAAAAAAAAADYQREdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOxwdnQCAAAAAAD8pxxvk7fHC/g2b48HAABssBIdAAAAAAAAAAA7KKIDAAAAAAAAAGAHRXQAAAAAAAAAAOygiA4AAAAAAAAAgB38sCgAAAAAAHczfqgUAIDbipXoAAAAAAAAAADYQREdAAAAAAAAAAA72M4FAAAAAAD8H7aHAQDABivRAQAAAAAAAACwg5XoAAAAAADg7pGXK+VZJQ8AyAGK6AAAAAAA4PbJ6+1hAAC4w9jO5TrTp09XuXLl5Obmpvr162vr1q2OTgkAAAD4z2FeDgAAgPyCIvo1Fi1apIEDB2rYsGH69ddfVbNmTYWFhSkxMdHRqQEAAAD/GczLAQAAkJ+wncs1oqKi1KtXL/Xo0UOSNGvWLK1cuVKffPKJ3njjDQdnBwAAAPw3MC8HcMfk9VYz7LEOAAUSK9H/v8uXLys2NlahoaFmm5OTk0JDQxUTE+PAzAAAAID/DublAAAAyG9Yif7//fPPP0pPT5evr69Nu6+vrw4cOJApPjU1Vampqeb15ORkSVJKSsrtTTQLl86fu+NjArg7pKS4ODqFfCHtUuqNgwD8ZxV2wPzNOmc0DOOOj53f3ey8XMpHc/NzaXd2PAD5jwPeUwAAuZfTeTlF9FwaM2aMRowYkak9ICDAAdkAQNYyv0oBADJ5f7rDhj537py8vLwcNn5BwdwcQP7BazoA3I1uNC+niP7/lSpVSoUKFVJCQoJNe0JCgvz8/DLFDxkyRAMHDjSvZ2Rk6MyZMypZsqQsFsttzxfISkpKigICAnT8+HF5eno6Oh0AyJd4rUR+YBiGzp07J39/f0enku/c7LxccuzcnNeUgo/HuODjMS74eIwLPh7jgu92PcY5nZdTRP//XFxcVLduXa1du1Zt27aVdHXyvXbtWvXt2zdTvKurq1xdXW3avL2970CmwI15enrypgEAN8BrJRyNFehZu9l5uZQ/5ua8phR8PMYFH49xwcdjXPDxGBd8t+Mxzsm8nCL6NQYOHKhu3bqpXr16evDBBzVp0iRduHBBPXr0cHRqAAAAwH8G83IAAADkJxTRr9GxY0edOnVKQ4cOVXx8vGrVqqVVq1Zl+lEjAAAAALcP83IAAADkJxTRr9O3b1+7XxMF8jtXV1cNGzYs09eZAQD/h9dK4O5wt8zLeU0p+HiMCz4e44KPx7jg4zEu+Bz9GFsMwzAcMjIAAAAAAAAAAPmck6MTAAAAAAAAAAAgv6KIDgAAAAAAAACAHRTRgQLgxx9/lMViUVJSUrZx5cqV06RJk+5ITgBQUPDaCQAAAAD/bRTRgTuoe/fuslgsslgscnFxUYUKFTRy5EhduXLllo7bsGFDnTx5Ul5eXpKkuXPnytvbO1Pctm3b1Lt371saCwDykvV18f3337dpX7ZsmSwWyx3NhddOADdr+vTpKleunNzc3FS/fn1t3brV0Skhh4YPH27Oy62XypUrm/2XLl36f+3de1zOd/8H8Feqq6PkkA50QDSRQ0xqdCCuDMOMzbpz2Lix3NiSzfbYcrvvUcwYN2Pb/SC7DdsQGjHqymFJIjlGKf1Maah1Qqf374/dfW/fVWgOGa/n43E9Htf1+Xyuz+F6P/r0/X6u7/X5Ijg4GM2bN4e5uTlGjhyJq1evqurIzs7G4MGDYWpqipYtWyI0NPSBj+vpj9u/fz+GDh0KOzs76OnpISoqSpUvIvjoo49ga2sLExMT+Pv748KFC6oyN27cQGBgICwsLGBpaYk333wTxcXFqjKpqano27cvjI2NYW9vj4ULFz7qodF/3SvGd55vVz8CAgJUZRjjJ9eCBQvw/PPPo3HjxmjZsiWGDx+OtLQ0VZmHNTfrdDq4u7vDyMgIzs7OWLt27aMeHuH+Yuzr61vj73jKlCmqMg0VYy6iEz1mAQEByMnJwYULFxASEoK5c+di0aJFD1SnRqOBjY3NPRecrKysYGpq+kBtERE9bMbGxoiIiEB+fn5Dd6VWnDuJqDabNm3CO++8g7CwMBw7dgxdu3aFVqtFXl5eQ3eN7lOnTp2Qk5OjPA4ePKjkvf3229ixYwe+++47xMfH48qVK3j55ZeV/MrKSgwePBhlZWX46aefEBkZibVr1+Kjjz5qiKEQgJKSEnTt2hUrVqyoNX/hwoVYtmwZVq1ahcTERJiZmUGr1eLWrVtKmcDAQJw+fRo//vgjoqOjsX//ftUX6YWFhRg4cCAcHR2RnJyMRYsWYe7cufjiiy8e+fjo3jEG/ne+Xf3YsGGDKp8xfnLFx8cjODgYhw8fxo8//ojy8nIMHDgQJSUlSpmHMTdnZmZi8ODB8PPzQ0pKCmbOnImJEydi9+7dj3W8z6L7iTEATJo0SfV3fOcXWQ0aYyGix2bcuHEybNgwVdqAAQOkd+/ecuPGDQkKChJLS0sxMTGRgIAAOX/+vFIuKytLhgwZIpaWlmJqaiqurq7yww8/iIhIXFycAJD8/Hzl+Z2PsLAwERFxdHSUJUuWiIjImDFjZPTo0aq+lJWVSfPmzSUyMlJERCorK2X+/Pni5OQkxsbG0qVLF/nuu+8ezYdDRM+kcePGyZAhQ+S5556T0NBQJX3r1q1y52HKgQMHpE+fPmJsbCytW7eWv/3tb1JcXKzkX7lyRV588UUxNjYWJycnWb9+vWrOExFZvHixdO7cWUxNTaV169YydepUKSoqEhHh3ElE9darVy8JDg5WXldWVoqdnZ0sWLCgAXtF9yssLEy6du1aa15BQYEYGhqq5u6zZ88KAElISBARkZ07d0qjRo0kNzdXKfP555+LhYWF3L59+5H2ne4NgGzdulV5XVVVJTY2NrJo0SIlraCgQIyMjGTDhg0iInLmzBkBIElJSUqZXbt2iZ6envz8888iIrJy5Upp2rSpKsbvvvuuuLi4POIR0e/9PsYitZ9v34kx/nPJy8sTABIfHy8iD29unj17tnTq1EnV1quvviparfZRD4l+5/cxFhHx8fGRGTNm1Pmehowxr0QnamAmJiYoKyvD+PHjcfToUWzfvh0JCQkQEbz44osoLy8HAAQHB+P27dvYv38/Tp48iYiICJibm9eoz8vLC0uXLoWFhYXyrd2sWbNqlAsMDMSOHTtUP13bvXs3SktLMWLECAC//dRm3bp1WLVqFU6fPo23334bf/nLXxAfH/+IPg0iehbp6+tj/vz5WL58OS5fvlwjPyMjAwEBARg5ciRSU1OxadMmHDx4ENOmTVPKjB07FleuXIFOp8PmzZvxxRdf1LgatFGjRli2bBlOnz6NyMhIxMbGYvbs2QA4dxJR/ZSVlSE5ORn+/v5KWqNGjeDv74+EhIQG7BnVx4ULF2BnZ4e2bdsiMDAQ2dnZAIDk5GSUl5er4vvcc8/BwcFBiW9CQgLc3NxgbW2tlNFqtSgsLMTp06cf70DonjIzM5Gbm6uKaZMmTeDh4aGKqaWlJXr27KmU8ff3R6NGjZCYmKiU8fb2hkajUcpotVqkpaU9sb+oe9bodDq0bNkSLi4umDp1Kq5fv67kMcZ/Lr/++isAoFmzZgAe3tyckJCgqqO6DP9/P36/j3G19evXo0WLFujcuTPmzJmD0tJSJa8hY2zwQO8moj9MRLBv3z7s3r0bgwYNQlRUFA4dOgQvLy8Av00a9vb2iIqKwqhRo5CdnY2RI0fCzc0NANC2bdta69VoNGjSpAn09PRgY2NTZ/tarRZmZmbYunUrgoKCAADffPMNXnrpJTRu3Bi3b9/G/PnzsXfvXnh6eiptHjx4EKtXr4aPj8/D/DiI6Bk3YsQIdOvWDWFhYfj3v/+tyluwYAECAwMxc+ZMAED79u2xbNky+Pj44PPPP0dWVhb27t2LpKQk5aToq6++Qvv27VX1VL8f+O1mof/85z8xZcoUrFy5knMnEdXLtWvXUFlZqTqBAwBra2ucO3eugXpF9eHh4YG1a9fCxcUFOTk5+Pvf/46+ffvi1KlTyM3NhUajqXGfDGtra+Tm5gIAcnNza41/dR49WapjUlvM7oxpy5YtVfkGBgZo1qyZqkybNm1q1FGd17Rp00fSf7o/AQEBePnll9GmTRtkZGTg/fffx6BBg5CQkAB9fX3G+E+kqqoKM2fOxAsvvIDOnTsDwEObm+sqU1hYiJs3b8LExORRDIl+p7YYA8Drr78OR0dH2NnZITU1Fe+++y7S0tKwZcsWAA0bYy6iEz1m0dHRMDc3R3l5OaqqqvD666/j5ZdfRnR0NDw8PJRyzZs3h4uLC86ePQsAmD59OqZOnYo9e/bA398fI0eORJcuXf5wPwwMDDB69GisX78eQUFBKCkpwbZt27Bx40YAQHp6OkpLSzFgwADV+8rKytC9e/c/3C4RUV0iIiLQr1+/GleAnzhxAqmpqVi/fr2SJiKoqqpCZmYmzp8/DwMDA7i7uyv5zs7ONU5y9u7diwULFuDcuXMoLCxERUUFbt26hdLS0vve85xzJxHR02HQoEHK8y5dusDDwwOOjo749ttvuYBC9Cf12muvKc/d3NzQpUsXtGvXDjqdDv3792/AnlF9BQcH49SpU6p7VdDTpa4Y33mPAjc3N9ja2qJ///7IyMhAu3btHnc3VbidC9FjVn1jgwsXLuDmzZuIjIy85w1BAWDixIm4ePEigoKCcPLkSfTs2RPLly9/oL4EBgZi3759yMvLQ1RUFExMTJS7l1dvVfDDDz8gJSVFeZw5cwbff//9A7VLRFQbb29vaLVazJkzR5VeXFyMyZMnq+aiEydO4MKFC/d9IJWVlYUhQ4agS5cu2Lx5M5KTk5WbUpWVldWrn5w7iahFixbQ19fH1atXVelXr169669Z6MllaWmJDh06ID09HTY2NigrK0NBQYGqzJ3xtbGxqTX+1Xn0ZKmOyd3+Zm1sbGpsBVdRUYEbN24w7n9Sbdu2RYsWLZCeng6AMf6zmDZtGqKjoxEXF4fWrVsr6Q9rbq6rjIWFBb9EfUzqinFtqi82vfPvuKFizEV0osfMzMwMzs7OcHBwgIHBbz8G6dixIyoqKpR92ADg+vXrSEtLg6urq5Jmb2+PKVOmYMuWLQgJCcGXX35ZaxsajQaVlZX37IuXlxfs7e2xadMmrF+/HqNGjYKhoSEAwNXVFUZGRsjOzoazs7PqYW9v/yAfARFRncLDw7Fjxw7VfnXu7u44c+ZMjbnI2dkZGo0GLi4uqKiowPHjx5X3pKenq/atTE5ORlVVFRYvXozevXujQ4cOuHLliqptzp1EdL80Gg169OiBffv2KWlVVVXYt2+fspUT/bkUFxcjIyMDtra26NGjBwwNDVXxTUtLQ3Z2thJfT09PnDx5UrUg9+OPP8LCwkJ1/E5PhjZt2sDGxkYV08LCQiQmJqpiWlBQgOTkZKVMbGwsqqqqlEUcT09P7N+/X7lvFfBb3F1cXLjNxxPo8uXLuH79OmxtbQEwxk86EcG0adOwdetWxMbG1thW52HNzZ6enqo6qsvw//ejd68Y1yYlJQUAVH/HDRbjB7otKRHVy93uFj5s2DBxdXWVAwcOSEpKigQEBIizs7OUlZWJiMiMGTMkJiZGLl68KMnJyeLh4SGjR48WEZG4uDgBIPn5+SIicujQIQEge/fulV9++UVKSkpERMTR0VGWLFmiaveDDz4QV1dXMTAwkAMHDtTIa968uaxdu1bS09MlOTlZli1bJmvXrn14HwoRPdNqmxeDgoLE2NhYqg9TTpw4ISYmJhIcHCzHjx+X8+fPS1RUlAQHByvv8ff3F3d3d0lMTJRjx46Jn5+fmJiYyNKlS0VEJCUlRQDI0qVLJSMjQ9atWyetWrXi3ElEf9jGjRvFyMhI1q5dK2fOnJG//vWvYmlpKbm5uQ3dNboPISEhotPpJDMzUw4dOiT+/v7SokULycvLExGRKVOmiIODg8TGxsrRo0fF09NTPD09lfdXVFRI586dZeDAgZKSkiIxMTFiZWUlc+bMaaghPfOKiork+PHjcvz4cQEgn376qRw/flwuXbokIiLh4eFiaWkp27Ztk9TUVBk2bJi0adNGbt68qdQREBAg3bt3l8TERDl48KC0b99exowZo+QXFBSItbW1BAUFyalTp2Tjxo1iamoqq1evfuzjfRbdLcZFRUUya9YsSUhIkMzMTNm7d6+4u7tL+/bt5datW0odjPGTa+rUqdKkSRPR6XSSk5OjPEpLS5UyD2NuvnjxopiamkpoaKicPXtWVqxYIfr6+hITE/NYx/ssuleM09PTZd68eXL06FHJzMyUbdu2Sdu2bcXb21upoyFjzEV0osfobovoN27ckKCgIGnSpImYmJiIVquV8+fPK/nTpk2Tdu3aiZGRkVhZWUlQUJBcu3ZNRGouoov89s+lefPmAkDCwsJEpPaFoDNnzggAcXR0lKqqKlVeVVWVLF26VFxcXMTQ0FCsrKxEq9VKfHz8A38WREQitc+LmZmZotFo5M7v+o8cOSIDBgwQc3NzMTMzky5dusjHH3+s5F+5ckUGDRokRkZG4ujoKN988420bNlSVq1apZT59NNPxdbWVplj161bx7mTiB7I8uXLxcHBQTQajfTq1UsOHz7c0F2i+/Tqq6+Kra2taDQaadWqlbz66quSnp6u5N+8eVPeeustadq0qZiamsqIESMkJydHVUdWVpYMGjRITExMpEWLFhISEiLl5eWPeyj0X9XnRL9/jBs3TkR++//84YcfirW1tRgZGUn//v0lLS1NVcf169dlzJgxYm5uLhYWFjJhwgQpKipSlTlx4oT06dNHjIyMpFWrVhIeHv64hvjMu1uMS0tLZeDAgWJlZSWGhobi6OgokyZNqvHFJmP85KottgBkzZo1SpmHNTfHxcVJt27dRKPRSNu2bVVt0KNzrxhnZ2eLt7e3NGvWTIyMjMTZ2VlCQ0Pl119/VdXTUDHW++8giIiIiJ4aly9fhr29Pfbu3csbSREREREREdED4SI6ERER/enFxsaiuLgYbm5uyMnJwezZs/Hzzz/j/Pnzyn7lRERERERERH+EQUN3gIiIiOhBlZeX4/3338fFixfRuHFjeHl5Yf369VxAJyIiIiIiogfGK9GJiIiIiIiIiIiIiOrQqKE7QERERERERERERET0pOIiOhERERERERERERFRHbiITkRERERERERERERUBy6iExERERERERERERHVgYvoRERERERERERERER14CI6ERE9VDqdDnp6eigoKGjorhARERERPdGysrKgp6eHlJSUhu6K4ty5c+jduzeMjY3RrVu3hu5OrXx9fTFz5syG7gYRPUO4iE5E9JT65ZdfMHXqVDg4OMDIyAg2NjbQarU4dOjQQ2ujtoNXLy8v5OTkoEmTJg+tnT9q/PjxGD58eEN3g4iIiIieUOPHj4eenh7Cw8NV6VFRUdDT02ugXjWssLAwmJmZIS0tDfv27auRv2rVKjRu3BgVFRVKWnFxMQwNDeHr66sqW32BTUZGxqPuNhHRI8VFdCKip9TIkSNx/PhxREZG4vz589i+fTt8fX1x/fr1R9quRqOBjY3NM3vSQURERER/LsbGxoiIiEB+fn5Dd+WhKSsr+8PvzcjIQJ8+feDo6IjmzZvXyPfz80NxcTGOHj2qpB04cAA2NjZITEzErVu3lPS4uDg4ODigXbt29e6HiKgW6omIGhIX0YmInkIFBQU4cOAAIiIi4OfnB0dHR/Tq1Qtz5szBSy+9pJSZOHEirKysYGFhgX79+uHEiRNKHXPnzkW3bt3w9ddfw8nJCU2aNMFrr72GoqIiAL9dtRMfH4/PPvsMenp60NPTQ1ZWVo3tXNauXQtLS0tER0fDxcUFpqameOWVV1BaWorIyEg4OTmhadOmmD59OiorK5X2b9++jVmzZqFVq1YwMzODh4cHdDqdkl9d7+7du9GxY0eYm5sjICAAOTk5Sv8jIyOxbds2pX93vp+IiIiICAD8/f1hY2ODBQsW1Fmm+tj4TkuXLoWTk5PyuvpXkPPnz4e1tTUsLS0xb948VFRUIDQ0FM2aNUPr1q2xZs2aGvWfO3cOXl5eMDY2RufOnREfH6/KP3XqFAYNGgRzc3NYW1sjKCgI165dU/J9fX0xbdo0zJw5Ey1atIBWq611HFVVVZg3bx5at24NIyMjdOvWDTExMUq+np4ekpOTMW/ePOjp6WHu3Lk16nBxcYGtra3q2Fqn02HYsGFo06YNDh8+rEr38/MD8Nvx/fTp09GyZUsYGxujT58+SEpKUpXV09PDrl270KNHDxgZGeHgwYMoKSnB2LFjYW5uDltbWyxevLhGn1auXIn27dvD2NgY1tbWeOWVV2odPxHRH8VFdCKip5C5uTnMzc0RFRWF27dv11pm1KhRyMvLw65du5CcnAx3d3f0798fN27cUMpkZGQgKioK0dHRiI6ORnx8vPJT188++wyenp6YNGkScnJykJOTA3t7+1rbKi0txbJly7Bx40bExMRAp9NhxIgR2LlzJ3bu3Imvv/4aq1evxvfff6+8Z9q0aUhISMDGjRuRmpqKUaNGISAgABcuXFDV+8knn+Drr7/G/v37kZ2djVmzZgEAZs2ahdGjRysL6zk5OfDy8nrgz5aIiIiIni76+vqYP38+li9fjsuXLz9QXbGxsbhy5Qr279+PTz/9FGFhYRgyZAiaNm2KxMRETJkyBZMnT67RTmhoKEJCQnD8+HF4enpi6NChyi9ICwoK0K9fP3Tv3h1Hjx5FTEwMrl69itGjR6vqiIyMhEajwaFDh7Bq1apa+/fZZ59h8eLF+OSTT5CamgqtVouXXnpJOcbOyclBp06dEBISgpycHOXY+vf8/PwQFxenvI6Li4Ovry98fHyU9Js3byIxMVFZRJ89ezY2b96MyMhIHDt2DM7OztBqtarzDwB47733EB4ejrNnz6JLly4IDQ1FfHw8tm3bhj179kCn0+HYsWNK+aNHj2L69OmYN28e0tLSEBMTA29v73vGioioXoSIiJ5K33//vTRt2lSMjY3Fy8tL5syZIydOnBARkQMHDoiFhYXcunVL9Z527drJ6tWrRUQkLCxMTE1NpbCwUMkPDQ0VDw8P5bWPj4/MmDFDVUdcXJwAkPz8fBERWbNmjQCQ9PR0pczkyZPF1NRUioqKlDStViuTJ08WEZFLly6Jvr6+/Pzzz6q6+/fvL3PmzKmz3hUrVoi1tbXyety4cTJs2LD7+ryIiIiI6Nlz5/Fi79695Y033hARka1bt8qdSyZhYWHStWtX1XuXLFkijo6OqrocHR2lsrJSSXNxcZG+ffsqrysqKsTMzEw2bNggIiKZmZkCQMLDw5Uy5eXl0rp1a4mIiBARkX/84x8ycOBAVdv/93//JwAkLS1NRH47Lu/evfs9x2tnZycff/yxKu3555+Xt956S3ndtWtXCQsLu2s9X375pZiZmUl5ebkUFhaKgYGB5OXlyTfffCPe3t4iIrJv3z4BIJcuXZLi4mIxNDSU9evXK3WUlZWJnZ2dLFy4UET+dx4RFRWllCkqKhKNRiPffvutknb9+nUxMTFRzkM2b94sFhYWqvMWIqKHzaDhlu+JiOhRGjlyJAYPHowDBw7g8OHD2LVrFxYuXIivvvoKJSUlKC4urrHH4c2bN1U3/XFyckLjxo2V17a2tsjLy6t3X0xNTVX7IFpbW8PJyQnm5uaqtOq6T548icrKSnTo0EFVz+3bt1V9/n29f7R/REREREQRERHo169fnVdf349OnTqhUaP//ejf2toanTt3Vl7r6+ujefPmNY5ZPT09lecGBgbo2bMnzp49CwA4ceIE4uLiVMfO1TIyMpRj5h49ety1b4WFhbhy5QpeeOEFVfoLL7yg2tbxfvj6+qKkpARJSUnIz89Hhw4dYGVlBR8fH0yYMAG3bt2CTqdD27Zt4eDggNTUVJSXl6vaNjQ0RK9evZRxVuvZs6dqfGVlZfDw8FDSmjVrBhcXF+X1gAED4OjoiLZt2yIgIAABAQEYMWIETE1N6zUmIqK74SI6EdFTzNjYGAMGDMCAAQPw4YcfYuLEiQgLC8Nbb71VYx/DapaWlspzQ0NDVZ6enh6qqqrq3Y/a6rlb3cXFxdDX10dycjL09fVV5e48eaitDhGpd/+IiIiIiLy9vaHVajFnzhyMHz9eldeoUaMax5nl5eU16qjvce/9KC4uxtChQxEREVEjz9bWVnluZmZ233U+KGdnZ7Ru3RpxcXHIz8+Hj48PAMDOzg729vb46aefEBcXh379+tW77vqOo3Hjxjh27Bh0Oh327NmDjz76CHPnzkVSUpLq3IaI6EFwT3QiomeIq6srSkpK4O7ujtzcXBgYGMDZ2Vn1aNGixX3Xp9FoVDcDfVi6d++OyspK5OXl1eifjY1Ng/ePiIiIiJ5O4eHh2LFjBxISElTpVlZWyM3NVS2kp6SkPLR277wZZ0VFBZKTk9GxY0cAgLu7O06fPg0nJ6cax8b1WXC2sLCAnZ0dDh06pEo/dOgQXF1d691nPz8/6HQ66HQ6+Pr6Kune3t7YtWsXjhw5ouyH3q5dO2W/9mrl5eVISkq6a9vt2rWDoaEhEhMTlbT8/HycP39eVc7AwAD+/v5YuHAhUlNTkZWVhdjY2HqPiYioLlxEJyJ6Cl2/fh39+vXDf/7zH6SmpiIzMxPfffcdFi5ciGHDhsHf3x+enp4YPnw49uzZg6ysLPz000/44IMPcPTo0ftux8nJCYmJicjKysK1a9f+0FXqtenQoQMCAwMxduxYbNmyBZmZmThy5AgWLFiAH374oV79S01NRVpaGq5du1br1UJERERERNXc3NwQGBiIZcuWqdJ9fX3xyy+/YOHChcjIyMCKFSuwa9euh9buihUrsHXrVpw7dw7BwcHIz8/HG2+8AQAIDg7GjRs3MGbMGCQlJSEjIwO7d+/GhAkT6n3BSGhoKCIiIrBp0yakpaXhvffeQ0pKCmbMmFHvPvv5+eHgwYNISUlRrkQHAB8fH6xevRplZWXKIrqZmRmmTp2K0NBQxMTE4MyZM5g0aRJKS0vx5ptv1tmGubk53nzzTYSGhiI2NhanTp3C+PHjVVvmREdHY9myZUhJScGlS5ewbt06VFVVqbZ8ISJ6UFxEJyJ6Cpmbm8PDwwNLliyBt7c3OnfujA8//BCTJk3Cv/71L+jp6WHnzp3w9vbGhAkT0KFDB7z22mu4dOkSrK2t77udWbNmQV9fH66urrCyskJ2dvZDG8OaNWswduxYhISEwMXFBcOHD0dSUhIcHBzuu45JkybBxcUFPXv2hJWVVY2rboiIiIiIfm/evHk1Lg7p2LEjVq5ciRUrVqBr1644cuTIA+2d/nvh4eEIDw9H165dcfDgQWzfvl35hWj11eOVlZUYOHAg3NzcMHPmTFhaWqoWk+/H9OnT8c477yAkJARubm6IiYnB9u3b0b59+3r32c/PDzdv3oSzs7PqHMLHxwdFRUVwcXFRbTcTHh6OkSNHIigoCO7u7khPT8fu3bvRtGnTu7azaNEi9O3bF0OHDoW/vz/69Omj2v/d0tISW7ZsQb9+/dCxY0esWrUKGzZsQKdOneo9JiKiuugJN48lIiIiIiIiIiIiIqoVr0QnIiIiIiIiIiIiIqoDF9GJiIiIiIiIiIiIiOrARXQiIiIiIiIiIiIiojpwEZ2IiIiIiIiIiIiIqA5cRCciIiIiIiIiIiIiqgMX0YmIiIiIiIiIiIiI6sBFdCIiIiIiIiIiIiKiOnARnYiIiIiIiIiIiIioDlxEJyIiIiIiIiIiIiKqAxfRiYiIiIiIiIiIiIjqwEV0IiIiIiIiIiIiIqI6cBGdiIiIiIiIiIiIiKgO/w9EaZ6DbhztYAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review Length Statistics:\n",
            "Mean Length: 233.7872\n",
            "Max Length: 2470\n",
            "Min Length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample the Data"
      ],
      "metadata": {
        "id": "afLgVacvkRJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set the sample fraction (e.g., 5%)\n",
        "sample_fraction = 0.05\n",
        "\n",
        "# Sample from the training data\n",
        "train_documents = dataset[\"train\"][\"text\"]\n",
        "train_labels = [\"pos\" if label == 1 else \"neg\" for label in dataset[\"train\"][\"label\"]]\n",
        "train_sample_size = int(len(train_documents) * sample_fraction)\n",
        "\n",
        "# Randomly sample reviews and labels from the training set\n",
        "random.seed(42)\n",
        "train_sample_indices = random.sample(range(len(train_documents)), train_sample_size)\n",
        "train_texts = [train_documents[i] for i in train_sample_indices]\n",
        "train_labels = [train_labels[i] for i in train_sample_indices]\n",
        "\n",
        "# Sample from the test data\n",
        "test_documents = dataset[\"test\"][\"text\"]\n",
        "test_labels = [\"pos\" if label == 1 else \"neg\" for label in dataset[\"test\"][\"label\"]]\n",
        "test_sample_size = int(len(test_documents) * sample_fraction)\n",
        "\n",
        "# Randomly sample reviews and labels from the test set\n",
        "test_sample_indices = random.sample(range(len(test_documents)), test_sample_size)\n",
        "test_texts = [test_documents[i] for i in test_sample_indices]\n",
        "test_labels = [test_labels[i] for i in test_sample_indices]\n",
        "\n",
        "# Print the number of samples in each split\n",
        "print(\"Number of training samples:\", len(train_texts))\n",
        "print(\"Number of testing samples:\", len(test_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1js_qXYhqUn",
        "outputId": "7bb73cb7-05b0-4307-c053-69c978a68686"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 1250\n",
            "Number of testing samples: 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test Split\n",
        "If the dataset already has predefined training and testing splits, as the IMDb dataset does, you **don’t need to use `train_test_split`** again. You can simply sample from each of these splits directly.\n",
        "\n",
        "- **Separate Sampling for Train and Test**: Since the IMDb dataset already has a `train` and `test` split, we sample 5% from each directly, without mixing data between the two sets.\n",
        "- **Random Sampling from Each Split**: We use `random.sample()` separately for `train` and `test`, maintaining independence between the two subsets.\n",
        "\n",
        "### Benefits of Using Predefined Splits\n",
        "Using the existing `train` and `test` splits is beneficial because:\n",
        "- **Consistency**: Many datasets are split to reflect real-world distributions, so using predefined splits can lead to more realistic evaluations.\n",
        "- **Avoids Data Leakage**: By not mixing data from `train` and `test`, you reduce the risk of data leakage, where information from the test set inadvertently influences the training process.\n"
      ],
      "metadata": {
        "id": "7kRwLGCnipYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the Data"
      ],
      "metadata": {
        "id": "ziQ0UpklkTY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize the training set\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Tokenize the test set\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Optional: Inspect tokenization on a few examples\n",
        "sample_texts = train_texts[:3]  # First three reviews in the training sample\n",
        "sample_labels = train_labels[:3]\n",
        "tokenized_texts = tokenizer(sample_texts, padding=True, truncation=True, max_length=128)\n",
        "\n",
        "# Display the original and tokenized versions\n",
        "for i, text in enumerate(sample_texts):\n",
        "    print(f\"Original Review {i+1}:\")\n",
        "    print(\"Text:\", text)\n",
        "    print(\"Label:\", \"Positive\" if sample_labels[i] == \"pos\" else \"Negative\")\n",
        "    print(\"\\nTokenized Review:\", tokenized_texts[\"input_ids\"][i])\n",
        "    print(\"Attention Mask:\", tokenized_texts[\"attention_mask\"][i])\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8v49V-Zhrmi",
        "outputId": "225aba5f-df75-49b1-f60b-9c7aa8e27d94"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Review 1:\n",
            "Text: Arguably this is a very good \"sequel\", better than the first live action film 101 Dalmatians. It has good dogs, good actors, good jokes and all right slapstick! <br /><br />Cruella DeVil, who has had some rather major therapy, is now a lover of dogs and very kind to them. Many, including Chloe Simon, owner of one of the dogs that Cruella once tried to kill, do not believe this. Others, like Kevin Shepherd (owner of 2nd Chance Dog Shelter) believe that she has changed. <br /><br />Meanwhile, Dipstick, with his mate, have given birth to three cute dalmatian puppies! Little Dipper, Domino and Oddball...<br /><br />Starring Eric Idle as Waddlesworth (the hilarious macaw), Glenn Close as Cruella herself and Gerard Depardieu as Le Pelt (another baddie, the name should give a clue), this is a good family film with excitement and lots more!! One downfall of this film is that is has a lot of painful slapstick, but not quite as excessive as the last film. This is also funnier than the last film.<br /><br />Enjoy \"102 Dalmatians\"! :-)\n",
            "Label: Positive\n",
            "\n",
            "Tokenized Review: [101, 15835, 2023, 2003, 1037, 2200, 2204, 1000, 8297, 1000, 1010, 2488, 2084, 1996, 2034, 2444, 2895, 2143, 7886, 17488, 18900, 7066, 1012, 2009, 2038, 2204, 6077, 1010, 2204, 5889, 1010, 2204, 13198, 1998, 2035, 2157, 14308, 21354, 999, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 10311, 2721, 6548, 1010, 2040, 2038, 2018, 2070, 2738, 2350, 7242, 1010, 2003, 2085, 1037, 7089, 1997, 6077, 1998, 2200, 2785, 2000, 2068, 1012, 2116, 1010, 2164, 9318, 4079, 1010, 3954, 1997, 2028, 1997, 1996, 6077, 2008, 10311, 2721, 2320, 2699, 2000, 3102, 1010, 2079, 2025, 2903, 2023, 1012, 2500, 1010, 2066, 4901, 11133, 1006, 3954, 1997, 3416, 3382, 3899, 7713, 1007, 2903, 2008, 2016, 2038, 2904, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 5564, 1010, 16510, 21354, 102]\n",
            "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "==================================================\n",
            "Original Review 2:\n",
            "Text: It's a good thing I didn't watch this while i was pregnant.I definitely would have cried my eyes out and/or vomit. It was Kind of gruesome mainly disturbing. I personally thought the baby was adorable in its own twisted little way.However as a mom I cringed when Beth stabbed herself in the stomach and when Virgina aborted the child during her 3rd trimester with rusty utensils no less.Also,as an animal lover i almost cried when she scratched the cat to a bloody pulp.However,As creepy and sinister as the baby was I was rooting for it to live.And as twisted as the movie was I am extremely intrigued to see the sequel...... ......... ....... ......... ......... ....... ...... .....\n",
            "Label: Negative\n",
            "\n",
            "Tokenized Review: [101, 2009, 1005, 1055, 1037, 2204, 2518, 1045, 2134, 1005, 1056, 3422, 2023, 2096, 1045, 2001, 6875, 1012, 1045, 5791, 2052, 2031, 6639, 2026, 2159, 2041, 1998, 1013, 2030, 23251, 1012, 2009, 2001, 2785, 1997, 24665, 15808, 8462, 3701, 14888, 1012, 1045, 7714, 2245, 1996, 3336, 2001, 23677, 1999, 2049, 2219, 6389, 2210, 2126, 1012, 2174, 2004, 1037, 3566, 1045, 23952, 2043, 7014, 13263, 2841, 1999, 1996, 4308, 1998, 2043, 6261, 2050, 11113, 15613, 1996, 2775, 2076, 2014, 3822, 12241, 20367, 2007, 13174, 21183, 6132, 12146, 2053, 2625, 1012, 2036, 1010, 2004, 2019, 4111, 7089, 1045, 2471, 6639, 2043, 2016, 15047, 1996, 4937, 2000, 1037, 6703, 16016, 1012, 2174, 1010, 2004, 17109, 1998, 16491, 2004, 1996, 3336, 2001, 1045, 2001, 7117, 2075, 2005, 2009, 2000, 2444, 1012, 102]\n",
            "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "==================================================\n",
            "Original Review 3:\n",
            "Text: This has to be the worst movie I have seen. Madsen fans don't be drawn into this like I was. He is only in it for a maximum of five minutes. This movie is so bad that the only reason why you would watch it is if all the rest of the movies on earth as well as t.v. had been destroyed.\n",
            "Label: Negative\n",
            "\n",
            "Tokenized Review: [101, 2023, 2038, 2000, 2022, 1996, 5409, 3185, 1045, 2031, 2464, 1012, 5506, 5054, 4599, 2123, 1005, 1056, 2022, 4567, 2046, 2023, 2066, 1045, 2001, 1012, 2002, 2003, 2069, 1999, 2009, 2005, 1037, 4555, 1997, 2274, 2781, 1012, 2023, 3185, 2003, 2061, 2919, 2008, 1996, 2069, 3114, 2339, 2017, 2052, 3422, 2009, 2003, 2065, 2035, 1996, 2717, 1997, 1996, 5691, 2006, 3011, 2004, 2092, 2004, 1056, 1012, 1058, 1012, 2018, 2042, 3908, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organizing Data for Model Input\n",
        "\n",
        "The next step is to prepare the tokenized data for training by organizing it into a format that the model can work with. Hugging Face models expect data in a specific structure, typically a PyTorch `Dataset` format. We’ll wrap our tokenized data (`train_encodings` and `test_encodings`) into a custom `Dataset` class so that it’s compatible with Hugging Face’s `Trainer` API.\n",
        "\n",
        "1. **Create a Custom Dataset Class**:\n",
        "   - This class will wrap around `train_encodings` and `train_labels` (and similarly for test data).\n",
        "   - It will format the data into a PyTorch `Dataset` so that it can be fed into the model.\n",
        "\n",
        "2. **Convert Labels to Numeric Format**:\n",
        "   - Since labels are strings (`\"pos\"` or `\"neg\"`), we’ll convert them to numerical values (`1` for positive and `0` for negative).\n",
        "\n",
        "---\n",
        "\n",
        "Here are the most important concepts to focus on regarding this `IMDbDataset` class:\n",
        "\n",
        "### 1. **Why We Need a Custom Dataset Class**\n",
        "   - **Compatibility with PyTorch**: Hugging Face's `Trainer` API, which we'll use for training, is built to work seamlessly with PyTorch `Dataset` objects. By creating a custom `IMDbDataset` class, we make our data compatible with `Trainer` and PyTorch’s `DataLoader`, allowing for efficient batch processing during training.\n",
        "   - **Standardized Access**: This class provides a structured way to access both the tokenized input data (`input_ids`, `attention_mask`) and the labels, so that each sample has both the input and label ready in a format the model expects.\n",
        "\n",
        "### 2. **Key Roles of the Class Methods**\n",
        "   - **`__getitem__`**: This method is crucial because it retrieves a single sample (input and label) in a consistent format. This structure is essential for iterating through the data during training, where each input-label pair needs to be correctly formatted.\n",
        "   - **`__len__`**: This method defines the dataset length, allowing PyTorch to understand the size of the dataset, which is needed for training loops.\n",
        "\n",
        "### 3. **Converting Labels to Tensors**\n",
        "   - **Numerical Labels**: The model expects numerical labels for binary classification, so we convert `\"pos\"` to `1` and `\"neg\"` to `0`. This conversion allows the model’s output to be compared directly to the target labels during training, enabling it to calculate loss and make updates accordingly.\n",
        "   - **Tensor Format**: PyTorch requires data to be in tensor format for efficient computations. By converting both the inputs and labels to tensors here, we ensure compatibility with PyTorch operations.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "- **Standardization for Model Training**: This class standardizes data access, ensuring each batch the model receives is in the correct format.\n",
        "- **PyTorch Compatibility**: Wrapping data in a `Dataset` makes it compatible with the tools used in training, like `Trainer` and `DataLoader`.\n",
        "- **Label Processing**: This step ensures that labels are in a usable, numerical format for binary classification.\n",
        "\n",
        "With this class in place, you’re ready to configure the training process, as the data is now in the ideal structure for training. Let me know if you’re ready for the next step!\n"
      ],
      "metadata": {
        "id": "-TJezYOOpHf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        # Convert labels to tensor format (1 for \"pos\" and 0 for \"neg\")\n",
        "        self.labels = torch.tensor([1 if label == \"pos\" else 0 for label in labels])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the item at the given index and add labels as 'labels' key\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create the training and testing dataset objects\n",
        "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
        "test_dataset = IMDbDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "bGjyJMIDpHAG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample from train_dataset:\\n\")\n",
        "print(train_dataset[0]['input_ids'])\n",
        "print(train_dataset[0]['attention_mask'])\n",
        "print(train_dataset[0]['labels'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffZwqPwlpG9w",
        "outputId": "1f71ce96-705b-4693-d152-fee18515fa0a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from train_dataset:\n",
            "\n",
            "tensor([  101, 15835,  2023,  2003,  1037,  2200,  2204,  1000,  8297,  1000,\n",
            "         1010,  2488,  2084,  1996,  2034,  2444,  2895,  2143,  7886, 17488,\n",
            "        18900,  7066,  1012,  2009,  2038,  2204,  6077,  1010,  2204,  5889,\n",
            "         1010,  2204, 13198,  1998,  2035,  2157, 14308, 21354,   999,  1026,\n",
            "         7987,  1013,  1028,  1026,  7987,  1013,  1028, 10311,  2721,  6548,\n",
            "         1010,  2040,  2038,  2018,  2070,  2738,  2350,  7242,  1010,  2003,\n",
            "         2085,  1037,  7089,  1997,  6077,  1998,  2200,  2785,  2000,  2068,\n",
            "         1012,  2116,  1010,  2164,  9318,  4079,  1010,  3954,  1997,  2028,\n",
            "         1997,  1996,  6077,  2008, 10311,  2721,  2320,  2699,  2000,  3102,\n",
            "         1010,  2079,  2025,  2903,  2023,  1012,  2500,  1010,  2066,  4901,\n",
            "        11133,  1006,  3954,  1997,  3416,  3382,  3899,  7713,  1007,  2903,\n",
            "         2008,  2016,  2038,  2904,  1012,  1026,  7987,  1013,  1028,  1026,\n",
            "         7987,  1013,  1028,  5564,  1010, 16510, 21354,   102])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1])\n",
            "tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Mask\n",
        "The **attention mask** is a key concept in transformer models. It’s used to indicate which tokens (words or subwords) in the input are actual content versus padding. Here’s a breakdown of what it is and why it matters:\n",
        "\n",
        "### 1. **Purpose of the Attention Mask**\n",
        "   - Transformer models, like BERT and DistilBERT, work with **fixed-length inputs**, meaning all sequences (e.g., sentences) need to be the same length.\n",
        "   - To make sequences of different lengths uniform, shorter sequences are padded with extra tokens (usually zeros) to reach the maximum length. These padding tokens are not real words and don’t contain useful information.\n",
        "   - The **attention mask** tells the model which tokens are actual data and which are just padding, helping the model **ignore padding** during processing.\n",
        "\n",
        "### 2. **How the Attention Mask Works**\n",
        "   - Each position in the attention mask is a binary value:\n",
        "     - `1` means the token at that position is **important** (an actual word or subword).\n",
        "     - `0` means the token is **padding** and should be ignored.\n",
        "   - When the model applies attention, it can use this mask to focus only on meaningful tokens, preventing it from wasting computation on padding tokens.\n",
        "\n",
        "### 3. **Why the Attention Mask is Important**\n",
        "   - **Efficiency**: It helps the model to ignore irrelevant tokens, improving the efficiency of computations.\n",
        "   - **Accuracy**: By focusing only on real tokens, the model can generate more accurate representations of the input text, which improves its ability to understand and respond to the data.\n",
        "\n",
        "### Example\n",
        "\n",
        "Suppose we have the sentence, `\"This is a test\"`, with a maximum length of 6 tokens:\n",
        "\n",
        "- **Tokenized input**: `[This, is, a, test, [PAD], [PAD]]`\n",
        "- **Attention mask**: `[1, 1, 1, 1, 0, 0]`\n",
        "\n",
        "Here, the first four tokens are actual words, while the last two tokens are padding. The attention mask `[1, 1, 1, 1, 0, 0]` tells the model to focus on the first four tokens and ignore the last two during processing.\n",
        "\n",
        "### Key Takeaway\n",
        "The attention mask is essential for helping transformer models handle variable-length inputs efficiently by distinguishing between real content and padding. This allows the model to allocate its attention effectively, improving both computational efficiency and model accuracy."
      ],
      "metadata": {
        "id": "xLGt5Td_rep0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Training Parameters**\n",
        "\n",
        "The `TrainingArguments` class helps configure important settings for training. Here’s what we’ll configure:\n",
        "\n",
        "- **Batch Size**: The number of samples processed together in one forward/backward pass. Smaller batch sizes help prevent memory issues.\n",
        "- **Learning Rate**: Determines the step size at each iteration while moving toward the minimum of the loss function.\n",
        "- **Number of Epochs**: Number of times the model will go through the entire dataset.\n",
        "- **Logging and Evaluation**: How frequently the model logs progress and evaluates performance on the test set."
      ],
      "metadata": {
        "id": "FZ-wYYNsytIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments with W&B logging disabled\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # Output directory for saving results\n",
        "    eval_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
        "    per_device_train_batch_size=8,      # Batch size for training\n",
        "    per_device_eval_batch_size=8,       # Batch size for evaluation\n",
        "    num_train_epochs=1,                 # Number of training epochs\n",
        "    learning_rate=2e-5,                 # Initial learning rate\n",
        "    weight_decay=0.01,                  # Weight decay for regularization\n",
        "    logging_dir=\"./logs\",               # Directory for storing logs\n",
        "    logging_steps=10,                   # Log every 10 steps\n",
        "    report_to=\"none\"                    # Disables W&B logging\n",
        ")\n"
      ],
      "metadata": {
        "id": "iFIFfjpmyeFs"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initialize the `Trainer`**\n",
        "\n",
        "With `training_args` defined, we can initialize the `Trainer` class, passing it:\n",
        "- The **model** we want to fine-tune.\n",
        "- The **training and testing datasets** (`train_dataset` and `test_dataset`).\n",
        "- The **training arguments**.\n",
        "- The **compute_metrics** function to evaluate performance on each epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "mHSwV9qWyUGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Define a compute_metrics function to evaluate performance\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # The model to train\n",
        "    args=training_args,                  # Training arguments\n",
        "    train_dataset=train_dataset,         # Training dataset\n",
        "    eval_dataset=test_dataset,           # Evaluation dataset\n",
        "    compute_metrics=compute_metrics      # Function to compute evaluation metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EptUAFwpG6o",
        "outputId": "149659c9-3c11-48fe-f240-2af98879fb4d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Warning Message\n",
        "\n",
        "The message you’re seeing is normal when using `DistilBERT` or similar models for a classification task. It indicates that **some layers have been newly initialized** because they’re specific to the classification task, and they weren’t part of the pre-trained DistilBERT model.\n",
        "\n",
        "Here’s a bit more context on what this means and why it’s expected:\n",
        "\n",
        "### Explanation of the Message\n",
        "\n",
        "1. **New Layers for Classification**:\n",
        "   - The original `distilbert-base-uncased` model was pre-trained on a general language modeling task, not on a classification task.\n",
        "   - When you load `DistilBertForSequenceClassification`, it adds new layers specifically for classification, such as `classifier` and `pre_classifier`. These layers don’t have pre-trained weights, so they’re initialized randomly.\n",
        "\n",
        "2. **Why This Is Normal**:\n",
        "   - This message is a reminder that because these new layers were initialized randomly, the model should be fine-tuned on your specific classification task (IMDb sentiment analysis, in this case) to learn meaningful weights for those layers.\n",
        "   - Fine-tuning will adjust all the model’s layers (both the pre-trained base and newly added classification layers) to your task, so it’s expected to see this message before training.\n",
        "\n",
        "3. **No Action Required**:\n",
        "   - You don’t need to do anything extra in response to this message; just proceed with training. The `Trainer` will fine-tune the model, updating both the pre-trained and new layers.\n",
        "\n",
        "### Summary\n",
        "\n",
        "This message is simply a reminder that the model should be trained on your task before using it for inference. Since we’re already in the process of fine-tuning, this is expected and requires no further action.\n",
        "\n",
        "Once training is complete, the model’s weights (including the classification layers) will be adjusted based on your IMDb data, making it ready for predictions. You’re good to go—start training, and let me know if you have further questions!"
      ],
      "metadata": {
        "id": "yQLnOOys0nkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Start Training**\n",
        "\n",
        "With the `Trainer` set up, we’re ready to start training. This process will automatically log progress and evaluate the model at each epoch.\n",
        "\n",
        "\n",
        "### Explanation of Key Concepts\n",
        "\n",
        "- **TrainingArguments**: This controls key aspects of training like batch size, logging, and learning rate, helping manage training efficiency and performance.\n",
        "- **Trainer API**: This simplifies the training loop and evaluation, allowing us to focus on configuration rather than manual coding.\n",
        "- **compute_metrics**: Provides metrics like accuracy, precision, recall, and F1 score to help monitor model performance during training.\n",
        "\n",
        "This should start the fine-tuning process! Let me know if you’d like any further explanation on these steps or if you’re ready to start training."
      ],
      "metadata": {
        "id": "zV_g8fwazhuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the duration in minutes\n",
        "duration = (end_time - start_time) / 60\n",
        "print(f\"Tokenization completed in {duration:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "ZBnIuub-pG3x",
        "outputId": "ceabcb92-a457-46c4-f16c-cb63c6cfedb8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 22:07, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.420500</td>\n",
              "      <td>0.447982</td>\n",
              "      <td>0.797600</td>\n",
              "      <td>0.800944</td>\n",
              "      <td>0.792835</td>\n",
              "      <td>0.809221</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization completed in 22.26 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "FvM2uVF81p4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "eval_results = trainer.evaluate()\n",
        "# print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "\n",
        "# Print each metric on a new line\n",
        "print(\"Evaluation Results:\")\n",
        "for metric, value in eval_results.items():\n",
        "    print(f\"{metric}: {value}\")\n",
        "\n",
        "\n",
        "# End the timer\n",
        "# end_time = time.time()\n",
        "\n",
        "# Calculate and print the duration in minutes\n",
        "# duration = (end_time - start_time) / 60\n",
        "print(f\"Tokenization completed in {duration:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "WoiMGeHipG1K",
        "outputId": "687d3cab-7992-4ca6-9d49-d4336e3e7aae"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='163' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 04:59]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.4479823708534241, 'eval_accuracy': 0.7976, 'eval_f1': 0.8009441384736428, 'eval_precision': 0.7928348909657321, 'eval_recall': 0.809220985691574, 'eval_runtime': 275.835, 'eval_samples_per_second': 4.532, 'eval_steps_per_second': 0.569, 'epoch': 1.0}\n",
            "Tokenization completed in 4.60 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Make predictions on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Extract true labels and predicted labels\n",
        "true_labels = predictions.label_ids\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "\n",
        "# Generate and print the classification report\n",
        "report = classification_report(true_labels, predicted_labels, target_names=[\"Negative\", \"Positive\"])\n",
        "print(report)\n",
        "\n",
        "# Calculate and print the duration in minutes\n",
        "duration = (end_time - start_time) / 60\n",
        "print(f\"Tokenization completed in {duration:.2f} minutes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "moCHEq9x1-wH",
        "outputId": "ae635bc4-51e5-4f35-bcba-0ea3c1a28387"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.80      0.79      0.79       621\n",
            "    Positive       0.79      0.81      0.80       629\n",
            "\n",
            "    accuracy                           0.80      1250\n",
            "   macro avg       0.80      0.80      0.80      1250\n",
            "weighted avg       0.80      0.80      0.80      1250\n",
            "\n",
            "Tokenization completed in -1.33 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict"
      ],
      "metadata": {
        "id": "YeJhHJFv2Pd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text for testing\n",
        "sample_text = \"I loved this movie! It was fantastic.\"\n",
        "\n",
        "# Tokenize and make a prediction using the in-memory tokenizer and model\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "outputs = trainer.model(**inputs)\n",
        "predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Map predictions back to labels\n",
        "label = \"Positive\" if predictions[0].item() == 1 else \"Negative\"\n",
        "print(\"Prediction:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Q6vO6w2Rp8",
        "outputId": "8859b7fa-e59f-4b34-fa4d-46a2906d2869"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model & Tokenizer\n",
        "Yes, saved transformer models and their tokenizers can be quite large, particularly for complex models with millions or even billions of parameters. Here’s a breakdown of why they are large and some considerations for managing these sizes.\n",
        "\n",
        "### 1. **Why Are Saved Models So Large?**\n",
        "   - **Number of Parameters**: Even smaller models like DistilBERT have tens of millions of parameters, while larger models like BERT or GPT-3 have hundreds of millions to billions of parameters. Each parameter is typically stored as a 32-bit floating-point number, so the more parameters, the larger the storage requirements.\n",
        "   - **Tokenizers**: Tokenizer files can also be large, as they include vocabulary files and additional configurations necessary for consistent tokenization. Although they’re smaller than the model itself, they still contribute to the overall storage size.\n",
        "   - **Configuration and Metadata**: Transformer models save not just the model weights but also configuration files (e.g., model architecture, hyperparameters) and training metadata, which, while minor, add to the file size.\n",
        "\n",
        "### 2. **Typical Size for Saved Models**\n",
        "   - **DistilBERT**: DistilBERT models, for instance, are usually around 200-300 MB when saved, depending on the specific fine-tuning layers added.\n",
        "   - **BERT-Base**: Full BERT models are around 400-500 MB.\n",
        "   - **Large Language Models (e.g., GPT-2, GPT-3)**: Models like GPT-2 and GPT-3 can reach several gigabytes, making storage and loading times significant.\n",
        "\n",
        "### 3. **Options for Reducing Model Size**\n",
        "If storage or deployment is a concern, there are some strategies for managing model sizes:\n",
        "\n",
        "   - **Model Distillation**: Create a smaller, distilled version of the model that approximates the performance of the larger model but has fewer parameters (e.g., DistilBERT is a distilled version of BERT).\n",
        "   - **Quantization**: Reduce the precision of model weights from 32-bit to 8-bit, which can significantly reduce model size without much impact on performance.\n",
        "   - **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like LoRA (Low-Rank Adaptation) add only a few new parameters, so you only need to store these additional parameters rather than the entire model.\n",
        "   - **Using Tokenizer Cache**: Hugging Face models store tokenizers and model weights in a cache directory, so you don’t need to download them each time if they’re already cached.\n",
        "\n",
        "### 4. **Best Practices for Handling Large Models**\n",
        "   - **Save Selectively**: Save the model and tokenizer only once when they’re fully trained and needed for deployment or inference.\n",
        "   - **Cloud Storage**: Use cloud storage solutions (e.g., AWS S3, Google Cloud Storage) to store models if you’re working with many or very large models.\n",
        "   - **On-Demand Loading**: If the model is only needed intermittently, you can load it into memory only when required to save memory and processing power.\n",
        "\n"
      ],
      "metadata": {
        "id": "Swf-h2yU12bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./finetuned-imdb-distilbert\")  # Saves model weights and configuration\n",
        "tokenizer.save_pretrained(\"./finetuned-imdb-distilbert\")  # Saves tokenizer"
      ],
      "metadata": {
        "id": "YZORUdOD16e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Model for Inference"
      ],
      "metadata": {
        "id": "GwvpdCOY1vYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned-imdb-distilbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned-imdb-distilbert\")\n",
        "\n",
        "# Example text for testing\n",
        "sample_text = \"I loved this movie! It was fantastic.\"\n",
        "\n",
        "# Tokenize and make a prediction\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "outputs = model(**inputs)\n",
        "predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Map predictions back to labels\n",
        "label = \"Positive\" if predictions[0].item() == 1 else \"Negative\"\n",
        "print(\"Prediction:\", label)\n"
      ],
      "metadata": {
        "id": "rkrq2SUB1uOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Script\n",
        "Creating a training script is a great idea, especially if you plan to fine-tune multiple models or datasets. However, each dataset will have unique requirements, so the key is to build the script in a **modular and flexible** way. Here’s a strategy for achieving that:\n",
        "\n",
        "### Why Use a Training Script?\n",
        "\n",
        "1. **Efficiency**: A script helps automate the repetitive parts of model training, like loading data, setting up the model, defining training parameters, and evaluating performance.\n",
        "2. **Consistency**: By standardizing your training process, you can ensure consistent steps across different experiments, which improves the reproducibility of results.\n",
        "3. **Modularity**: You can build the script to support optional configurations, making it adaptable to different datasets or tasks without modifying the core structure.\n",
        "\n",
        "### Key Components of a Flexible Training Script\n",
        "\n",
        "To make the script adaptable to unique dataset needs, consider structuring it with customizable functions and parameter options.\n",
        "\n",
        "#### 1. **Configurable Parameters**\n",
        "   - Use a configuration file (e.g., JSON, YAML) or a command-line argument parser (`argparse`) to specify parameters like the model name, batch size, number of epochs, learning rate, and dataset location.\n",
        "   - These configurable parameters let you adjust the script for each dataset or model by changing only the configuration, not the code.\n",
        "\n",
        "#### 2. **Dataset-Specific Preprocessing Functions**\n",
        "   - Each dataset may have unique preprocessing requirements (e.g., tokenization settings, label encoding). Write separate preprocessing functions or classes that can be called based on the dataset.\n",
        "   - Example: Use a function to handle tokenization, label encoding, and attention mask generation specific to the input data.\n",
        "\n",
        "#### 3. **Flexible Model and Tokenizer Loading**\n",
        "   - Make the model loading step configurable so you can easily swap between models (e.g., BERT, DistilBERT, GPT-2).\n",
        "   - Define the model in the script based on a parameter, such as `model_name`, which allows you to specify the desired pre-trained model.\n",
        "\n",
        "#### 4. **Training and Evaluation Functions**\n",
        "   - Wrap the training loop, evaluation, and metric calculation in reusable functions that can be called with different datasets or configurations.\n",
        "   - Use `Trainer` or define a custom training loop, depending on the level of customization you need.\n",
        "\n",
        "#### 5. **Save and Load Functionality**\n",
        "   - Add save functions to export the fine-tuned model and tokenizer, along with a function to load the model and tokenizer for testing or inference.\n",
        "   - Optional: Add checkpoints or early stopping if the training process is long or resource-intensive.\n",
        "\n",
        "\n",
        "### Explanation of Each Section\n",
        "\n",
        "- **Parameter Parsing**: `parse_args` lets you customize the script’s parameters without editing the code directly, supporting flexibility.\n",
        "- **Data Loading and Preprocessing**: `load_and_preprocess_data` can include dataset-specific preprocessing logic, like handling unique labels or special tokenization requirements.\n",
        "- **Model Training and Evaluation**: `train_model` and `evaluate_model` manage training and provide an evaluation summary, making it easy to swap datasets without changing core logic.\n",
        "- **Modularity**: Each function is self-contained, enabling easy updates or replacements for individual components.\n",
        "\n",
        "### Final Considerations\n",
        "\n",
        "While each dataset will have specific needs, this modular approach allows you to plug in custom preprocessing functions, change model configurations, and control training parameters without rewriting the entire script each time. It combines efficiency with flexibility, making it ideal for handling multiple datasets while supporting experimentation.\n",
        "\n",
        "Let me know if you'd like further guidance on implementing or adapting any specific part!"
      ],
      "metadata": {
        "id": "yQUT2bS53uJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "\n",
        "# Step 1: Parse Configurations\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"distilbert-base-uncased\")\n",
        "    parser.add_argument(\"--dataset_name\", type=str, default=\"imdb\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "    parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./model_output\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "def load_and_preprocess_data(dataset_name, tokenizer):\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    # Tokenize the dataset (define based on dataset requirements)\n",
        "    # Return tokenized train and test splits\n",
        "    pass\n",
        "\n",
        "# Step 3: Initialize Model and Trainer\n",
        "def train_model(model_name, train_dataset, test_dataset, args):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        learning_rate=args.learning_rate,\n",
        "        num_train_epochs=args.epochs,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return model, trainer\n",
        "\n",
        "# Step 4: Evaluation\n",
        "def evaluate_model(trainer, test_dataset):\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    true_labels = predictions.label_ids\n",
        "    predicted_labels = torch.argmax(predictions.predictions, axis=1)\n",
        "    report = classification_report(true_labels, predicted_labels)\n",
        "    print(report)\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "    train_dataset, test_dataset = load_and_preprocess_data(args.dataset_name, tokenizer)\n",
        "    model, trainer = train_model(args.model_name, train_dataset, test_dataset, args)\n",
        "    evaluate_model(trainer, test_dataset)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "tO3fy2Xp1uL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parse Args Function\n",
        "\n",
        "This `parse_args` function is designed to create a **configurable and flexible training script**. Here are the main lessons to focus on:\n",
        "\n",
        "### 1. **Parameterizing the Script**\n",
        "   - By using `argparse`, this code allows you to pass in different arguments from the command line, such as `model_name`, `dataset_name`, `batch_size`, etc., without changing the code directly. This makes the script adaptable and reusable, which is important when training on various datasets or experimenting with different configurations.\n",
        "   - Each argument corresponds to an essential parameter in model training, such as the model architecture, dataset choice, training hyperparameters, and output location.\n",
        "\n",
        "### 2. **Encouraging Experimentation and Reproducibility**\n",
        "   - **Experimentation**: With these arguments, you can quickly experiment with different models, learning rates, batch sizes, or other hyperparameters by passing in different values, which speeds up experimentation.\n",
        "   - **Reproducibility**: Defining these parameters explicitly allows you to easily log and track different experimental setups, making it easier to replicate results in the future.\n",
        "\n",
        "### 3. **Scalability**\n",
        "   - If you need to scale up and manage multiple training runs, this approach is efficient because it allows you to use a single script with various parameter combinations.\n",
        "   - This pattern makes it possible to run the script programmatically, for instance, by calling it within a larger pipeline or loop that automates experiments over a range of values for each parameter.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Modularity and Reusability**: The function is modular, making the script reusable across different datasets and models.\n",
        "- **Efficiency**: It removes the need to edit code directly when changing configurations, reducing human error and saving time.\n",
        "- **Experiment-Friendly**: This approach is essential for machine learning experiments, where adjusting parameters and tracking configurations is crucial.\n",
        "\n",
        "By using `argparse` effectively, you’re setting up a robust foundation for systematic experimentation and quick adaptability in your training pipeline.\n",
        "\n",
        "When you run a Python script with `argparse`, you can specify different argument values directly in the command line. Here are some examples based on the arguments in the `parse_args()` function.\n",
        "\n",
        "Assuming your script is saved as `train_model.py`, you can adjust the arguments as follows:\n",
        "\n",
        "### 1. Basic Command with Default Arguments\n",
        "If you simply run:\n",
        "```bash\n",
        "python train_model.py\n",
        "```\n",
        "It will use all the default values defined in `parse_args()`:\n",
        "- `model_name`: `\"distilbert-base-uncased\"`\n",
        "- `dataset_name`: `\"imdb\"`\n",
        "- `batch_size`: `8`\n",
        "- `learning_rate`: `2e-5`\n",
        "- `epochs`: `3`\n",
        "- `output_dir`: `\"./model_output\"`\n",
        "\n",
        "### 2. Specifying a Different Model and Dataset\n",
        "To change the model and dataset, you can override `--model_name` and `--dataset_name`:\n",
        "```bash\n",
        "python train_model.py --model_name \"bert-base-uncased\" --dataset_name \"yelp_polarity\"\n",
        "```\n",
        "This will load the `bert-base-uncased` model and the `yelp_polarity` dataset for training.\n",
        "\n",
        "### 3. Adjusting Hyperparameters for Faster Training\n",
        "If you want to experiment with a larger batch size and fewer epochs, you could specify:\n",
        "```bash\n",
        "python train_model.py --batch_size 16 --epochs 2\n",
        "```\n",
        "This will train with a batch size of 16 and run for only 2 epochs.\n",
        "\n",
        "### 4. Trying a Different Learning Rate\n",
        "If you want to test a different learning rate (for example, a higher one), you can set `--learning_rate`:\n",
        "```bash\n",
        "python train_model.py --learning_rate 5e-5\n",
        "```\n",
        "This will set the learning rate to `5e-5` instead of the default `2e-5`.\n",
        "\n",
        "### 5. Changing the Output Directory\n",
        "You can change where the model and logs are saved by specifying `--output_dir`:\n",
        "```bash\n",
        "python train_model.py --output_dir \"./my_experiment\"\n",
        "```\n",
        "This will save all outputs to `./my_experiment` rather than the default `./model_output`.\n",
        "\n",
        "### 6. Combining Multiple Argument Changes\n",
        "You can change several arguments at once. For example:\n",
        "```bash\n",
        "python train_model.py --model_name \"roberta-base\" --dataset_name \"ag_news\" --batch_size 32 --learning_rate 3e-5 --epochs 5 --output_dir \"./experiment_roberta\"\n",
        "```\n",
        "This command will:\n",
        "- Load the `roberta-base` model.\n",
        "- Use the `ag_news` dataset.\n",
        "- Set batch size to 32, learning rate to `3e-5`, and epochs to 5.\n",
        "- Save results in the `./experiment_roberta` directory.\n",
        "\n",
        "### General Tips for Argument Variations\n",
        "- **Testing Parameters**: For quick tests, lower values for `--epochs` and `--batch_size` can speed up initial trials.\n",
        "- **Different Output Directories**: Use unique output directories for each run so that you can compare results without overwriting previous ones.\n",
        "\n",
        "Using these commands in the terminal makes it easy to experiment with different settings and keep track of how each variation affects model performance."
      ],
      "metadata": {
        "id": "EAZigqD74_up"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3ZF58n-1uJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OQBD_UPU1uGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# End the timer and calculate duration\n",
        "end_time = time.time()\n",
        "duration = (end_time - start_time) / 60  # Convert seconds to minutes\n",
        "print(f\"Dataset and model loaded in {duration:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "1rvj-K-5hrj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mrVNRWrGhrg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSueFdSjhrc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}