{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_038_langchain_few_shot_prompt_templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Roles & Prompt Templates\n",
        "\n",
        "Understanding the **system**, **user**, and **assistant** roles in LLMs is essential to grasping **LangChain templates** and their importance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Roles in LLMs**\n",
        "1. **System Role**\n",
        "   - **Purpose**: Defines the behavior, tone, and personality of the assistant.\n",
        "   - **Example**: `\"You are a helpful assistant that explains complex concepts in simple terms.\"`\n",
        "   - **Impact**: This role sets the \"rules\" for how the model behaves throughout the conversation.\n",
        "\n",
        "2. **User Role**\n",
        "   - **Purpose**: Represents the input or query from the user.\n",
        "   - **Example**: `\"Can you explain how photosynthesis works?\"`\n",
        "   - **Impact**: This is the core of what the model responds to.\n",
        "\n",
        "3. **Assistant Role**\n",
        "   - **Purpose**: Represents the model’s response to the user.\n",
        "   - **Example**: `\"Photosynthesis is the process by which plants convert sunlight into energy.\"`\n",
        "   - **Impact**: The assistant role helps maintain context in multi-turn conversations.\n",
        "\n",
        "---\n",
        "\n",
        "### **How LangChain Templates Relate to These Roles**\n",
        "LangChain templates use these roles to **structure and program interactions** with LLMs. They enable developers to build modular, reusable, and dynamic workflows that guide the model’s behavior and ensure clarity in communication.\n",
        "\n",
        "---\n",
        "\n",
        "### **Importance of LangChain and Prompt Templates**\n",
        "\n",
        "#### **1. Define the Assistant’s Behavior (System Role)**\n",
        "   - **Why It Matters**: The system role shapes the overall tone and capabilities of the assistant.\n",
        "   - **How Templates Help**: LangChain allows you to programmatically define the system message to create assistants tailored for specific tasks (e.g., recipe assistant, technical tutor, chatbot).\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import SystemMessagePromptTemplate\n",
        "     system_template = SystemMessagePromptTemplate.from_template(\n",
        "         \"You are a math tutor who explains concepts step-by-step.\"\n",
        "     )\n",
        "     ```\n",
        "\n",
        "#### **2. Handle Dynamic User Inputs (User Role)**\n",
        "   - **Why It Matters**: User queries vary, and templates allow dynamic placeholders for flexibility.\n",
        "   - **How Templates Help**: LangChain makes it easy to insert user inputs into a structured format.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import HumanMessagePromptTemplate\n",
        "     human_template = HumanMessagePromptTemplate.from_template(\n",
        "         \"Explain the concept of {topic}.\"\n",
        "     )\n",
        "     formatted = human_template.format(topic=\"gravity\")\n",
        "     print(formatted)  # Output: \"Explain the concept of gravity.\"\n",
        "     ```\n",
        "\n",
        "#### **3. Combine Roles into a Conversation Flow**\n",
        "   - **Why It Matters**: Many tasks require combining the system, user, and assistant roles for context and continuity.\n",
        "   - **How Templates Help**: LangChain’s `ChatPromptTemplate` combines multiple roles into a unified template, enabling structured multi-turn conversations.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "     chat_prompt = ChatPromptTemplate.from_messages([\n",
        "         SystemMessagePromptTemplate.from_template(\n",
        "             \"You are a helpful assistant that specializes in answering scientific questions.\"\n",
        "         ),\n",
        "         HumanMessagePromptTemplate.from_template(\n",
        "             \"Can you explain {topic} in simple terms?\"\n",
        "         )\n",
        "     ])\n",
        "     formatted_prompt = chat_prompt.format_prompt(topic=\"black holes\").to_messages()\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why LangChain Templates are Powerful**\n",
        "1. **Control Behavior**:\n",
        "   - By defining the system message, you can ensure the LLM behaves consistently (e.g., as a friendly assistant, technical expert, or creative writer).\n",
        "\n",
        "2. **Reusable Components**:\n",
        "   - Templates modularize prompts so they can be reused across tasks, saving time and reducing errors.\n",
        "\n",
        "3. **Dynamic Inputs**:\n",
        "   - Handle varying user queries by using placeholders like `{topic}` or `{name}`, making the assistant flexible and adaptive.\n",
        "\n",
        "4. **Complex Workflows**:\n",
        "   - Combine multiple roles into one structured interaction for tasks requiring context and continuity (e.g., customer support, tutoring, brainstorming).\n",
        "\n",
        "5. **Scalability**:\n",
        "   - Templates make it easy to maintain and extend applications as complexity grows.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-qR8LuoB7OM"
      },
      "id": "Y-qR8LuoB7OM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "-55MiGi0clP_"
      },
      "id": "-55MiGi0clP_"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain-openai"
      ],
      "metadata": {
        "id": "2xy1mJj_AVPk"
      },
      "id": "2xy1mJj_AVPk",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "RBm9CWQJcqwL"
      },
      "id": "RBm9CWQJcqwL"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "import json\n",
        "import langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# Set the environment variable globally for libraries like LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# Print the API key to confirm it's loaded correctly\n",
        "print(\"API Key loaded from .env:\",os.environ[\"OPENAI_API_KEY\"][0:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sInrAOplDA0m",
        "outputId": "9cf68529-8556-4f7c-8e90-020727e5e18e"
      },
      "id": "sInrAOplDA0m",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded from .env: sk-proj-e1GUWruINPRnrozmiakkRM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Shot Prompt Template"
      ],
      "metadata": {
        "id": "tL84LPgOktVC"
      },
      "id": "tL84LPgOktVC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define system and human message templates\n",
        "system_template = (\n",
        "    \"You are an AI recipe assistant that specializes in {dietary_preference} dishes \"\n",
        "    \"that can be prepared in {cooking_time}.\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_template = \"{recipe_request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# Step 2: Combine templates into a chat prompt\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# Step 3: Format the prompt with user inputs\n",
        "formatted_prompt = chat_prompt.format_prompt(\n",
        "    dietary_preference=\"Vegan\",\n",
        "    cooking_time=\"15 min\",\n",
        "    recipe_request=\"I need a quick snack idea.\"\n",
        ").to_messages()\n",
        "\n",
        "# Step 4: Initialize the chat model\n",
        "chat = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Step 5: Get a response from the model\n",
        "response = chat(formatted_prompt)\n",
        "\n",
        "# Step 6: Print the response content\n",
        "print(\"AI Recipe Suggestion:\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFbZK9dRiUfY",
        "outputId": "57e68d25-5113-4f0e-ab56-93906866c1fc"
      },
      "id": "wFbZK9dRiUfY",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Recipe Suggestion:\n",
            "How about making a **Vegan Avocado Toast**? It's quick, delicious, and packed with nutrients! Here’s how to whip it up in just 15 minutes:\n",
            "\n",
            "### Ingredients:\n",
            "- 1 ripe avocado\n",
            "- 2 slices of whole grain or gluten-free bread\n",
            "- Salt and pepper, to taste\n",
            "- Optional toppings: cherry tomatoes, radishes, red pepper flakes, lemon juice, or fresh herbs (like cilantro or basil)\n",
            "\n",
            "### Instructions:\n",
            "1. **Toast the Bread**: Start by toasting your bread slices until they are golden brown and crispy.\n",
            "\n",
            "2. **Prepare the Avocado**: While the bread is toasting, cut the avocado in half, remove the pit, and scoop the flesh into a bowl. Mash it with a fork until smooth or leave it slightly chunky, depending on your preference.\n",
            "\n",
            "3. **Season the Avocado**: Add salt and pepper to taste. If you like, squeeze in a bit of lemon juice for extra flavor and to prevent browning.\n",
            "\n",
            "4. **Assemble the Toast**: Once the bread is toasted, spread the mashed avocado evenly on each slice.\n",
            "\n",
            "5. **Add Toppings**: Top with your choice of cherry tomatoes, sliced radishes, a sprinkle of red pepper flakes, or fresh herbs for added flavor.\n",
            "\n",
            "6. **Serve**: Enjoy your avocado toast as a quick and nutritious snack!\n",
            "\n",
            "This snack is not only quick to make but also filling and satisfying!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few Shot Prompt Templates\n",
        "\n",
        "Here are the **most important lessons** you should take away from this code:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Context Matters**\n",
        "- The **system message** defines the assistant's behavior. By stating that it translates complex legal terms, you establish the assistant's \"role\" and set the tone for the entire interaction.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Few-Shot Learning**\n",
        "- Providing **example input-output pairs** (legal text and its plain-language translation) teaches the model how to respond in similar contexts. These examples act as a demonstration of the desired behavior.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Dynamic Input Handling**\n",
        "- The **human message template** uses placeholders (`{legal_text}`) to dynamically insert new text, allowing for reusable prompts that can adapt to different inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Structured Prompt Construction**\n",
        "- Combining the system message, examples, and human input into a **chat prompt** creates a cohesive, multi-part input that the model can understand and respond to effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Model Invocation**\n",
        "- Using LangChain's `ChatOpenAI` and `invoke` method sends the formatted prompt to the model, ensuring the request adheres to best practices and is compatible with the latest updates.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Modular and Scalable Design**\n",
        "- By breaking the process into steps (system setup, examples, dynamic input, response generation), the code becomes modular, reusable, and scalable for similar tasks in other domains.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**\n",
        "This code demonstrates how to effectively combine **role definition**, **few-shot learning**, and **dynamic prompt structuring** to guide an LLM toward generating specific, high-quality responses for complex tasks like translating legal text."
      ],
      "metadata": {
        "id": "oNL8Xf6vpF9T"
      },
      "id": "oNL8Xf6vpF9T"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the system message\n",
        "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable terms.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# Step 2: Define an example input-output pair\n",
        "legal_text = \"The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
        "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
        "\n",
        "plain_text = \"The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\"\n",
        "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
        "\n",
        "# Step 3: Define the human message template\n",
        "human_template = \"{legal_text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# Step 4: Combine all messages into a chat prompt\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
        ")\n",
        "\n",
        "# Step 5: Format the prompt with new legal text\n",
        "some_example_text = (\n",
        "    \"The grantor, being the fee simple owner of the real property herein described, \"\n",
        "    \"conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, \"\n",
        "    \"and interest in and to the said property, subject to all existing encumbrances, liens, and easements, \"\n",
        "    \"as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions \"\n",
        "    \"affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\n",
        ")\n",
        "request = chat_prompt.format_prompt(legal_text=some_example_text).to_messages()\n",
        "\n",
        "# Step 6: Initialize the chat model\n",
        "chat = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Step 7: Get a response from the model\n",
        "response = chat.invoke(request)  # Use `invoke` instead of the deprecated `__call__`\n",
        "\n",
        "# Step 8: Print the request content\n",
        "for message in request:\n",
        "    if isinstance(message, SystemMessage):\n",
        "        print(\"System Prompt:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        print(\"User Prompt:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(\"AI Response:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    else:\n",
        "        print(\"Unknown message type:\", message)\n",
        "        print()\n",
        "\n",
        "# Step 9: Print the response content\n",
        "print(\"AI Translation of Legal Text:\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3WpNwk6mSn-",
        "outputId": "28183dec-5e21-40a3-b3a9-e0d0791e86af"
      },
      "id": "x3WpNwk6mSn-",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Prompt:\n",
            "You are a helpful assistant that translates complex legal terms into plain and understandable terms.\n",
            "\n",
            "User Prompt:\n",
            "The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\n",
            "\n",
            "AI Response:\n",
            "The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\n",
            "\n",
            "User Prompt:\n",
            "The grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, and interest in and to the said property, subject to all existing encumbrances, liens, and easements, as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\n",
            "\n",
            "AI Translation of Legal Text:\n",
            "The seller, who fully owns the property described here, transfers all their rights and ownership of that property to the buyer and their future heirs or assignees. This transfer is subject to any current claims, debts, or easements recorded in the county's official records, as well as any rules or restrictions that apply to the property. This transfer is made in exchange for the amount of [purchase price] that the buyer has paid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Few-Shot Learning**\n",
        "- **Definition**: Few-shot learning refers to providing a small number of **examples** directly in the prompt (during inference, not training) to guide the model on how to respond.\n",
        "- **How It Works**:\n",
        "  - You include **example input-output pairs** in the prompt as a demonstration of the task.\n",
        "  - The model uses these examples as context to generate responses that follow the same pattern.\n",
        "- **Key Characteristics**:\n",
        "  - Happens **at runtime** (no model weights are updated).\n",
        "  - Requires the model to generalize from the provided examples.\n",
        "  - Does not permanently modify the model.\n",
        "\n",
        "- **Example**:\n",
        "  ```python\n",
        "  prompt = \"\"\"\n",
        "  Translate legal terms into plain language:\n",
        "  \n",
        "  Legal: The provisions herein shall be severable.\n",
        "  Plain: The rules in this agreement can be separated.\n",
        "  \n",
        "  Legal: The grantor conveys and warrants the property to the grantee.\n",
        "  Plain: [Your turn]\n",
        "  \"\"\"\n",
        "  ```\n",
        "\n",
        "- **Analogy**: It's like giving the model a few worked-out examples before asking it to solve a new problem.\n",
        "\n",
        "---\n",
        "\n",
        "### **Fine-Tuning**\n",
        "- **Definition**: Fine-tuning involves retraining the model on a **specific dataset** (even if it's small) to optimize it for a particular task or domain.\n",
        "- **How It Works**:\n",
        "  - You provide a labeled dataset of input-output pairs.\n",
        "  - The model's weights are updated during the training process to improve performance on that specific task.\n",
        "- **Key Characteristics**:\n",
        "  - Happens **offline** (requires training the model, not just using it).\n",
        "  - Produces a custom version of the model that retains its new behavior permanently.\n",
        "  - Requires computational resources and expertise.\n",
        "\n",
        "- **Example**:\n",
        "  - Fine-tuning GPT-3 to specialize in translating legal text by training it on a dataset of thousands of legal text/plain-language pairs.\n",
        "\n",
        "- **Analogy**: It's like teaching a person a new skill through focused practice sessions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison of Few-Shot Learning and Fine-Tuning**\n",
        "\n",
        "| **Aspect**               | **Few-Shot Learning**                     | **Fine-Tuning**                    |\n",
        "|---------------------------|-------------------------------------------|-------------------------------------|\n",
        "| **Process**               | Examples provided in the prompt at runtime. | Model is retrained on specific data. |\n",
        "| **Model Modification**    | No (uses the same base model).            | Yes (updates model weights).       |\n",
        "| **Customization**         | Temporary and specific to the given prompt. | Permanent for the fine-tuned task. |\n",
        "| **Data Requirement**      | Very few examples (1–10).                 | Requires a larger labeled dataset. |\n",
        "| **Flexibility**           | Can switch tasks easily by changing the prompt. | Task-specific; not flexible.       |\n",
        "| **Infrastructure**        | Lightweight and fast (no retraining).     | Requires compute power for training. |\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Use Each**\n",
        "1. **Few-Shot Learning**:\n",
        "   - Use it when:\n",
        "     - You need to customize behavior temporarily.\n",
        "     - You don’t have the resources to fine-tune.\n",
        "     - The model already performs decently on the task but just needs guidance.\n",
        "\n",
        "2. **Fine-Tuning**:\n",
        "   - Use it when:\n",
        "     - You need consistent, optimized performance for a specific task.\n",
        "     - You have access to a high-quality dataset.\n",
        "     - You can afford the time and resources for training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**\n",
        "Few-shot learning is about **demonstrating the task** during runtime using examples, while fine-tuning is about **retraining the model** to specialize in the task permanently. Few-shot learning is quick, flexible, and resource-efficient, whereas fine-tuning offers deeper, task-specific optimization."
      ],
      "metadata": {
        "id": "I3qVIpx7pnrr"
      },
      "id": "I3qVIpx7pnrr"
    },
    {
      "cell_type": "code",
      "source": [
        " from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Step 1: Define the system message\n",
        "template = \"You are a helpful assistant that translates complex programming concepts into plain and understandable terms.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# Step 2: Define an example input-output pair\n",
        "technical_text = (\n",
        "    \"A binary search algorithm finds the position of a target value within a sorted array. \"\n",
        "    \"It compares the target value to the middle element of the array. If they are not equal, \"\n",
        "    \"the half in which the target cannot lie is eliminated and the search continues on the remaining half, \"\n",
        "    \"again taking the middle element to compare to the target value. This process is repeated until the target value is found.\"\n",
        ")\n",
        "example_input_one = HumanMessagePromptTemplate.from_template(technical_text)\n",
        "\n",
        "plain_text = (\n",
        "    \"Binary search is a way to find something in a sorted list quickly. \"\n",
        "    \"You look at the middle of the list and decide if the thing you're looking for is on the left or the right side. \"\n",
        "    \"Then you keep checking the middle of the smaller list until you find it.\"\n",
        ")\n",
        "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
        "\n",
        "# Step 3: Define the human message template\n",
        "human_template = \"{technical_text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# Step 4: Combine all messages into a chat prompt\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
        ")\n",
        "\n",
        "# Step 5: Format the prompt with new technical text\n",
        "new_technical_text = (\n",
        "    \"Object-oriented programming (OOP) is a paradigm that organizes software design around data, \"\n",
        "    \"or objects, rather than functions and logic. An object can be defined as a data field that has unique attributes \"\n",
        "    \"and behavior. OOP focuses on the objects developers want to manipulate rather than the logic required to manipulate them, \"\n",
        "    \"and it is centered around four main concepts: encapsulation, abstraction, inheritance, and polymorphism.\"\n",
        ")\n",
        "request = chat_prompt.format_prompt(technical_text=new_technical_text).to_messages()\n",
        "\n",
        "# Step 6: Initialize the chat model\n",
        "chat = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Step 7: Get a response from the model\n",
        "response = chat.invoke(request)  # Use `invoke` instead of the deprecated `__call__`\n",
        "\n",
        "# Step 8: Print the request content\n",
        "for message in request:\n",
        "    if isinstance(message, SystemMessage):\n",
        "        print(\"System Prompt:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        print(\"User Prompt:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(\"AI Response:\")\n",
        "        print(message.content)\n",
        "        print()\n",
        "    else:\n",
        "        print(\"Unknown message type:\", message)\n",
        "        print()\n",
        "\n",
        "# Step 9: Print the response content\n",
        "print(\"AI Simplification of Technical Text:\")\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG_rfi-ooMXY",
        "outputId": "149f7223-5beb-439c-945f-d7eae1109aaf"
      },
      "id": "aG_rfi-ooMXY",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System Prompt:\n",
            "You are a helpful assistant that translates complex programming concepts into plain and understandable terms.\n",
            "\n",
            "User Prompt:\n",
            "A binary search algorithm finds the position of a target value within a sorted array. It compares the target value to the middle element of the array. If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, again taking the middle element to compare to the target value. This process is repeated until the target value is found.\n",
            "\n",
            "AI Response:\n",
            "Binary search is a way to find something in a sorted list quickly. You look at the middle of the list and decide if the thing you're looking for is on the left or the right side. Then you keep checking the middle of the smaller list until you find it.\n",
            "\n",
            "User Prompt:\n",
            "Object-oriented programming (OOP) is a paradigm that organizes software design around data, or objects, rather than functions and logic. An object can be defined as a data field that has unique attributes and behavior. OOP focuses on the objects developers want to manipulate rather than the logic required to manipulate them, and it is centered around four main concepts: encapsulation, abstraction, inheritance, and polymorphism.\n",
            "\n",
            "AI Simplification of Technical Text:\n",
            "Object-oriented programming (OOP) is a way to design software that focuses on \"objects\" instead of just actions or functions. \n",
            "\n",
            "1. **Objects**: Think of an object as a package that contains both data (like properties or characteristics) and actions (like methods or behaviors). For example, a \"Car\" object might have attributes like color and model, and behaviors like drive and stop.\n",
            "\n",
            "2. **Encapsulation**: This concept means keeping the details of how an object works hidden from the outside. You can interact with the object through a well-defined interface, which helps protect the data and keeps everything organized.\n",
            "\n",
            "3. **Abstraction**: This simplifies complex reality by providing a clear and simplified view of an object. You only need to know what an object does, not how it does it. For instance, you know how to drive a car without needing to understand the mechanics of the engine.\n",
            "\n",
            "4. **Inheritance**: This allows new objects to take on properties and behaviors from existing objects. For example, if you have a \"Vehicle\" object, you could create a \"Car\" object that inherits from it, meaning the car has all the basic features of a vehicle but can also have its own specific features.\n",
            "\n",
            "5. **Polymorphism**: This allows objects to be treated as instances of their parent class, even though they might be different types. It means that a single function can work with different types of objects, making the code more flexible and reusable.\n",
            "\n",
            "In summary, OOP helps developers create programs that are easier to manage and expand by organizing code around objects that represent real-world things.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nc6G3NoXzfg6"
      },
      "id": "nc6G3NoXzfg6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}