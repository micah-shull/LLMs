{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_037_langchain_promp_templates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Roles & Prompt Templates\n",
        "\n",
        "Understanding the **system**, **user**, and **assistant** roles in LLMs is essential to grasping **LangChain templates** and their importance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Roles in LLMs**\n",
        "1. **System Role**\n",
        "   - **Purpose**: Defines the behavior, tone, and personality of the assistant.\n",
        "   - **Example**: `\"You are a helpful assistant that explains complex concepts in simple terms.\"`\n",
        "   - **Impact**: This role sets the \"rules\" for how the model behaves throughout the conversation.\n",
        "\n",
        "2. **User Role**\n",
        "   - **Purpose**: Represents the input or query from the user.\n",
        "   - **Example**: `\"Can you explain how photosynthesis works?\"`\n",
        "   - **Impact**: This is the core of what the model responds to.\n",
        "\n",
        "3. **Assistant Role**\n",
        "   - **Purpose**: Represents the model’s response to the user.\n",
        "   - **Example**: `\"Photosynthesis is the process by which plants convert sunlight into energy.\"`\n",
        "   - **Impact**: The assistant role helps maintain context in multi-turn conversations.\n",
        "\n",
        "---\n",
        "\n",
        "### **How LangChain Templates Relate to These Roles**\n",
        "LangChain templates use these roles to **structure and program interactions** with LLMs. They enable developers to build modular, reusable, and dynamic workflows that guide the model’s behavior and ensure clarity in communication.\n",
        "\n",
        "---\n",
        "\n",
        "### **Importance of LangChain and Prompt Templates**\n",
        "\n",
        "#### **1. Define the Assistant’s Behavior (System Role)**\n",
        "   - **Why It Matters**: The system role shapes the overall tone and capabilities of the assistant.\n",
        "   - **How Templates Help**: LangChain allows you to programmatically define the system message to create assistants tailored for specific tasks (e.g., recipe assistant, technical tutor, chatbot).\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import SystemMessagePromptTemplate\n",
        "     system_template = SystemMessagePromptTemplate.from_template(\n",
        "         \"You are a math tutor who explains concepts step-by-step.\"\n",
        "     )\n",
        "     ```\n",
        "\n",
        "#### **2. Handle Dynamic User Inputs (User Role)**\n",
        "   - **Why It Matters**: User queries vary, and templates allow dynamic placeholders for flexibility.\n",
        "   - **How Templates Help**: LangChain makes it easy to insert user inputs into a structured format.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import HumanMessagePromptTemplate\n",
        "     human_template = HumanMessagePromptTemplate.from_template(\n",
        "         \"Explain the concept of {topic}.\"\n",
        "     )\n",
        "     formatted = human_template.format(topic=\"gravity\")\n",
        "     print(formatted)  # Output: \"Explain the concept of gravity.\"\n",
        "     ```\n",
        "\n",
        "#### **3. Combine Roles into a Conversation Flow**\n",
        "   - **Why It Matters**: Many tasks require combining the system, user, and assistant roles for context and continuity.\n",
        "   - **How Templates Help**: LangChain’s `ChatPromptTemplate` combines multiple roles into a unified template, enabling structured multi-turn conversations.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "     chat_prompt = ChatPromptTemplate.from_messages([\n",
        "         SystemMessagePromptTemplate.from_template(\n",
        "             \"You are a helpful assistant that specializes in answering scientific questions.\"\n",
        "         ),\n",
        "         HumanMessagePromptTemplate.from_template(\n",
        "             \"Can you explain {topic} in simple terms?\"\n",
        "         )\n",
        "     ])\n",
        "     formatted_prompt = chat_prompt.format_prompt(topic=\"black holes\").to_messages()\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Why LangChain Templates are Powerful**\n",
        "1. **Control Behavior**:\n",
        "   - By defining the system message, you can ensure the LLM behaves consistently (e.g., as a friendly assistant, technical expert, or creative writer).\n",
        "\n",
        "2. **Reusable Components**:\n",
        "   - Templates modularize prompts so they can be reused across tasks, saving time and reducing errors.\n",
        "\n",
        "3. **Dynamic Inputs**:\n",
        "   - Handle varying user queries by using placeholders like `{topic}` or `{name}`, making the assistant flexible and adaptive.\n",
        "\n",
        "4. **Complex Workflows**:\n",
        "   - Combine multiple roles into one structured interaction for tasks requiring context and continuity (e.g., customer support, tutoring, brainstorming).\n",
        "\n",
        "5. **Scalability**:\n",
        "   - Templates make it easy to maintain and extend applications as complexity grows.\n",
        "\n",
        "---\n",
        "\n",
        "### **Comparison: Without vs. With LangChain Templates**\n",
        "\n",
        "#### **Without Templates**\n",
        "```python\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(openai_api_key=\"your_api_key\")\n",
        "response = llm(\"Explain the concept of gravity.\")\n",
        "print(response)\n",
        "```\n",
        "- **Limitations**:\n",
        "  - Hardcoded behavior.\n",
        "  - No clear structure for roles (system, user, assistant).\n",
        "  - No reusability or flexibility.\n",
        "\n",
        "#### **With Templates**\n",
        "```python\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"You are a physics tutor who explains concepts clearly.\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"Can you explain {topic}?\")\n",
        "])\n",
        "\n",
        "formatted_prompt = chat_prompt.format_prompt(topic=\"gravity\").to_messages()\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "chat = ChatOpenAI(openai_api_key=\"your_api_key\")\n",
        "response = chat(formatted_prompt)\n",
        "print(response.content)\n",
        "```\n",
        "- **Advantages**:\n",
        "  - Clear separation of roles.\n",
        "  - Reusable and dynamic structure.\n",
        "  - Scalable for multi-turn conversations or complex workflows.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaways**\n",
        "1. **System, User, and Assistant Roles**:\n",
        "   - Define the behavior, input, and response structure for the LLM.\n",
        "2. **LangChain Templates**:\n",
        "   - Leverage these roles to create structured, reusable, and programmable interactions.\n",
        "3. **Importance**:\n",
        "   - Templates add flexibility, clarity, and scalability to your applications, making them suitable for real-world use cases.\n",
        "\n",
        "Would you like to try building a complete example using these roles with LangChain templates?"
      ],
      "metadata": {
        "id": "Y-qR8LuoB7OM"
      },
      "id": "Y-qR8LuoB7OM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "-55MiGi0clP_"
      },
      "id": "-55MiGi0clP_"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "# !pip install openai\n",
        "# !pip install python-dotenv"
      ],
      "metadata": {
        "id": "2xy1mJj_AVPk"
      },
      "id": "2xy1mJj_AVPk",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "RBm9CWQJcqwL"
      },
      "id": "RBm9CWQJcqwL"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "import json\n",
        "import langchain\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# Set the environment variable globally for libraries like LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# Print the API key to confirm it's loaded correctly\n",
        "print(\"API Key loaded from .env:\",os.environ[\"OPENAI_API_KEY\"][0:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sInrAOplDA0m",
        "outputId": "42d49b3d-bd34-4d9d-85ca-5bec03713db2"
      },
      "id": "sInrAOplDA0m",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded from .env: sk-proj-e1GUWruINPRnrozmiakkRM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing various LLM integrations from LangChain\n",
        "\n",
        "from langchain.llms import OpenAI            # For OpenAI models (e.g., GPT-3, GPT-4)\n",
        "from langchain.chat_models import ChatOpenAI # For OpenAI chat models (e.g., GPT-3.5 Turbo, GPT-4)\n",
        "from langchain.llms import HuggingFaceHub    # For models hosted on Hugging Face Hub\n",
        "from langchain.llms import Cohere            # For Cohere's LLMs (e.g., command-xlarge)\n",
        "from langchain.llms import GPT4All           # For running local GPT4All models\n",
        "from langchain.llms import Custom            # For integrating custom model endpoints\n"
      ],
      "metadata": {
        "id": "orloGKZKVRM5"
      },
      "id": "orloGKZKVRM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Prompt"
      ],
      "metadata": {
        "id": "10eAVNhjH0wA"
      },
      "id": "10eAVNhjH0wA"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bdb72664-ce8d-4d88-b003-8a930ff2c2ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdb72664-ce8d-4d88-b003-8a930ff2c2ab",
        "outputId": "8b1a7675-d36b-4299-dd7d-c110f86ce1bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fun Fact about Pluto:\n",
            "Pluto was named by an 11-year-old girl named Venetia Burney in 1930. She suggested the name to her grandfather after learning about the discovery of the new planet.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Initialize the OpenAI LLM\n",
        "llm = OpenAI(openai_api_key=api_key)\n",
        "\n",
        "# Get the response\n",
        "response = llm('Here is a fun fact about Pluto:')\n",
        "\n",
        "# Print the response in a clean format\n",
        "print(\"Fun Fact about Pluto:\")\n",
        "print(response.strip())  # Use `.strip()` to remove any extra whitespace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f579bfe0-103e-430e-bac6-fa9fa50055b5",
      "metadata": {
        "id": "f579bfe0-103e-430e-bac6-fa9fa50055b5"
      },
      "source": [
        "### Multiple Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "24db968b-ef5a-4a31-bbb9-a08e48cb0a34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24db968b-ef5a-4a31-bbb9-a08e48cb0a34",
        "outputId": "72845217-9472-4b67-a93c-897d55d587dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response to Prompt 1: Pluto was first discovered in 1930 by American astronomer Clyde Tombaugh. However, the name \"Pluto\" was not given to the planet until several months later. At first, it was suggested that the planet be named \"Minerva\" after the Roman goddess of wisdom, but this name was ultimately rejected. The name \"Pluto\" was chosen by an 11-year-old girl named Venetia Burney, who thought it would be fitting since the planet is so far from the sun and the Roman god Pluto was the ruler of the underworld.\n",
            "Response to Prompt 2: Mars has the largest volcano in the solar system, called Olympus Mons. It is about 22 kilometers tall, making it almost three times taller than Mount Everest.\n"
          ]
        }
      ],
      "source": [
        "# needs to be a list\n",
        "result = llm.generate(\n",
        "    ['Here is a fun fact about Pluto:',\n",
        "     'Here is a fun fact about Mars:']\n",
        "    )\n",
        "\n",
        "# Extract and print the generated text for each prompt\n",
        "for i, generation in enumerate(result.generations):\n",
        "    print(f\"Response to Prompt {i+1}: {generation[0].text.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "b23e6c06-1543-42a5-94e7-dbb6bf8f4c8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b23e6c06-1543-42a5-94e7-dbb6bf8f4c8e",
        "outputId": "2462f02f-742b-4e02-cbe5-a4bf81636894"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[Generation(text='\\n\\nPluto was first discovered in 1930 by American astronomer Clyde Tombaugh. However, the name \"Pluto\" was not given to the planet until several months later. At first, it was suggested that the planet be named \"Minerva\" after the Roman goddess of wisdom, but this name was ultimately rejected. The name \"Pluto\" was chosen by an 11-year-old girl named Venetia Burney, who thought it would be fitting since the planet is so far from the sun and the Roman god Pluto was the ruler of the underworld. ', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\nMars has the largest volcano in the solar system, called Olympus Mons. It is about 22 kilometers tall, making it almost three times taller than Mount Everest. ', generation_info={'finish_reason': 'stop', 'logprobs': None})]]\n"
          ]
        }
      ],
      "source": [
        "print(result.generations)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain Templates\n",
        "\n",
        "LangChain **templates** are tools used to standardize and structure prompts for large language models (LLMs). They help ensure consistency, clarity, and reusability when creating inputs for the model. Let’s break this down and explore their utility.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Are LangChain Templates?**\n",
        "A **prompt template** in LangChain defines the structure of a prompt that will be sent to the LLM. Templates can:\n",
        "1. **Include placeholders** for dynamic content (e.g., user inputs).\n",
        "2. **Standardize prompts** to ensure consistent communication with the model.\n",
        "3. **Enable reusability** by separating the structure from the dynamic content.\n",
        "\n",
        "---\n",
        "\n",
        "### **Components of a Prompt Template**\n",
        "1. **`input_variables`**\n",
        "   - These are placeholders for dynamic inputs that will be filled at runtime.\n",
        "   - Example: If your template is `\"Tell me a fact about {topic}\"`, then `\"topic\"` is an `input_variable`.\n",
        "\n",
        "2. **`template`**\n",
        "   - This is the base string or structure of the prompt. It may include:\n",
        "     - Static text (e.g., `\"Tell me a fact about\"`)\n",
        "     - Placeholders for dynamic variables (e.g., `\"{topic}\"`).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Are Templates Useful?**\n",
        "1. **Reusability**\n",
        "   - Templates allow you to define the structure once and reuse it with different dynamic inputs.\n",
        "   - Example:\n",
        "     ```python\n",
        "     template = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}\")\n",
        "     print(template.format(topic=\"Pluto\"))\n",
        "     # Output: \"Tell me a fact about Pluto.\"\n",
        "     ```\n",
        "\n",
        "2. **Consistency**\n",
        "   - Standardized prompts ensure consistent communication with the model, leading to predictable behavior.\n",
        "\n",
        "3. **Dynamic Inputs**\n",
        "   - You can use the same template with various inputs, making it adaptable to different contexts.\n",
        "\n",
        "4. **Separation of Logic**\n",
        "   - Templates separate the prompt's structure from the actual input, making your code easier to maintain and extend.\n",
        "\n"
      ],
      "metadata": {
        "id": "rW13H2KDNCLy"
      },
      "id": "rW13H2KDNCLy"
    },
    {
      "cell_type": "markdown",
      "id": "38d37ccf-99d2-4aa6-ab73-0bd22e55b7b6",
      "metadata": {
        "tags": [],
        "id": "38d37ccf-99d2-4aa6-ab73-0bd22e55b7b6"
      },
      "source": [
        "\n",
        "\n",
        "### No Input Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "1e34d990-cc7c-47f6-a191-854a575aabca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1e34d990-cc7c-47f6-a191-854a575aabca",
        "outputId": "d7a4ba48-738c-442f-a370-9884f240d79b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a fact'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# An example prompt with no input variables\n",
        "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a fact\")\n",
        "no_input_prompt.format()\n",
        "# -> \"Tell me a fact.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be521325-8963-4505-8bf3-83b05e8a796a",
      "metadata": {
        "id": "be521325-8963-4505-8bf3-83b05e8a796a"
      },
      "source": [
        "### Single Input Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "361467f3-81d3-4553-9a22-1198f44598c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "361467f3-81d3-4553-9a22-1198f44598c0",
        "outputId": "8fda83a0-1c8d-45ab-a9cb-725dbbe57da7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a fact about Mars.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# An example prompt with one input variable\n",
        "one_input_prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}.\")\n",
        "# Notice how the stirng \"topic\" gets automatically converted to a parameter name, very convienent!\n",
        "one_input_prompt.format(topic=\"Mars\")\n",
        "# -> \"Tell me a fact about Mars\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e8c513-b8d0-4616-9015-e0133cfc45d4",
      "metadata": {
        "id": "e3e8c513-b8d0-4616-9015-e0133cfc45d4"
      },
      "source": [
        "### Multiple Input Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "aba25263-4b12-40d8-8c56-e21b9bc37891",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aba25263-4b12-40d8-8c56-e21b9bc37891",
        "outputId": "fe2bb005-8046-4b8a-b0b9-cbbe09ca32d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a fact about Mars for a student 8th Grade level.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# An example prompt with multiple input variables\n",
        "multiple_input_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"level\"],\n",
        "    template=\"Tell me a fact about {topic} for a student {level} level.\"\n",
        ")\n",
        "multiple_input_prompt.format(topic='Mars',level='8th Grade')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System Template\n",
        "\n",
        "\n",
        "1. **Dynamic Templates**:\n",
        "   - The `system_template` includes placeholders (`{dietary_preference}` and `{cooking_time}`) that allow for flexible, dynamic prompts. This enables the assistant to adapt its behavior based on different contexts.\n",
        "\n",
        "2. **Role-Specific Prompts**:\n",
        "   - This defines a **system message**, which sets the overall behavior of the AI. System prompts are crucial for guiding the tone, domain, or expertise of the model.\n",
        "\n",
        "3. **Reusable Prompt Structures**:\n",
        "   - The use of `SystemMessagePromptTemplate.from_template` standardizes the creation of structured prompts, making it easier to reuse and maintain across different tasks.\n"
      ],
      "metadata": {
        "id": "mrdame5uOxLt"
      },
      "id": "mrdame5uOxLt"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "cbc8eeab-11ff-4c46-b69b-e963066980d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbc8eeab-11ff-4c46-b69b-e963066980d0",
        "outputId": "f0cfe385-b83a-4efc-fc52-6bcae86ac35a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cooking_time', 'dietary_preference']\n"
          ]
        }
      ],
      "source": [
        "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "print(system_message_prompt.input_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human Template\n",
        "\n",
        "1. **Dynamic User Input:**\n",
        "   - The `human_template` includes a placeholder (`{recipe_request}`), making the prompt adaptable to user-specific inputs. This allows the model to handle dynamic queries.\n",
        "\n",
        "2. **Role-Specific Messaging:**\n",
        "   - This defines a **human message**, which represents the user's input in a conversation. This contrasts with the **system message**, which guides the AI's overall behavior.\n",
        "\n",
        "3. **Template Consistency:**\n",
        "   - Just like the system message in the previous example, this human message template standardizes how user inputs are structured, ensuring clear communication with the model.\n",
        "\n",
        "4. **Input Variables Extraction:**\n",
        "   - The `.input_variables` property again identifies the required dynamic inputs (`recipe_request`) for the template, ensuring the correct variables are passed at runtime.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Relates to the Previous Code:**\n",
        "1. **Complementary Roles:**\n",
        "   - The **system message** sets the model's behavior (e.g., as a recipe assistant), while the **human message** represents the user query or request (e.g., a specific recipe).\n",
        "\n",
        "2. **Unified Framework:**\n",
        "   - Both templates use the same underlying approach (`from_template`), ensuring consistency in how prompts are defined and used.\n",
        "\n",
        "3. **Conversation Flow:**\n",
        "   - Together, they form the basis of a multi-turn interaction where the system defines behavior and the user drives the conversation with their inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "5tTyE3E_PJ9N"
      },
      "id": "5tTyE3E_PJ9N"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "2348e5e3-e878-403a-94e9-be61359fbb44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2348e5e3-e878-403a-94e9-be61359fbb44",
        "outputId": "276d15aa-ef11-42a9-f6d3-a311bbb8ada3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['recipe_request']\n"
          ]
        }
      ],
      "source": [
        "human_template=\"{recipe_request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "print(human_message_prompt.input_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "1. **Combining Roles in a Conversation:**\n",
        "   - The `ChatPromptTemplate.from_messages` combines the **system message** (for setting behavior) and the **human message** (for user input) into a single cohesive structure.\n",
        "   - This template represents a full conversation flow, ensuring all role-specific prompts work together seamlessly.\n",
        "\n",
        "2. **Unified Input Handling:**\n",
        "   - The `chat_prompt.input_variables` collects all placeholders (`{dietary_preference}`, `{cooking_time}`, `{recipe_request}`) from the system and human message templates, ensuring the model has all the necessary inputs for the interaction.\n",
        "\n",
        "3. **Reusability and Modularity:**\n",
        "   - By combining smaller, reusable templates, this approach allows you to build complex conversation flows without duplicating code or logic.\n",
        "\n",
        "4. **Dynamic Flexibility:**\n",
        "   - The resulting `ChatPromptTemplate` can dynamically adapt to various inputs and scenarios, making it ideal for tasks requiring multi-turn conversations.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Relates to the Previous Code**\n",
        "1. **Building Blocks:**\n",
        "   - The **system message** defines the assistant’s behavior.\n",
        "   - The **human message** provides the user’s query or input.\n",
        "   - The `ChatPromptTemplate` combines these building blocks to create a complete conversation template.\n",
        "\n",
        "2. **Scalability:**\n",
        "   - This approach enables the addition of more roles (e.g., `assistant_message`) or templates as the conversation becomes more complex.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway:**\n",
        "The `ChatPromptTemplate` combines role-specific templates into a structured conversation framework, ensuring that the system and user inputs are handled cohesively and dynamically. It demonstrates how to scale prompts for more sophisticated, multi-role interactions."
      ],
      "metadata": {
        "id": "uuax3Uu3PeG1"
      },
      "id": "uuax3Uu3PeG1"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "706756df-1264-4121-8043-b733e60188c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "706756df-1264-4121-8043-b733e60188c0",
        "outputId": "da4c9a32-3ca1-4a37-fffb-7f870098bac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cooking_time', 'dietary_preference', 'recipe_request']\n"
          ]
        }
      ],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt])\n",
        "print(chat_prompt.input_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chat Prompt\n",
        "\n",
        "1. **Dynamic Prompt Filling:**\n",
        "   - The `chat_prompt.format_prompt(...)` method dynamically replaces placeholders (`{cooking_time}`, `{dietary_preference}`, `{recipe_request}`) in the system and human message templates with the specified values.\n",
        "   - Example: `\"You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.\"`\n",
        "\n",
        "2. **Creating Structured Messages:**\n",
        "   - The `to_messages()` method converts the formatted prompt into a list of structured messages, ready to be sent to a chat-based LLM (e.g., OpenAI’s `gpt-3.5-turbo`).\n",
        "   - These messages include **roles** like `system` and `user`, ensuring proper context for the conversation.\n",
        "\n",
        "3. **Final Step Before Model Interaction:**\n",
        "   - This is typically the last step before passing the prompt to the model. It ensures that all variables are filled and the message format matches what the LLM expects.\n",
        "\n",
        "4. **Real-World Example of Templates in Action:**\n",
        "   - This demonstrates how templates enable seamless and dynamic conversations, where inputs (e.g., cooking time, preference, request) are integrated into a predefined structure.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Relates to the Previous Code**\n",
        "1. **Executing the Conversation Template:**\n",
        "   - The `chat_prompt` combines the **system** and **human** messages, and `format_prompt` customizes them with the provided values.\n",
        "\n",
        "2. **End-to-End Workflow:**\n",
        "   - While the earlier code defined reusable templates, this code:\n",
        "     - Dynamically fills the placeholders.\n",
        "     - Prepares the final input for the chat model.\n",
        "\n",
        "3. **Seamless Interaction:**\n",
        "   - By combining all roles and filling placeholders dynamically, this code ensures the LLM has the full context for generating an appropriate response.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway:**\n",
        "This code demonstrates how to take a dynamic, reusable conversation template and fill it with specific input values, creating a structured set of messages ready for interaction with a chat-based LLM. It's the bridge between designing prompts and actually using them to engage with the model."
      ],
      "metadata": {
        "id": "rpAFJulXQB9n"
      },
      "id": "rpAFJulXQB9n"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "3b19ae01-3be8-41c9-a470-3fd6fc69e801",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b19ae01-3be8-41c9-a470-3fd6fc69e801",
        "outputId": "4683b5d6-c713-4e05-88db-e3183d6e4a9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Quick Snack', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# get a chat completion from the formatted messages\n",
        "chat_prompt.format_prompt(\n",
        "    cooking_time=\"15 min\",\n",
        "    dietary_preference=\"Vegan\",\n",
        "    recipe_request=\"Quick Snack\").to_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Message Objects\n",
        "The returned `request` is a **list** of structured message objects. Each item in the list corresponds to a message in the conversation, such as a `SystemMessage` or `HumanMessage`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Characteristics of the `request` List**\n",
        "1. **List Structure**\n",
        "   - The `request` is a standard Python list.\n",
        "   - Each element is a message object, such as `SystemMessage`, `HumanMessage`, or (in other contexts) `AssistantMessage`.\n",
        "\n",
        "2. **Message Objects**\n",
        "   - Each message object contains attributes like:\n",
        "     - `content`: The main text of the message.\n",
        "     - `additional_kwargs`: Any additional parameters associated with the message.\n",
        "     - `response_metadata`: Metadata about the message (if provided).\n",
        "\n",
        "3. **Order Matters**\n",
        "   - The order of the messages in the list matches the order they are defined in the `ChatPromptTemplate`. For example:\n",
        "     - The **system message** (defining the AI's role) comes first.\n",
        "     - The **human message** (user's input) follows.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example of the `request` List**\n",
        "Here’s the structure of the returned list for your example:\n",
        "```python\n",
        "[\n",
        "    SystemMessage(\n",
        "        content=\"You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.\",\n",
        "        additional_kwargs={},\n",
        "        response_metadata={}\n",
        "    ),\n",
        "    HumanMessage(\n",
        "        content=\"Quick Snack\",\n",
        "        additional_kwargs={},\n",
        "        response_metadata={}\n",
        "    )\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Work with the List**\n",
        "1. **Access Individual Messages**\n",
        "   - Use list indexing to access specific messages:\n",
        "     ```python\n",
        "     system_message = request[0]  # First message\n",
        "     human_message = request[1]  # Second message\n",
        "     ```\n",
        "\n",
        "2. **Extract Attributes**\n",
        "   - Access attributes like `content` for the text:\n",
        "     ```python\n",
        "     print(system_message.content)\n",
        "     print(human_message.content)\n",
        "     ```\n",
        "\n",
        "3. **Iterate Over Messages**\n",
        "   - Loop through the list to process all messages:\n",
        "     ```python\n",
        "     for message in request:\n",
        "         print(message.content)  # Print the text of each message\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Takeaway**\n",
        "The `request` is a **list** of structured message objects, making it easy to access, manipulate, or send individual messages as needed. Each message includes details about its role (e.g., system or user) and content. This structure ensures a clear and organized conversation flow, ready for interaction with chat-based LLMs."
      ],
      "metadata": {
        "id": "8YbF7QtyRIpS"
      },
      "id": "8YbF7QtyRIpS"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "73121537-8cee-42fc-ba0e-fc1c18154957",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73121537-8cee-42fc-ba0e-fc1c18154957",
        "outputId": "0a660249-450d-4e9d-f0ea-cd5e2c25f5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message 1:\n",
            "Role: SystemMessage\n",
            "Content: You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.\n",
            "Additional Kwargs: {}\n",
            "Response Metadata: {}\n",
            "\n",
            "Message 2:\n",
            "Role: HumanMessage\n",
            "Content: Quick Snack\n",
            "Additional Kwargs: {}\n",
            "Response Metadata: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "request = chat_prompt.format_prompt(\n",
        "    cooking_time=\"15 min\",\n",
        "    dietary_preference=\"Vegan\",\n",
        "    recipe_request=\"Quick Snack\").to_messages()\n",
        "\n",
        "# Loop through each message in the request\n",
        "for i, message in enumerate(request):\n",
        "    print(f\"Message {i+1}:\")\n",
        "    print(f\"Role: {type(message).__name__}\")  # Message type (e.g., SystemMessage, HumanMessage)\n",
        "    print(f\"Content: {message.content}\")     # Main content of the message\n",
        "    print(f\"Additional Kwargs: {message.additional_kwargs}\")  # Additional data\n",
        "    print(f\"Response Metadata: {message.response_metadata}\")  # Metadata\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract just the content\n",
        "contents = [message.content for message in request]\n",
        "\n",
        "# Print the extracted content\n",
        "for i, content in enumerate(contents):\n",
        "    print(f\"Message {i+1} Content: {content}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMCvETlLQsmM",
        "outputId": "8d8d3be2-0571-4381-f8e6-221f21de7905"
      },
      "id": "GMCvETlLQsmM",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message 1 Content: You are an AI recipe assistant that specializes in Vegan dishes that can be prepared in 15 min.\n",
            "Message 2 Content: Quick Snack\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c276622f-d81c-40ed-864b-63d4c4311d84",
      "metadata": {
        "id": "c276622f-d81c-40ed-864b-63d4c4311d84"
      },
      "source": [
        "### LLM Template Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "c1496d66-efec-4cd5-a62e-d33f9669941c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1496d66-efec-4cd5-a62e-d33f9669941c",
        "outputId": "cb38c7ff-d6c1-4f96-d3cb-6cfc8b323247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One quick and easy vegan snack you can make in 15 minutes is avocado toast. Here's a simple recipe for you:\n",
            "\n",
            "Ingredients:\n",
            "- 1 ripe avocado\n",
            "- 2 slices of whole grain bread\n",
            "- Salt and pepper to taste\n",
            "- Optional toppings: cherry tomatoes, red pepper flakes, sesame seeds, or a squeeze of lemon juice\n",
            "\n",
            "Instructions:\n",
            "1. Toast the slices of bread in a toaster until golden brown.\n",
            "2. While the bread is toasting, mash the ripe avocado in a bowl using a fork until smooth.\n",
            "3. Once the bread is toasted, spread the mashed avocado evenly on top of each slice.\n",
            "4. Season with salt and pepper to taste.\n",
            "5. Add your favorite optional toppings like cherry tomatoes, red pepper flakes, sesame seeds, or a squeeze of lemon juice.\n",
            "6. Enjoy your quick and delicious avocado toast snack!\n",
            "\n",
            "Response Metadata:\n",
            "\n",
            "{'completion_tokens': 173, 'prompt_tokens': 34, 'total_tokens': 207, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}\n"
          ]
        }
      ],
      "source": [
        "chat = ChatOpenAI(openai_api_key=api_key)\n",
        "result = chat(request)\n",
        "\n",
        "# Print the message object\n",
        "print(result.content)\n",
        "\n",
        "# Access specific attributes\n",
        "print(\"\\nResponse Metadata:\\n\")\n",
        "print(result.response_metadata['token_usage'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "88f47c1b-123c-4f47-8ee1-b72c88cbd048",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88f47c1b-123c-4f47-8ee1-b72c88cbd048",
        "outputId": "2539f27f-30b3-4c99-8b9d-11972e1c7fb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"How about making a simple and delicious avocado toast? Here's a quick recipe for you:\\n\\nIngredients:\\n- 1 ripe avocado\\n- 2 slices of bread (use your favorite bread, like whole grain or sourdough)\\n- Salt and pepper to taste\\n- Optional toppings: sliced tomatoes, red pepper flakes, nutritional yeast, or a squeeze of lemon juice\\n\\nInstructions:\\n1. Toast the bread slices in a toaster or on a skillet until golden brown.\\n2. Meanwhile, scoop out the flesh of the avocado into a small bowl and mash it with a fork until creamy.\\n3. Spread the mashed avocado evenly on the toasted bread slices.\\n4. Season with salt and pepper to taste.\\n5. Add your favorite toppings, such as sliced tomatoes, red pepper flakes, nutritional yeast, or a squeeze of lemon juice.\\n6. Enjoy your quick and tasty avocado toast snack!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 177, 'prompt_tokens': 34, 'total_tokens': 211, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-36e50b1a-f4de-4897-aa23-dce82979e6c9-0')"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Full Code"
      ],
      "metadata": {
        "id": "8fn85FS1g1m7"
      },
      "id": "8fn85FS1g1m7"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "a72ee656-ec4b-4c5b-b839-1b3c2f269f07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a72ee656-ec4b-4c5b-b839-1b3c2f269f07",
        "outputId": "56f214b6-894e-4c70-8b6a-33be430cc4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Recipe Suggestion:\n",
            "How about making **Avocado Toast with Cherry Tomatoes**? It’s quick, delicious, and packed with nutrients!\n",
            "\n",
            "### Ingredients:\n",
            "- 1 ripe avocado\n",
            "- 2 slices of whole grain or gluten-free bread\n",
            "- A handful of cherry tomatoes, halved\n",
            "- Salt and pepper to taste\n",
            "- Olive oil (optional)\n",
            "- Red pepper flakes or lemon juice (optional for extra flavor)\n",
            "- Fresh herbs (like basil or cilantro, optional)\n",
            "\n",
            "### Instructions:\n",
            "1. **Toast the Bread**: Start by toasting the slices of bread in a toaster or on a skillet until golden brown.\n",
            "  \n",
            "2. **Mash the Avocado**: While the bread is toasting, scoop the avocado into a bowl and mash it with a fork. Season with salt and pepper to taste. You can also add a splash of lemon juice for flavor.\n",
            "\n",
            "3. **Assemble**: Once the bread is toasted, spread the mashed avocado evenly on each slice.\n",
            "\n",
            "4. **Top with Tomatoes**: Place the halved cherry tomatoes on top of the avocado toast. Drizzle with a little olive oil if desired, and sprinkle with red pepper flakes or fresh herbs for an extra kick.\n",
            "\n",
            "5. **Serve and Enjoy**: Slice the toast in half if you prefer and enjoy your quick, nutritious snack!\n",
            "\n",
            "This snack takes just about 10 minutes, and it’s both satisfying and healthy!\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Step 1: Define system and human message templates\n",
        "system_template = (\n",
        "    \"You are an AI recipe assistant that specializes in {dietary_preference} dishes \"\n",
        "    \"that can be prepared in {cooking_time}.\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_template = \"{recipe_request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# Step 2: Combine templates into a chat prompt\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# Step 3: Format the prompt with user inputs\n",
        "formatted_prompt = chat_prompt.format_prompt(\n",
        "    dietary_preference=\"Vegan\",\n",
        "    cooking_time=\"15 min\",\n",
        "    recipe_request=\"I need a quick snack idea.\"\n",
        ").to_messages()\n",
        "\n",
        "# Step 4: Initialize the chat model\n",
        "chat = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Step 5: Get a response from the model\n",
        "response = chat(formatted_prompt)\n",
        "\n",
        "# Step 6: Print the response content\n",
        "print(\"AI Recipe Suggestion:\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3aePfiLUBnV"
      },
      "id": "e3aePfiLUBnV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}