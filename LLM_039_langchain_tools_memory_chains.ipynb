{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_039_langchain_tools_memory_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain Core Concepts\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Chains**\n",
        "A **chain** in LangChain is a sequence of operations where the output of one step becomes the input for the next. These operations can include:\n",
        "\n",
        "- Calling a language model (like OpenAI's GPT-4).\n",
        "- Using the outputs to query a database, fetch an API, or trigger another tool.\n",
        "- Returning a final response or taking further action.\n",
        "\n",
        "#### Key Types of Chains:\n",
        "1. **LLMChain**: The simplest chain, consisting of an input, a language model call, and an output.\n",
        "2. **SequentialChain**: A series of chains executed in sequence, passing outputs to subsequent steps.\n",
        "3. **RouterChain**: Dynamically selects the next step based on the input.\n",
        "4. **Custom Chains**: You can build your own chains to fit complex workflows.\n",
        "\n",
        "Example: An LLMChain that takes a user query, reformulates it, and returns an answer.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Tools**\n",
        "**Tools** are external functionalities that the language model can interact with. LangChain allows you to enhance your application by integrating external APIs, databases, or utilities.\n",
        "\n",
        "Examples of tools:\n",
        "- Search engines (e.g., Google, Bing).\n",
        "- APIs (e.g., weather, stock prices).\n",
        "- Python REPL (for calculations).\n",
        "- SQL connectors (to query databases).\n",
        "- Custom APIs or functions you define.\n",
        "\n",
        "**How tools work in LangChain:**\n",
        "- The language model is instructed to \"ask for help\" when it needs external information.\n",
        "- Tools are invoked, and their outputs are used by the model to continue processing.\n",
        "\n",
        "Example: A chatbot equipped with a calculator tool can compute \"What is 123 + 456?\" and provide the correct result.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Memory**\n",
        "**Memory** enables LangChain to maintain context across interactions. This is crucial for applications where continuity is important, like chatbots or personal assistants.\n",
        "\n",
        "#### Types of Memory:\n",
        "1. **Short-term Memory**: Maintains context for the current session.\n",
        "   - Example: Remembering the user’s name during a conversation.\n",
        "2. **Long-term Memory**: Stores information across sessions.\n",
        "   - Example: Saving user preferences for future interactions.\n",
        "3. **ConversationBufferMemory**: Stores all past interactions in the buffer.\n",
        "4. **ConversationSummaryMemory**: Summarizes interactions to save space.\n",
        "5. **Custom Memory**: You can implement your own memory logic for specific use cases.\n",
        "\n",
        "---\n",
        "\n",
        "### How They Work Together\n",
        "Imagine building a weather assistant:\n",
        "1. **Chains**: A chain that takes a query like \"Will it rain tomorrow?\" and processes it into a weather API request.\n",
        "2. **Tools**: The API tool fetches weather data.\n",
        "3. **Memory**: The assistant remembers that the user likes temperature updates in Celsius.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-qR8LuoB7OM"
      },
      "id": "Y-qR8LuoB7OM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "-55MiGi0clP_"
      },
      "id": "-55MiGi0clP_"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain\n",
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain-openai"
      ],
      "metadata": {
        "id": "2xy1mJj_AVPk"
      },
      "id": "2xy1mJj_AVPk",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "RBm9CWQJcqwL"
      },
      "id": "RBm9CWQJcqwL"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import openai\n",
        "import json\n",
        "import langchain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# Set the environment variable globally for libraries like LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# Print the API key to confirm it's loaded correctly\n",
        "print(\"API Key loaded from .env:\",os.environ[\"OPENAI_API_KEY\"][0:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sInrAOplDA0m",
        "outputId": "bac5f5be-6649-453a-8cfe-a86ba034eabc"
      },
      "id": "sInrAOplDA0m",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded from .env: sk-proj-e1GUWruINPRnrozmiakkRM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Simple Chain**:\n",
        "We'll create a simple chain where a user inputs a topic, and the system generates a short, creative description of that topic. This is a basic example to demonstrate how to use **chains** with a **prompt template**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### **What’s Happening Here?**\n",
        "1. **LLM Initialization**:\n",
        "   - The `ChatOpenAI` object sets up a connection to the OpenAI GPT model.\n",
        "   - `temperature=0.7` controls creativity (higher = more creative).\n",
        "\n",
        "2. **Prompt Template**:\n",
        "   - The `PromptTemplate` allows you to define a reusable text structure.\n",
        "   - `{topic}` is a placeholder that gets replaced with user input.\n",
        "\n",
        "3. **Chain**:\n",
        "   - `LLMChain` links the LLM with the prompt template.\n",
        "   - When you call `.run()`, the chain processes the input and generates output.\n",
        "\n"
      ],
      "metadata": {
        "id": "RzQC_cyJe1yr"
      },
      "id": "RzQC_cyJe1yr"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1: Define the LLM (OpenAI GPT model in this case)\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)  # Use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
        "\n",
        "# Step 2: Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],  # Input placeholder\n",
        "    template=\"Write a creative and engaging description about {topic}.\"\n",
        ")\n",
        "\n",
        "# Step 3: Define the chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Step 4: Use the chain with an input\n",
        "topic = \"artificial intelligence\"\n",
        "response = chain.run(topic=topic)\n",
        "\n",
        "# Print the result\n",
        "print(\"Input Topic:\", topic)\n",
        "print(\"Generated Description:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc6G3NoXzfg6",
        "outputId": "f4e7675a-ed1a-40dc-86bc-6d727fb1031d"
      },
      "id": "nc6G3NoXzfg6",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Topic: artificial intelligence\n",
            "Generated Description: Artificial Intelligence (AI) is a fascinating tapestry woven from the threads of human ingenuity and technological innovation. Imagine a realm where machines possess the ability to learn, adapt, and think, not unlike the way we do. AI is the digital brain that empowers computers to interpret data, recognize patterns, and make decisions, transforming the mundane into the extraordinary.\n",
            "\n",
            "Picture a world where virtual assistants anticipate your needs, where algorithms curate personalized experiences just for you, and where autonomous vehicles navigate the roads with precision. AI is not just a tool; it is a collaborator, a partner in creativity, and a catalyst for change. It breathes life into industries, from healthcare, where it analyzes complex medical data to identify treatments, to entertainment, where it crafts immersive experiences that captivate our imaginations.\n",
            "\n",
            "Yet, the story of AI is not merely one of efficiency and automation. It is a reflection of our aspirations and ethical dilemmas, a mirror held up to society as we explore questions of consciousness, responsibility, and the essence of what it means to be human. With every breakthrough, AI challenges us to redefine our relationship with technology and to envision a future where the boundaries between human and machine blur.\n",
            "\n",
            "As we journey deeper into the age of AI, we find ourselves at the intersection of possibility and responsibility, innovation and ethics. The narrative is still being written, and as architects of this new frontier, we hold the pen. Will we create a world where AI enhances our lives, or will we let it drift into the realm of the unknown? The future is ours to shape, and with AI as our ally, the possibilities are limitless.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down the **importance** and **value** of LangChain in the broader context of leveraging Language Learning Models (LLMs).\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Modular Design for Flexibility**\n",
        "LangChain abstracts complexities and provides a modular structure for:\n",
        "- **Swapping LLMs**: Easily switch between models like OpenAI’s GPT-4, Anthropic’s Claude, Hugging Face Transformers, or custom models.\n",
        "- **Experimentation**: Test different LLMs to find the best performance or cost-effectiveness for your application.\n",
        "\n",
        "#### Why This Matters:\n",
        "Without LangChain, you’d need to write separate boilerplate code for each model. LangChain simplifies this by letting you change the backend model with minimal code adjustments.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Scalability**\n",
        "LangChain is designed for **scalable** and **complex applications**:\n",
        "- **Combining Models and Tools**: Chains allow you to seamlessly integrate multiple steps, like calling APIs, querying databases, or applying business logic.\n",
        "- **Reusable Components**: Prompt templates, chains, and tools can be reused and customized, reducing development time.\n",
        "\n",
        "#### Why This Matters:\n",
        "If you’re building an application that needs to:\n",
        "- Call external APIs,\n",
        "- Remember user context (via memory),\n",
        "- Use structured workflows (chains),\n",
        "LangChain organizes and scales these tasks efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Memory and Context Management**\n",
        "Out of the box, LLMs are **stateless**, meaning they don’t retain knowledge of previous interactions. LangChain solves this:\n",
        "- **Short-term Memory**: Manage conversation context within a single session.\n",
        "- **Long-term Memory**: Retain user preferences or interaction history across sessions.\n",
        "\n",
        "#### Why This Matters:\n",
        "Building personalized or interactive systems (e.g., customer support bots, virtual assistants) requires context-aware interactions. LangChain enables this functionality.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Multi-Tool Orchestration**\n",
        "LangChain supports **tool integration**, allowing the model to interact with external systems:\n",
        "- **APIs**: Fetch live data (e.g., weather, stock prices).\n",
        "- **Databases**: Query information from SQL or vector databases (for search and recommendations).\n",
        "- **Calculators**: Handle complex computations the model can't perform natively.\n",
        "\n",
        "#### Why This Matters:\n",
        "Combining LLMs with tools dramatically enhances their utility. For instance:\n",
        "- A model alone might not answer “What’s the current price of Bitcoin?”\n",
        "- With LangChain and a tool like a finance API, it can retrieve real-time data.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Prompt Engineering and Experimentation**\n",
        "LangChain makes **prompt engineering** easier by:\n",
        "- Separating prompts from logic (using templates).\n",
        "- Supporting dynamic prompts (filling placeholders with user input).\n",
        "\n",
        "#### Why This Matters:\n",
        "Well-crafted prompts improve the accuracy and relevance of LLM outputs. LangChain lets you experiment and iterate efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Building Advanced Applications**\n",
        "LangChain enables sophisticated applications that would otherwise be tedious to implement manually:\n",
        "- **Chatbots**: Add memory, tools, and stateful conversation flows.\n",
        "- **Search Engines**: Combine LLMs with vector-based retrieval for intelligent search.\n",
        "- **Agent Systems**: Build systems that take actions (e.g., send emails, schedule tasks).\n",
        "\n",
        "#### Why This Matters:\n",
        "LangChain reduces the technical barrier to building advanced AI-powered systems by providing a robust, pre-built foundation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is This Code Valuable?**\n",
        "This example introduces:\n",
        "1. **Reusability**: The prompt-template approach can adapt to different tasks (summarization, explanation, etc.) by simply changing the template text.\n",
        "2. **Interchangeability**: You can swap out the LLM (e.g., OpenAI, Hugging Face) without changing how the chain works.\n",
        "3. **Foundation**: It's the starting point to build much more complex systems, like:\n",
        "   - A personalized tutor,\n",
        "   - A multi-step workflow assistant,\n",
        "   - An AI tool that augments business workflows.\n",
        "\n",
        "---\n",
        "\n",
        "### **In Short**\n",
        "LangChain’s value lies in **streamlining LLM-powered application development**, making it flexible, scalable, and feature-rich. It saves time, supports complex workflows, and ensures your application can grow and adapt as needs evolve.\n"
      ],
      "metadata": {
        "id": "tcetzVZnihQ9"
      },
      "id": "tcetzVZnihQ9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. Adding Memory**\n",
        "Memory allows your application to maintain context across multiple interactions, enabling more dynamic and personalized conversations. LangChain offers several types of memory, including **ConversationBufferMemory** and **ConversationSummaryMemory**.\n",
        "\n",
        "#### **Code Example: Adding Memory**\n",
        "We'll add **short-term memory** to a chatbot that remembers the conversation.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **How This Works**\n",
        "- **ConversationBufferMemory**: Keeps a record of all exchanges (as messages) during the session.\n",
        "- **ConversationChain**: Combines the memory with an LLM to enable context-aware interactions.\n",
        "- **Dynamic Context**: The AI responds based on the current input and the conversation history.\n",
        "\n",
        "---\n",
        "\n",
        "### **LLMs Are Stateless**\n",
        "1. **Limited Context Window**:\n",
        "   - LLMs process each input as a new request.\n",
        "   - They only \"see\" the current input along with any previous conversation provided in the prompt.\n",
        "   - The amount of prior conversation they can \"remember\" is limited by the **context window size** (e.g., ~4,096 tokens for GPT-3.5, ~8,192 or more for GPT-4).\n",
        "\n",
        "2. **Ephemeral Memory**:\n",
        "   - If you don’t include past interactions in the prompt, the model won’t retain them.\n",
        "   - This means you, as the developer, need to provide all relevant history with every request to ensure continuity.\n",
        "\n",
        "---\n",
        "\n",
        "### **How LangChain Memory Is Different**\n",
        "LangChain **persists and manages conversation history** outside the LLM itself, making it:\n",
        "1. **Explicit**: The history is stored and managed programmatically, so you don't need to re-insert it manually with every prompt.\n",
        "2. **Customizable**:\n",
        "   - Store **entire conversations** (e.g., ConversationBufferMemory).\n",
        "   - **Summarize conversations** (e.g., ConversationSummaryMemory) to avoid exceeding token limits.\n",
        "   - Create **long-term memory** to persist key information across sessions.\n",
        "3. **Dynamic Management**:\n",
        "   - Automatically injects conversation history into the prompt, reducing developer overhead.\n",
        "   - Filters, summarizes, or restructures history to optimize usage of the LLM's context window.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Matters**\n",
        "- **Stateful Interaction**: LangChain ensures the AI remembers the user's name, preferences, or ongoing topics dynamically, rather than needing the developer to provide this history repeatedly.\n",
        "- **Token Efficiency**: By summarizing or managing memory, LangChain prevents history from overwhelming the LLM's context window.\n",
        "- **Persistence**: LangChain memory can be stored and reused across sessions, enabling long-term personalization, which stateless LLMs cannot do natively.\n",
        "\n",
        "---\n",
        "\n",
        "### **Real-World Example**\n",
        "Imagine a personal assistant:\n",
        "- **Without memory**: If a user says, \"Schedule a meeting for tomorrow,\" the assistant has no idea what you're referring to in subsequent interactions unless you explicitly repeat it.\n",
        "- **With memory**: The assistant remembers this request and provides updates like, \"Your meeting is scheduled for tomorrow at 10 AM.\"\n",
        "\n",
        "LangChain's memory makes the difference between a \"reactive\" bot and a **context-aware, dynamic assistant**.\n",
        "\n"
      ],
      "metadata": {
        "id": "--jmxF_ViuAj"
      },
      "id": "--jmxF_ViuAj"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1: Define the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# Step 2: Create a memory object\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "# Step 3: Create the conversational chain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "# Step 4: Interact with the chain\n",
        "print(\"Start chatting with your AI. Type 'exit' to stop.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = conversation.run(user_input)\n",
        "    print(\"AI:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0XhIe6wfwZj",
        "outputId": "4c4d146a-26ba-4cca-d2c7-6906db704af9"
      },
      "id": "y0XhIe6wfwZj",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with your AI. Type 'exit' to stop.\n",
            "AI: Hello! It's great to talk with you. How can I assist you today?\n",
            "AI: I'm sorry, but it seems like there's a typo or misunderstanding in your question. \"Lanchain\" doesn't seem to refer to any known concept or term in my database. Could you please provide more context or check the spelling? I'm here to help with subjects like technology, history, science, culture, and more.\n",
            "You: exit\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Let’s dive into **Tools**, a powerful feature in LangChain that enables an LLM to interact with external systems like search engines, APIs, or databases.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Are Tools in LangChain?**\n",
        "**Tools** are external functionalities or utilities that an LLM can call during its execution. These might include:\n",
        "- **Search APIs**: For real-time information.\n",
        "- **Calculators**: For mathematical operations.\n",
        "- **Databases**: For retrieving structured information.\n",
        "- **Custom APIs**: For domain-specific functionality.\n",
        "\n",
        "The LLM learns to \"ask for help\" from tools when it cannot answer a query directly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Code Example: Using a Search Tool**\n",
        "We’ll create an application where the LLM uses a **search tool** to fetch real-time information.\n",
        "\n",
        "#### **Setup: Install Required Libraries**\n",
        "Make sure you have the `serpapi` library installed:\n",
        "```bash\n",
        "pip install google-search-results\n",
        "```\n",
        "\n",
        "You’ll also need a free [SerpAPI](https://serpapi.com/) key for Google search.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **What’s Happening Here?**\n",
        "1. **LLM**: The OpenAI model is the reasoning engine for the application.\n",
        "2. **Tool (Search)**: The `SerpAPIWrapper` connects to the Google search API to fetch real-time web results.\n",
        "3. **Agent**: The agent decides when to invoke the tool based on the query. It uses a reasoning framework (e.g., ReAct) to:\n",
        "   - Decide whether it knows the answer.\n",
        "   - Use the search tool when needed.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works in Practice**\n",
        "1. **Input**: The user asks a question like, \"Who won the Nobel Prize in Literature in 2023?\"\n",
        "2. **Reasoning**:\n",
        "   - The agent evaluates whether the LLM can answer using its internal knowledge.\n",
        "   - If not, it calls the search tool (e.g., SerpAPI) and retrieves the result.\n",
        "3. **Output**: The agent combines LLM reasoning with the fetched information and provides an answer.\n",
        "\n",
        "---\n",
        "\n",
        "### **Next Steps**\n",
        "- **Custom Tools**: Create your own tool for specific APIs (e.g., weather, stock market).\n",
        "- **Multi-Tool Setup**: Add multiple tools (e.g., search, calculator, translator) to make the agent more capable.\n",
        "- **Chaining**: Combine tools with memory or other chains to build advanced workflows.\n",
        "\n"
      ],
      "metadata": {
        "id": "OTTzyrlqp5o4"
      },
      "id": "OTTzyrlqp5o4"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.tools import tool\n",
        "from langchain.agents.tools import SerpAPIWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1: Define the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# Step 2: Set up the search tool\n",
        "search_tool = SerpAPIWrapper(serpapi_api_key=\"your_serpapi_api_key\")\n",
        "\n",
        "# Step 3: Define the tool for the agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Search\",\n",
        "        func=search_tool.run,\n",
        "        description=\"Use this tool to search the web for information.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Step 4: Initialize the agent\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "# Step 5: Interact with the agent\n",
        "query = \"Who won the Nobel Prize in Literature in 2023?\"\n",
        "response = agent.run(query)\n",
        "\n",
        "# Print the result\n",
        "print(\"AI Response:\", response)"
      ],
      "metadata": {
        "id": "AFH3HyWlfwXS"
      },
      "id": "AFH3HyWlfwXS",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **What Is Chaining?**\n",
        "Chaining involves:\n",
        "1. Breaking a complex task into smaller steps.\n",
        "2. Passing outputs from one step as inputs to the next.\n",
        "3. Using modular components (like LLMs, tools, or prompts) for each step.\n",
        "\n",
        "LangChain makes this seamless by providing support for **Sequential Chains** and custom workflows.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "1. **Description Generation**:\n",
        "   - The first chain takes a `topic` as input and generates a creative description using the `description_prompt`.\n",
        "2. **Summary**:\n",
        "   - The second chain takes the generated description and summarizes it into one sentence using the `summary_prompt`.\n",
        "3. **Sequential Execution**:\n",
        "   - LangChain passes the output of the first step as input to the second step.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advanced Example: Multi-Tool Chain**\n",
        "Let’s enhance this by adding a **search tool** to fetch live information before generating the description. This would look like:\n",
        "1. Use the search tool to gather information.\n",
        "2. Create a description based on the search results.\n",
        "3. Summarize the description.\n"
      ],
      "metadata": {
        "id": "tgudFl5NqyqW"
      },
      "id": "tgudFl5NqyqW"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.schema import RunnableSequence\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Step 1: Define the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "\n",
        "# Step 2: Define prompt templates for each step\n",
        "description_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a creative and engaging description about {topic}.\"\n",
        ")\n",
        "\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"description\"],\n",
        "    template=\"Summarize the following description in one sentence:\\n{description}\"\n",
        ")\n",
        "\n",
        "# Step 3: Create chains for each task\n",
        "description_chain = RunnableSequence(first=description_prompt, last=llm)\n",
        "summary_chain = RunnableSequence(first=summary_prompt, last=llm)\n",
        "\n",
        "# Step 4: Combine chains into a sequential workflow\n",
        "def generate_and_summarize(topic):\n",
        "    # Step 4.1: Generate the description\n",
        "    description = description_chain.invoke({\"topic\": topic})\n",
        "\n",
        "    # Step 4.2: Summarize the description\n",
        "    summary = summary_chain.invoke({\"description\": description})\n",
        "\n",
        "    return description, summary\n",
        "\n",
        "# Step 5: Run the chain with input\n",
        "topic = \"artificial intelligence\"\n",
        "description, summary = generate_and_summarize(topic)\n",
        "\n",
        "# Print results\n",
        "print(\"Generated Description:\", description)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "id": "-EQHv4XQrU9d"
      },
      "id": "-EQHv4XQrU9d",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}