{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJbh2jtz0JntKT89KRJZfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_044_langchain_sequential_chains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning Langchain Chains**\n",
        "Chains are the backbone of LangChain and allow you to sequence multiple steps for complex workflows. Start with simple chains and progress to more advanced ones.\n",
        "\n",
        "- **Simple Chains**: Create a chain that connects a prompt to an LLM (similar to what you’ve already done).\n",
        "- **Sequential Chains**: Link multiple prompts or LLM calls together for tasks like:\n",
        "  - Summarizing a document and generating follow-up questions.\n",
        "  - Translating text into multiple languages in one workflow."
      ],
      "metadata": {
        "id": "7jizPueKYDYB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdJ7zhe6W2RZ"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain\n",
        "# !pip install openai\n",
        "# !pip install python-dotenv\n",
        "# !pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema.runnable import RunnableSequence\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "# Set the environment variable globally for libraries like LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "# Print the API key to confirm it's loaded correctly\n",
        "print(\"API Key loaded from .env:\",os.environ[\"OPENAI_API_KEY\"][0:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOfNnaN3W-Ax",
        "outputId": "b86ac319-794b-415a-d8eb-0bcae82aff07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded from .env: sk-proj-e1GUWruINPRnrozmiakkRM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chains\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "\n",
        "#### **1. Prompt Template**\n",
        "- **What it is**: A template that defines the structure of the input you provide to the LLM.\n",
        "- **Purpose**: It allows you to dynamically insert variables (e.g., `{question}`) into a predefined format.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  simple_prompt = PromptTemplate(input_variables=[\"question\"], template=\"Answer the question: {question}\")\n",
        "  ```\n",
        "  - `input_variables`: A list of placeholders (`[\"question\"]`) that will be replaced with actual values.\n",
        "  - `template`: The string structure for the prompt. In this case, the LLM will receive:\n",
        "    ```\n",
        "    Answer the question: [your question here]\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. LLM Initialization**\n",
        "- **What it is**: The `ChatOpenAI` object connects to OpenAI's LLMs (e.g., GPT-4) and handles the communication.\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Chaining with RunnableSequence**\n",
        "- **What it is**: A modular way to connect components (like prompts and models) into a sequential workflow.\n",
        "- **Purpose**: To send the output of one step (the prompt) as input to the next step (the LLM).\n",
        "- **How it Works**:\n",
        "  ```python\n",
        "  simple_chain = simple_prompt | llm\n",
        "  ```\n",
        "  - `simple_prompt`: Creates the formatted text based on your input (e.g., `{\"question\": \"What is the capital of France?\"}` becomes `Answer the question: What is the capital of France?`).\n",
        "  - `|`: Passes the output of the prompt directly to the LLM.\n",
        "  - `llm`: Takes the formatted input and generates a response.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Invoking the Chain**\n",
        "- **What it is**: The `invoke` method runs the entire chain with the provided input.\n",
        "- **Purpose**: Executes all the steps in the chain and returns the final output.\n",
        "- **Example**:\n",
        "  ```python\n",
        "  response = simple_chain.invoke({\"question\": \"What is the capital of France?\"})\n",
        "  ```\n",
        "  - The `{\"question\": \"What is the capital of France?\"}` input replaces `{question}` in the prompt template.\n",
        "  - The formatted prompt (`Answer the question: What is the capital of France?`) is sent to the LLM.\n",
        "  - The LLM generates a response, e.g., `\"The capital of France is Paris.\"`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Response Output**\n",
        "- **What it is**: The final result generated by the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This is Important**\n",
        "- **Modularity**: The chain is modular, meaning you can easily add or remove components (e.g., preprocessors, postprocessors).\n",
        "- **Reusability**: The prompt and LLM setup can be reused across different workflows.\n",
        "- **Simplicity**: By chaining steps together, you simplify the logic needed to handle complex tasks.\n",
        "\n",
        "---\n",
        "\n",
        "### **What You’re Learning Here**\n",
        "1. **Prompt Design**: How to structure prompts dynamically using templates.\n",
        "2. **LLM Querying**: How to interact with LLMs using LangChain’s `ChatOpenAI`.\n",
        "3. **Chaining**: How to connect multiple components in a sequence using `RunnableSequence`.\n"
      ],
      "metadata": {
        "id": "x29F__Uubohm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Simple Chain"
      ],
      "metadata": {
        "id": "abl_wpElyrW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a simple prompt template\n",
        "simple_prompt = PromptTemplate(input_variables=[\"question\"], template=\"Answer the question: {question}\")\n",
        "\n",
        "# Step 2: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4\", temperature=0.5)\n",
        "\n",
        "# Step 3: Create a runnable sequence\n",
        "simple_chain = simple_prompt | llm  # This is the new RunnableSequence style\n",
        "\n",
        "# Step 4: Run the chain\n",
        "response = simple_chain.invoke({\"question\": \"What is the capital of France?\"})\n",
        "\n",
        "# Step 5: Print the response\n",
        "print(\"Simple Chain Response:\", response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rSysUk8bqlb",
        "outputId": "17b150f7-2082-46d6-c2dc-0969dc108509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Chain Response: The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **When the LLM is Not the Last Step**\n",
        "\n",
        "While it's common to place the **LLM** as the last step in the pipeline to generate the final output, there are scenarios where you might include additional steps after the model. It depends entirely on the workflow and the use case you're addressing.\n",
        "\n",
        "#### **1. Post-Processing After the LLM**\n",
        "- **Why?**: Sometimes, the raw output from the LLM needs to be cleaned, formatted, or transformed before it's useful.\n",
        "- **Example**: Extracting structured information from a long LLM-generated response.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "  # Define a post-processing function\n",
        "  def extract_first_sentence(output: str) -> str:\n",
        "      return output.split(\".\")[0] + \".\"\n",
        "\n",
        "  post_processor = RunnableLambda(extract_first_sentence)\n",
        "\n",
        "  # Add post-processing after the LLM\n",
        "  pipeline = step_one | step_two | simple_prompt | llm | post_processor\n",
        "\n",
        "  response = pipeline.invoke({\"input_data\": \"Tell me a fun fact about space.\"})\n",
        "  print(\"Processed Response:\", response)\n",
        "  ```\n",
        "\n",
        "#### **2. Combining Results**\n",
        "- **Why?**: If you're running multiple parallel LLM calls, you may need a final step to combine their outputs into one coherent result.\n",
        "- **Example**: Translate text into multiple languages and then merge all translations into a single JSON.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "  # Combine translations into a dictionary\n",
        "  def combine_translations(outputs: dict) -> dict:\n",
        "      return {\"translations\": outputs}\n",
        "\n",
        "  combiner = RunnableLambda(combine_translations)\n",
        "\n",
        "  # Parallel LLM chains\n",
        "  pipeline = french_translation_chain | spanish_translation_chain | combiner\n",
        "  ```\n",
        "\n",
        "#### **3. Routing After the LLM**\n",
        "- **Why?**: The LLM might provide intermediate insights, and the next step could depend on its output (e.g., decision-making logic).\n",
        "- **Example**: The LLM categorizes a query into \"math\" or \"language,\" and the output is routed to the appropriate post-processor.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "  # Routing logic based on LLM output\n",
        "  def route_based_on_output(output: str):\n",
        "      if \"math\" in output.lower():\n",
        "          return \"math_pipeline\"\n",
        "      else:\n",
        "          return \"language_pipeline\"\n",
        "\n",
        "  router = RunnableLambda(route_based_on_output)\n",
        "\n",
        "  # Add routing after LLM\n",
        "  pipeline = step_one | llm | router\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **When the LLM is the Last Step**\n",
        "\n",
        "Placing the LLM as the last step is more appropriate for straightforward workflows, such as:\n",
        "1. **Text Completion**: When the LLM's response is directly consumed (e.g., generating a story or answering a question).\n",
        "2. **Single-Step Outputs**: If there’s no need for further processing after the LLM.\n",
        "3. **Conversational Agents**: Where the LLM produces the final response to display to a user.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "pipeline = step_one | step_two | llm\n",
        "response = pipeline.invoke({\"input_data\": \"Generate a creative story about AI.\"})\n",
        "print(\"Final Output:\", response)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "a4qgxz4afF1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Understanding Chains in LangChain**\n",
        "\n",
        "#### **What Are Chains?**\n",
        "Chains in LangChain are workflows that combine multiple steps, such as prompts, LLMs, tools, or custom functions, into a **sequential** or **modular process**. Each step in a chain takes input, processes it, and passes its output to the next step. This enables the creation of **complex, multi-step workflows** that involve large language models (LLMs) and external tools.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Chains?**\n",
        "\n",
        "1. **Simplify Complex Tasks**:\n",
        "   - Chains break down complex workflows into smaller, manageable steps.\n",
        "   - Example: Summarizing a document → generating questions → translating the questions.\n",
        "\n",
        "2. **Reusability**:\n",
        "   - Individual steps (e.g., prompts or models) can be reused across multiple workflows.\n",
        "   - Example: A summarization chain can be reused in workflows for question generation or translation.\n",
        "\n",
        "3. **Modularity**:\n",
        "   - Chains are modular, so you can easily add, remove, or replace steps in the process.\n",
        "   - Example: Add a post-processing step after generating questions.\n",
        "\n",
        "4. **Dynamic Input and Output**:\n",
        "   - Chains allow you to connect inputs and outputs dynamically, making them adaptable to various tasks.\n",
        "   - Example: Pass the output of a summarization step as input to a question-generation step.\n",
        "\n",
        "5. **Debugging and Flexibility**:\n",
        "   - Chains make it easier to debug and test intermediate outputs at each step of the workflow.\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Chains**\n",
        "\n",
        "1. **Simple Chains**:\n",
        "   - A single-step chain that connects a **prompt** to an **LLM**.\n",
        "   - **Use Case**: Answering a question, text generation.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     simple_chain = simple_prompt | llm\n",
        "     response = simple_chain.invoke({\"question\": \"What is the capital of France?\"})\n",
        "     ```\n",
        "\n",
        "2. **Sequential Chains**:\n",
        "   - Combine multiple steps in sequence, where the output of one step becomes the input to the next.\n",
        "   - **Use Case**: Summarization → question generation → translation.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     sequential_chain = SequentialChain(chains=[summary_chain, question_chain])\n",
        "     response = sequential_chain.invoke({\"input\": \"Your text here\"})\n",
        "     ```\n",
        "\n",
        "3. **Parallel Chains**:\n",
        "   - Run multiple tasks in parallel and combine their outputs.\n",
        "   - **Use Case**: Translating text into multiple languages simultaneously.\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     parallel_chain = ParallelChain(chains={\"French\": french_chain, \"Spanish\": spanish_chain})\n",
        "     response = parallel_chain.invoke({\"input\": \"Your text here\"})\n",
        "     ```\n",
        "\n",
        "4. **Router Chains**:\n",
        "   - Dynamically route inputs to different sub-chains based on the input type or content.\n",
        "   - **Use Case**: Handling queries that require specialized processing (e.g., math vs. language queries).\n",
        "   - **Example**:\n",
        "     ```python\n",
        "     router_chain = MultiPromptChain(prompt_to_chain_map={\"math\": math_chain, \"language\": language_chain})\n",
        "     response = router_chain.run({\"question\": \"What is 5+5?\"})\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Components of a Chain**\n",
        "\n",
        "1. **Prompt Templates**:\n",
        "   - Define the structure of input for the LLM.\n",
        "   - Example:\n",
        "     ```python\n",
        "     summary_prompt = PromptTemplate(input_variables=[\"text\"], template=\"Summarize the following text: {text}\")\n",
        "     ```\n",
        "\n",
        "2. **LLM (Language Model)**:\n",
        "   - The LLM performs the core computation (e.g., generating responses or processing text).\n",
        "   - Example:\n",
        "     ```python\n",
        "     llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
        "     ```\n",
        "\n",
        "3. **Custom Functions**:\n",
        "   - Use custom Python functions for pre- or post-processing.\n",
        "   - Example:\n",
        "     ```python\n",
        "     process_output = RunnableLambda(lambda output: output.content.upper())\n",
        "     ```\n",
        "\n",
        "4. **Input and Output Keys**:\n",
        "   - Define how data flows between steps in the chain.\n",
        "   - Example: The first chain outputs `summary`, which becomes the input for the next chain.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Features of Chains**\n",
        "\n",
        "1. **Dynamic Workflow**:\n",
        "   - Chains can dynamically adapt to inputs, making them flexible for diverse use cases.\n",
        "\n",
        "2. **Multi-Output Support**:\n",
        "   - Chains can produce multiple outputs for downstream tasks (e.g., a summary and generated questions).\n",
        "\n",
        "3. **Integration with Tools**:\n",
        "   - Chains can interact with external tools, like calculators or APIs, in multi-step workflows.\n",
        "\n",
        "4. **Error Handling**:\n",
        "   - Chains simplify debugging by providing clear intermediate outputs at each step.\n",
        "\n",
        "---\n",
        "\n",
        "### **Use Cases of Chains**\n",
        "\n",
        "1. **Text Processing Pipelines**:\n",
        "   - Summarize text, extract key points, and generate questions.\n",
        "   - Example: An educational app that provides summaries and quizzes.\n",
        "\n",
        "2. **Conversational AI**:\n",
        "   - Carry out multi-turn conversations with memory and contextual awareness.\n",
        "   - Example: A chatbot that remembers user preferences.\n",
        "\n",
        "3. **Data Transformation**:\n",
        "   - Convert text into structured formats, such as JSON or CSV.\n",
        "   - Example: Extracting data from documents for automation workflows.\n",
        "\n",
        "4. **Document Search and QA**:\n",
        "   - Search a knowledge base for relevant documents, summarize them, and answer questions.\n",
        "   - Example: Customer support bots or legal document analysis.\n",
        "\n",
        "5. **Multilingual Translation**:\n",
        "   - Translate text into multiple languages in a single workflow.\n",
        "   - Example: Cross-border customer communication tools.\n",
        "\n",
        "---\n",
        "\n",
        "### **Best Practices**\n",
        "\n",
        "1. **Start Simple**:\n",
        "   - Build small chains first (e.g., prompt → LLM) and expand as needed.\n",
        "\n",
        "2. **Debug Intermediate Outputs**:\n",
        "   - Test and print outputs at each step to ensure the workflow is functioning correctly.\n",
        "\n",
        "3. **Use Modular Components**:\n",
        "   - Design chains with reusable steps for flexibility and scalability.\n",
        "\n",
        "4. **Specify Input/Output Keys**:\n",
        "   - Explicitly define input and output keys to prevent data flow issues.\n",
        "\n",
        "5. **Test with Real Data**:\n",
        "   - Use realistic inputs to validate the performance and robustness of the chain.\n",
        "\n"
      ],
      "metadata": {
        "id": "WkjofVETfy0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Sequential Chain"
      ],
      "metadata": {
        "id": "JPuzrDIEr_oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\", temperature=0.5)\n",
        "\n",
        "# Step 2: Define the first prompt (summarization)\n",
        "summary_prompt = PromptTemplate(input_variables=[\"input\"], template=\"Summarize the following text: {input}\")\n",
        "summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"summary\")\n",
        "\n",
        "# Step 3: Define the second prompt (question generation)\n",
        "question_prompt = PromptTemplate(input_variables=[\"summary\"], template=\"Generate 3 questions based on this summary: {summary}\")\n",
        "question_chain = LLMChain(prompt=question_prompt, llm=llm, output_key=\"questions\")\n",
        "\n",
        "# Step 4: Create a sequential chain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[summary_chain, question_chain],\n",
        "    input_variables=[\"input\"],  # Input to the first chain\n",
        "    output_variables=[\"summary\", \"questions\"]  # Final output keys\n",
        ")\n",
        "\n",
        "# Step 5: Run the chain\n",
        "text_to_summarize = \"LangChain is a framework that simplifies building applications with large language models.\"\n",
        "response = sequential_chain.invoke({\"input\": text_to_summarize})\n",
        "\n",
        "# Step 6: Print the response\n",
        "print(\"Summary:\", response[\"summary\"])\n",
        "print(\"\\n-----------  Questions   -----------\\n\")\n",
        "print(response[\"questions\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n7Vh39orJ2e",
        "outputId": "7b263e24-94bb-4e96-86cf-94a9e510adf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: LangChain is a framework designed to make it easier to develop applications using large language models.\n",
            "\n",
            "-----------  Questions   -----------\n",
            "\n",
            "1. What are the primary features of the LangChain framework that facilitate the development of applications using large language models?  \n",
            "2. How does LangChain simplify the integration of large language models into various application types?  \n",
            "3. In what ways can developers benefit from using LangChain when working with large language models compared to traditional methods?  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Code Elements\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Defining Prompts for Each Step**\n",
        "\n",
        "```python\n",
        "summary_prompt = PromptTemplate(input_variables=[\"input\"], template=\"Summarize the following text: {input}\")\n",
        "question_prompt = PromptTemplate(input_variables=[\"summary\"], template=\"Generate 3 questions based on this summary: {summary}\")\n",
        "```\n",
        "\n",
        "- **Explanation**:\n",
        "  - The first prompt (`summary_prompt`) takes an input text (`input`) and asks the LLM to summarize it.\n",
        "  - The second prompt (`question_prompt`) uses the output from the first step (`summary`) as its input to generate questions.\n",
        "- **Key Concept**: Prompts in sequential chains must match the input/output flow between steps.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Creating Individual Chains**\n",
        "\n",
        "```python\n",
        "summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"summary\")\n",
        "question_chain = LLMChain(prompt=question_prompt, llm=llm, output_key=\"questions\")\n",
        "```\n",
        "\n",
        "- **Explanation**:\n",
        "  - **`summary_chain`**: Connects the `summary_prompt` to the LLM and outputs the result as `summary`.\n",
        "  - **`question_chain`**: Connects the `question_prompt` to the LLM and outputs the result as `questions`.\n",
        "- **Key Concept**: Use `output_key` to define what each chain outputs so that subsequent chains can use it.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Creating the Sequential Chain**\n",
        "\n",
        "```python\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[summary_chain, question_chain],\n",
        "    input_variables=[\"input\"],  # Input to the first chain\n",
        "    output_variables=[\"summary\", \"questions\"]  # Final output keys\n",
        ")\n",
        "```\n",
        "\n",
        "- **Explanation**:\n",
        "  - **`chains`**: A list of chains to run in sequence. Each chain's output is passed as the next chain's input.\n",
        "  - **`input_variables`**: Specifies the inputs for the first chain (`input` in this case).\n",
        "  - **`output_variables`**: Specifies which outputs to return from the final chain(s).\n",
        "- **Key Concept**: The sequential chain ensures data flows from one step to the next in the correct order.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Running the Chain**\n",
        "\n",
        "```python\n",
        "response = sequential_chain.invoke({\"input\": text_to_summarize})\n",
        "```\n",
        "\n",
        "- **Explanation**:\n",
        "  - **Input**: You pass `{\"input\": text_to_summarize}` as the starting input to the chain.\n",
        "  - **Output**: The chain returns a dictionary containing the outputs of all steps defined in `output_variables` (`summary` and `questions`).\n",
        "- **Key Concept**: `invoke()` executes the entire sequence and gathers all specified outputs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Overall Flow**\n",
        "\n",
        "1. **Input (`\"input\"`)** → Passed into `summary_chain`.\n",
        "2. **Output (`\"summary\"`)** → Generated by `summary_chain` and passed as input to `question_chain`.\n",
        "3. **Output (`\"questions\"`)** → Generated by `question_chain` as the final result.\n"
      ],
      "metadata": {
        "id": "hCPIfUC0tMpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### **Why Use `LLMChain` and `SequentialChain` in This Example?**\n",
        "\n",
        "The reason for using the structure with `LLMChain` and `SequentialChain` instead of `RunnableSequence` (`|` operator) comes down to **complexity management**, **reusability**, and **functionality differences**.\n",
        "\n",
        "1. **Clear Input/Output Mapping**\n",
        "   - **`LLMChain`** allows you to explicitly define:\n",
        "     - **Input Keys**: The variable(s) required for the step (e.g., `\"input\"` for summarization).\n",
        "     - **Output Keys**: The variable(s) produced by the step (e.g., `\"summary\"`).\n",
        "   - In a multi-step chain like this (summarization → question generation), these explicit mappings are crucial to ensure that:\n",
        "     - The output of `summary_chain` (`summary`) feeds correctly into the `question_chain` as its input.\n",
        "   - This explicit input/output structure is harder to achieve with `RunnableSequence`.\n",
        "\n",
        "   **Example**:\n",
        "   ```python\n",
        "   summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"summary\")\n",
        "   question_chain = LLMChain(prompt=question_prompt, llm=llm, output_key=\"questions\")\n",
        "   ```\n",
        "   - Here, `summary_chain` produces a key `\"summary\"`, and `question_chain` knows to consume it.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Multi-Output Support**\n",
        "   - **`SequentialChain`** supports multiple outputs (`output_variables=[\"summary\", \"questions\"]`).\n",
        "   - This means you can explicitly request multiple outputs at the end of the chain and use them independently.\n",
        "   - With `RunnableSequence`, it’s less intuitive to define multiple outputs, as it’s primarily designed for linear workflows where the final step provides a single result.\n",
        "\n",
        "   **Example**:\n",
        "   ```python\n",
        "   sequential_chain = SequentialChain(\n",
        "       chains=[summary_chain, question_chain],\n",
        "       input_variables=[\"input\"],\n",
        "       output_variables=[\"summary\", \"questions\"]\n",
        "   )\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "3. **Reusability**\n",
        "   - **`LLMChain`** provides a modular design:\n",
        "     - Each chain (`summary_chain`, `question_chain`) is reusable and can be tested independently.\n",
        "     - For instance, you could use the `summary_chain` in a different workflow without modifying the overall chain structure.\n",
        "   - **`RunnableSequence`**, while modular for simpler workflows, does not explicitly separate and name each step, making reusability less straightforward.\n",
        "\n",
        "   **Why Reusability Matters**:\n",
        "   - If you wanted to replace the `question_chain` with a translation step (e.g., translating the summary to another language), you could simply swap out that chain without affecting the rest of the `SequentialChain`.\n",
        "\n"
      ],
      "metadata": {
        "id": "XWnrpgwku2VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Sequential Chain - Paraphrasing and Sentiment Analysis"
      ],
      "metadata": {
        "id": "5aNr9grDvYf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\", temperature=0.5)\n",
        "\n",
        "# Step 2: Define the first prompt (paraphrasing)\n",
        "# paraphrase_prompt = PromptTemplate(\n",
        "#     input_variables=[\"text\"],\n",
        "#     template=\"Rewrite the following text to be more concise: {text}\"\n",
        "# )\n",
        "\n",
        "# Clearer Instruction: Adding \"much shorter\" emphasizes that the rewrite should minimize wordiness.\n",
        "paraphrase_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Rewrite the following text to be much shorter and concise while keeping the key points: {text}\"\n",
        ")\n",
        "\n",
        "paraphrase_chain = LLMChain(prompt=paraphrase_prompt, llm=llm, output_key=\"paraphrased_text\")\n",
        "\n",
        "# Step 3: Define the second prompt (sentiment analysis)\n",
        "sentiment_prompt = PromptTemplate(\n",
        "    input_variables=[\"paraphrased_text\"],\n",
        "    template=\"Analyze the sentiment of the following text and classify it as positive, negative, or neutral: {paraphrased_text}\"\n",
        ")\n",
        "sentiment_chain = LLMChain(prompt=sentiment_prompt, llm=llm, output_key=\"sentiment\")\n",
        "\n",
        "# Step 4: Create a sequential chain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[paraphrase_chain, sentiment_chain],\n",
        "    input_variables=[\"text\"],  # Input to the first chain\n",
        "    output_variables=[\"paraphrased_text\", \"sentiment\"]  # Final output keys\n",
        ")\n",
        "\n",
        "# Step 5: Run the chain\n",
        "input_text = (\n",
        "    \"I just watched the new movie 'Wicked' and I have to say, it completely exceeded my expectations! \"\n",
        "    \"The performances were absolutely magical, with Elphaba's journey brought to life in such a captivating way. \"\n",
        "    \"The visuals were stunning, from the Emerald City to the flying scenes, and the music gave me chills—it's everything a fan could hope for. \"\n",
        "    \"Sure, there were a few small pacing issues, but overall, this was a breathtaking adaptation that I can't stop thinking about. \"\n",
        "    \"I left the theater feeling inspired, uplifted, and ready to see it again!\"\n",
        ")\n",
        "\n",
        "response = sequential_chain.invoke({\"text\": input_text})\n",
        "\n",
        "# Step 6: Print the outputs\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"\\nParaphrased Text:\", response[\"paraphrased_text\"])\n",
        "print(\"\\nSentiment Analysis:\", response[\"sentiment\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPuM83dabqdY",
        "outputId": "eca53817-d309-4410-c45b-edeb0a8ebd2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I just watched the new movie 'Wicked' and I have to say, it completely exceeded my expectations! The performances were absolutely magical, with Elphaba's journey brought to life in such a captivating way. The visuals were stunning, from the Emerald City to the flying scenes, and the music gave me chills—it's everything a fan could hope for. Sure, there were a few small pacing issues, but overall, this was a breathtaking adaptation that I can't stop thinking about. I left the theater feeling inspired, uplifted, and ready to see it again!\n",
            "\n",
            "Paraphrased Text: I just watched 'Wicked' and it exceeded my expectations! The performances were magical, especially Elphaba's journey. The stunning visuals and chilling music made it a must-see for fans. Despite minor pacing issues, it was a breathtaking adaptation that left me inspired and eager to see it again!\n",
            "\n",
            "Sentiment Analysis: The sentiment of the text is positive. The author expresses excitement and satisfaction with their experience watching 'Wicked', highlighting the performances, visuals, and music as exceptional. Even though they mention minor pacing issues, the overall tone is enthusiastic and appreciative, indicating a strong positive sentiment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: Sequential Chain - Summarization and Keyword Extraction"
      ],
      "metadata": {
        "id": "PRStaaCjxRkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SequentialChain, LLMChain\n",
        "\n",
        "\n",
        "# Step 1: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\", temperature=0.5)\n",
        "\n",
        "# Step 2: Define the first prompt (Summarization)\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize the following text into 2-3 sentences: {text}\"\n",
        ")\n",
        "summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"summary\")\n",
        "\n",
        "# Step 3: Define the second prompt (Keyword Extraction)\n",
        "keywords_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=\"From the following summary, extract the 5 most important keywords or topics:\\n{summary}\"\n",
        ")\n",
        "keywords_chain = LLMChain(prompt=keywords_prompt, llm=llm, output_key=\"keywords\")\n",
        "\n",
        "# Step 4: Create a sequential chain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[summary_chain, keywords_chain],\n",
        "    input_variables=[\"text\"],  # Input to the first chain\n",
        "    output_variables=[\"summary\", \"keywords\"]  # Final output keys\n",
        ")\n",
        "\n",
        "# Step 5: Run the chain\n",
        "input_text = (\n",
        "    \"The new movie 'Wicked' has captivated audiences with its stunning visuals, powerful performances, and iconic music. \"\n",
        "    \"Elphaba's journey from an outcast to a powerful figure is portrayed beautifully, and the Emerald City scenes are breathtaking. \"\n",
        "    \"Although some critics noted minor pacing issues, the overall production delivers an emotional and uplifting experience. \"\n",
        "    \"Fans of the Broadway show will appreciate the faithful adaptation, while newcomers will be enchanted by the storytelling and visuals.\"\n",
        ")\n",
        "\n",
        "response = sequential_chain.invoke({\"text\": input_text})\n",
        "\n",
        "# Step 6: Print the outputs\n",
        "print(\"Original Text:\", input_text)\n",
        "print(\"\\nSummary:\", response[\"summary\"])\n",
        "print(\"\\nKeywords:\", response[\"keywords\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfYSpUXtxTYI",
        "outputId": "96cf78fb-df40-4764-ab3b-0ae457a69e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The new movie 'Wicked' has captivated audiences with its stunning visuals, powerful performances, and iconic music. Elphaba's journey from an outcast to a powerful figure is portrayed beautifully, and the Emerald City scenes are breathtaking. Although some critics noted minor pacing issues, the overall production delivers an emotional and uplifting experience. Fans of the Broadway show will appreciate the faithful adaptation, while newcomers will be enchanted by the storytelling and visuals.\n",
            "\n",
            "Summary: The new movie 'Wicked' has impressed audiences with its stunning visuals, strong performances, and memorable music, effectively showcasing Elphaba's transformation from an outcast to a powerful figure. While some critics pointed out minor pacing issues, the overall production offers an emotional and uplifting experience that appeals to both fans of the Broadway show and newcomers alike.\n",
            "\n",
            "Keywords: 1. Wicked\n",
            "2. Elphaba\n",
            "3. Visuals\n",
            "4. Performances\n",
            "5. Music\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Example: Three-Step Sequential Chain\n",
        "\n",
        "#### Workflow Steps:\n",
        "1. **Summarization**: Shorten the input text into a concise summary.\n",
        "2. **Keyword Extraction**: Extract the 5 most important keywords or topics from the summary.\n",
        "3. **Question Generation**: Generate 3 follow-up questions based on the extracted keywords.\n",
        "\n"
      ],
      "metadata": {
        "id": "axeMYWUaybmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\", temperature=0.5)\n",
        "\n",
        "# Step 2: Define the first prompt (Summarization)\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize the following text into 2-3 sentences: {text}\"\n",
        ")\n",
        "summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"summary\")\n",
        "\n",
        "# Step 3: Define the second prompt (Keyword Extraction)\n",
        "keywords_prompt = PromptTemplate(\n",
        "    input_variables=[\"summary\"],\n",
        "    template=\"From the following summary, extract the 5 most important keywords or topics:\\n{summary}\"\n",
        ")\n",
        "keywords_chain = LLMChain(prompt=keywords_prompt, llm=llm, output_key=\"keywords\")\n",
        "\n",
        "# Step 4: Define the third prompt (Follow-Up Questions)\n",
        "questions_prompt = PromptTemplate(\n",
        "    input_variables=[\"keywords\"],\n",
        "    template=\"Using the following keywords, generate 3 engaging follow-up questions:\\n{keywords}\"\n",
        ")\n",
        "questions_chain = LLMChain(prompt=questions_prompt, llm=llm, output_key=\"follow_up_questions\")\n",
        "\n",
        "# Step 5: Create a sequential chain\n",
        "sequential_chain = SequentialChain(\n",
        "    chains=[summary_chain, keywords_chain, questions_chain],\n",
        "    input_variables=[\"text\"],  # Input to the first chain\n",
        "    output_variables=[\"summary\", \"keywords\", \"follow_up_questions\"]  # Final output keys\n",
        ")\n",
        "\n",
        "# Step 6: Run the chain\n",
        "input_text = (\n",
        "    \"The new movie 'Wicked' has captivated audiences with its stunning visuals, powerful performances, and iconic music. \"\n",
        "    \"Elphaba's journey from an outcast to a powerful figure is portrayed beautifully, and the Emerald City scenes are breathtaking. \"\n",
        "    \"Although some critics noted minor pacing issues, the overall production delivers an emotional and uplifting experience. \"\n",
        "    \"Fans of the Broadway show will appreciate the faithful adaptation, while newcomers will be enchanted by the storytelling and visuals.\"\n",
        ")\n",
        "\n",
        "response = sequential_chain.invoke({\"text\": input_text})\n",
        "\n",
        "# Step 7: Print the outputs\n",
        "print(\"Original Text:\\n\", input_text)\n",
        "print(\"\\nSummary:\\n\", response[\"summary\"])\n",
        "print(\"\\nKeywords:\\n\", response[\"keywords\"])\n",
        "print(\"\\nFollow-Up Questions:\\n\", response[\"follow_up_questions\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSqLNniJxTVi",
        "outputId": "91ebc1ef-d7a5-4a25-c44b-0575e197c924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " The new movie 'Wicked' has captivated audiences with its stunning visuals, powerful performances, and iconic music. Elphaba's journey from an outcast to a powerful figure is portrayed beautifully, and the Emerald City scenes are breathtaking. Although some critics noted minor pacing issues, the overall production delivers an emotional and uplifting experience. Fans of the Broadway show will appreciate the faithful adaptation, while newcomers will be enchanted by the storytelling and visuals.\n",
            "\n",
            "Summary:\n",
            " The new movie 'Wicked' has impressed audiences with its stunning visuals, strong performances, and memorable music, effectively portraying Elphaba's transformation from an outcast to a powerful figure. While some critics pointed out minor pacing issues, the overall production offers an emotional and uplifting experience that appeals to both fans of the Broadway show and newcomers alike.\n",
            "\n",
            "Keywords:\n",
            " 1. Wicked\n",
            "2. Elphaba\n",
            "3. Visuals\n",
            "4. Performances\n",
            "5. Music\n",
            "\n",
            "Follow-Up Questions:\n",
            " Sure! Here are three engaging follow-up questions based on your keywords:\n",
            "\n",
            "1. How do the visuals in \"Wicked\" enhance Elphaba's character development throughout the performances?\n",
            "2. In what ways do the musical elements of \"Wicked\" contribute to the emotional impact of Elphaba's journey?\n",
            "3. Which specific performance of \"Wicked\" do you think best captures the essence of Elphaba, and what visuals or musical choices stood out to you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example: SEO Optimization"
      ],
      "metadata": {
        "id": "-mywqsu22CNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the LLM\n",
        "llm = ChatOpenAI(openai_api_key=api_key, model=\"gpt-4o-mini\", temperature=0.5)\n",
        "\n",
        "# Step 2: Define the first prompt (Summarization for Meta Description)\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize the following content into 2-3 sentences to be used as a meta description for SEO:\\n{text}\"\n",
        ")\n",
        "summary_chain = LLMChain(prompt=summary_prompt, llm=llm, output_key=\"meta_description\")\n",
        "\n",
        "# Step 3: Define the second prompt (Keyword Extraction)\n",
        "keywords_prompt = PromptTemplate(\n",
        "    input_variables=[\"meta_description\"],\n",
        "    template=\"From the following meta description, extract the 5 most relevant SEO keywords:\\n{meta_description}\"\n",
        ")\n",
        "keywords_chain = LLMChain(prompt=keywords_prompt, llm=llm, output_key=\"seo_keywords\")\n",
        "\n",
        "# Step 4: Define the third prompt (Generate Related Search Questions)\n",
        "questions_prompt = PromptTemplate(\n",
        "    input_variables=[\"seo_keywords\"],\n",
        "    template=\"Using the following keywords, generate 5 engaging search queries or follow-up questions that users might search for:\\n{seo_keywords}\"\n",
        ")\n",
        "questions_chain = LLMChain(prompt=questions_prompt, llm=llm, output_key=\"related_questions\")\n",
        "\n",
        "# Step 5: Create a sequential chain\n",
        "seo_chain = SequentialChain(\n",
        "    chains=[summary_chain, keywords_chain, questions_chain],\n",
        "    input_variables=[\"text\"],  # Input to the first chain\n",
        "    output_variables=[\"meta_description\", \"seo_keywords\", \"related_questions\"]  # Final outputs\n",
        ")\n",
        "\n",
        "# Step 6: Run the chain\n",
        "input_text = (\n",
        "    \"The new movie 'Wicked' has captivated audiences with its stunning visuals, powerful performances, and iconic music. \"\n",
        "    \"Elphaba's journey from an outcast to a powerful figure is portrayed beautifully, and the Emerald City scenes are breathtaking. \"\n",
        "    \"Although some critics noted minor pacing issues, the overall production delivers an emotional and uplifting experience. \"\n",
        "    \"Fans of the Broadway show will appreciate the faithful adaptation, while newcomers will be enchanted by the storytelling and visuals.\"\n",
        ")\n",
        "\n",
        "response = seo_chain.invoke({\"text\": input_text})\n",
        "\n",
        "# Step 7: Print the outputs\n",
        "print(\"Meta Description (SEO):\\n\", response[\"meta_description\"])\n",
        "print(\"\\nSEO Keywords:\\n\", response[\"seo_keywords\"])\n",
        "print(\"\\nRelated Search Queries:\\n\", response[\"related_questions\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZsFCmTZxTS7",
        "outputId": "0b222ab0-e758-4280-e8f4-43058bc16ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta Description (SEO):\n",
            " Experience the magic of 'Wicked,' a visually stunning film that brings Elphaba's transformative journey to life with powerful performances and iconic music. While some critics mention minor pacing issues, the adaptation captivates both Broadway fans and newcomers alike, delivering an emotional and uplifting cinematic experience. Don't miss this enchanting tale set against the breathtaking backdrop of the Emerald City.\n",
            "\n",
            "SEO Keywords:\n",
            " 1. Wicked\n",
            "2. Elphaba\n",
            "3. Broadway\n",
            "4. cinematic experience\n",
            "5. Emerald City\n",
            "\n",
            "Related Search Queries:\n",
            " Sure! Here are five engaging search queries or follow-up questions based on the provided keywords:\n",
            "\n",
            "1. \"How does Elphaba's journey in Wicked compare to the original Wizard of Oz in the cinematic experience?\"\n",
            "2. \"What are the must-see moments from Wicked on Broadway that bring the Emerald City to life?\"\n",
            "3. \"Is there a film adaptation of Wicked featuring Elphaba, and when can we expect it to be released?\"\n",
            "4. \"What makes the Broadway production of Wicked a unique cinematic experience for fans of the Emerald City?\"\n",
            "5. \"How has Elphaba's character evolved in Wicked from the stage to the screen, and what can audiences expect in the upcoming cinematic experience?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKLoflrBf5km"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}