{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_006_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827",
      "metadata": {
        "id": "8b0e11f2-9ea4-48c2-b8d2-d0a4ba967827"
      },
      "source": [
        "## Gradio - What is it?\n",
        "\n",
        "Gradio is an open-source Python library that allows you to quickly create and deploy user interfaces (UIs) for machine learning models, APIs, or any interactive applications. It is particularly useful for creating simple web interfaces where users can interact with models in real time, such as uploading images, entering text, or recording audio, and then receiving outputs directly on the web page.\n",
        "\n",
        "What makes Gradio special:\n",
        "- **Ease of use**: You can create a fully functional interface with just a few lines of Python code.\n",
        "- **Real-time interaction**: It enables users to interact with models and see results instantly, which is especially useful for testing models like LLMs, image classifiers, or translators.\n",
        "- **Customizable interfaces**: Gradio supports a wide range of input and output types (text, images, video, audio, etc.), making it flexible for various use cases.\n",
        "- **Shareability**: Gradio automatically generates a shareable link, allowing you to easily share your model's interface with others.\n",
        "- **Deployment**: It is widely used for demos, prototyping, or even full-scale deployment, and integrates smoothly with models from frameworks like PyTorch, TensorFlow, Hugging Face, etc.\n",
        "\n",
        "Gradio is especially appealing for rapid prototyping and creating accessible tools for machine learning applications."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "h_7udReKTYx6"
      },
      "id": "h_7udReKTYx6"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "# !pip install google-generativeai\n",
        "# !pip install anthropic\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "fGZjrqC9Tctf"
      },
      "id": "fGZjrqC9Tctf",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "eAeNeqsTUKNl"
      },
      "id": "eAeNeqsTUKNl"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
      "metadata": {
        "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import google.generativeai\n",
        "import anthropic\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write API Keys to .env file"
      ],
      "metadata": {
        "id": "anzSPW02UFj6"
      },
      "id": "anzSPW02UFj6"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
      "metadata": {
        "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa8ebf4-720e-441f-8d66-44f884a5d174"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env file created at: /content/API_KEYS.env\n",
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Oct 25 18:39 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 25 18:36 ..\n",
            "-rw-r--r-- 1 root root  362 Oct 25 18:39 API_KEYS.env\n",
            "drwxr-xr-x 4 root root 4096 Oct 24 13:20 .config\n",
            "drwxr-xr-x 1 root root 4096 Oct 24 13:20 sample_data\n"
          ]
        }
      ],
      "source": [
        "# Path to the .env file\n",
        "env_file_path = '/content/API_KEYS.env' # naming file makes it visible, no name makes it hidden\n",
        "\n",
        "# Your OpenAI API key\n",
        "OPEN_API_KEY = \"sk-proj-e1GUWru####\"\n",
        "ANTHROPIC_API_KEY = \"sk-ant-api####\"\n",
        "GOOGLE_API_KEY = \"AIzaSyDh3aiDL####\"\n",
        "\n",
        "# Create the .env file and write the API keys\n",
        "with open(env_file_path, 'w') as f:\n",
        "    f.write(f\"OPENAI_API_KEY={OPEN_API_KEY}\\n\")\n",
        "    f.write(f\"ANTHROPIC_API_KEY={ANTHROPIC_API_KEY}\\n\")\n",
        "    f.write(f\"GOOGLE_API_KEY={GOOGLE_API_KEY}\\n\")\n",
        "\n",
        "print(f\".env file created at: {env_file_path}\")\n",
        "# List all files (including hidden ones) in the /content/ folder\n",
        "!ls -la /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables\n",
        "\n",
        "Let's clarify how `os.getenv()` and environment variables work to clear up any confusion.\n",
        "\n",
        "1. **What `os.getenv()` does**:\n",
        "   - `os.getenv('key')` retrieves the value of the environment variable named `'key'`. It does not store values; it simply **fetches** the value of an environment variable. Each time you call `os.getenv()`, it fetches the value of the specific environment variable you ask for.\n",
        "   - For example:\n",
        "     ```python\n",
        "     openai_key = os.getenv('OPENAI_API_KEY')  # Fetches the OpenAI API key\n",
        "     google_key = os.getenv('GOOGLE_API_KEY')  # Fetches the Google API key\n",
        "     ```\n",
        "     Each call retrieves a **different** environment variable. These are independent of each other.\n",
        "\n",
        "\n",
        "This approach is better because:\n",
        "1. **It avoids redundancy**: You don't need to manually set `os.environ` after loading the variables with `os.getenv()`.\n",
        "2. **Direct use of `os.getenv()`**: It is cleaner and ensures that you fetch the API key values directly from the environment.\n",
        "3. **Ensures proper error handling**: You can check if each key is loaded correctly and raise an error or alert if something is missing.\n",
        "\n",
        "### When should you use the alternative (`os.environ` method)?\n",
        "You would typically set `os.environ` manually only if you need the environment variables to be explicitly available in a subprocess or for other libraries that expect the keys in the environment (rare cases).\n"
      ],
      "metadata": {
        "id": "9YTHkEsXUCXe"
      },
      "id": "9YTHkEsXUCXe"
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')  # Ensure this is the correct path to your file\n",
        "\n",
        "# Get the API keys from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Check if the keys are loaded correctly and print a portion of them\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key loaded: {openai_api_key[0:10]}...\")  # Only print part of the key\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded correctly.\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key loaded: {anthropic_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Anthropic API key not loaded correctly.\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key loaded: {google_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Google API key not loaded correctly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHboIt7whdF8",
        "outputId": "3be7a559-5100-4078-d254-ce195fbd7f0d"
      },
      "id": "BHboIt7whdF8",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded: sk-proj-e1...\n",
            "Anthropic API Key loaded: sk-ant-api...\n",
            "Google API Key loaded: AIzaSyDh3a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to OpenAI, Anthropic and Google\n",
        "\n",
        "This code block is configuring connections to three different large language model (LLM) services: OpenAI, Anthropic (Claude), and Google Generative AI. By setting the appropriate API keys, it authenticates your script with each service, enabling you to send requests to these platforms and use their models for tasks like text generation, answering questions, or other AI-driven functionalities. Once connected, you can interact with each platform's models securely and programmatically within your script."
      ],
      "metadata": {
        "id": "-Kr2mcoaixU4"
      },
      "id": "-Kr2mcoaixU4"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai\n",
        "\n",
        "# Connect to OpenAI\n",
        "openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "\n",
        "# Connect to Anthropic (Claude)\n",
        "claude = anthropic.Anthropic(api_key=anthropic_api_key)  # Set Anthropic API key\n",
        "\n",
        "# Connect to Google Generative AI\n",
        "google.generativeai.configure(api_key=google_api_key)  # Set Google API key"
      ],
      "metadata": {
        "id": "722xDZKZi88n"
      },
      "id": "722xDZKZi88n",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Call to GPT-4o-Mini\n",
        "\n",
        "This function, `message_gpt()`, sends a user’s prompt to OpenAI’s GPT-4 model (specifically `'gpt-4o-mini'`), along with a predefined system message (`\"You are a helpful assistant\"`), and returns the model's response.\n",
        "\n",
        "### Summary of what it does:\n",
        "1. **System message**: The system message establishes the behavior of the AI as a \"helpful assistant.\"\n",
        "2. **User input**: The user's prompt is passed to the function.\n",
        "3. **API call**: The function sends both the system message and the user prompt to OpenAI’s API (for GPT-4), specifying the model to use.\n",
        "4. **Response**: It retrieves the first response from the model and returns it.\n",
        "\n",
        "In short, it wraps an API call to GPT-4, allowing the user to send a prompt and get a response based on the assistant-like behavior defined by the system message."
      ],
      "metadata": {
        "id": "JTy1WcC7jy2Y"
      },
      "id": "JTy1WcC7jy2Y"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "02ef9b69-ef31-427d-86d0-b8c799e1c1b1",
      "metadata": {
        "id": "02ef9b69-ef31-427d-86d0-b8c799e1c1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "379d4178-d1fd-481d-e48e-7c31d27486c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Today's date is October 3, 2023.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# A generic system message - no more snarky adversarial AIs!\n",
        "\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "# Let's wrap a call to GPT-4o-mini in a simple function\n",
        "\n",
        "def message_gpt(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    completion = openai.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "message_gpt(\"What is today's date?\") # last day of training for gpt-4o-mini"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "today = date.today()\n",
        "print(\"Today's actual date:\", today)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbYYlKGL_JIc",
        "outputId": "797cb556-8fdb-4ffd-b61c-3fa8fb7d0b35"
      },
      "id": "cbYYlKGL_JIc",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Today's actual date: 2024-10-25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94013d1-4f27-4329-97e8-8c58db93636a",
      "metadata": {
        "id": "f94013d1-4f27-4329-97e8-8c58db93636a"
      },
      "source": [
        "## Gradio Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc664b7a-c01d-4fea-a1de-ae22cdd5141a",
      "metadata": {
        "id": "bc664b7a-c01d-4fea-a1de-ae22cdd5141a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ca7582fa-9252-48f1-9879-95a81cf02ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shout has been called with input hello\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HELLO'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# here's a simple function\n",
        "\n",
        "def shout(text):\n",
        "    print(f\"Shout has been called with input {text}\")\n",
        "    return text.upper()\n",
        "\n",
        "shout(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradio User Interface"
      ],
      "metadata": {
        "id": "gDnFxrSDra7Q"
      },
      "id": "gDnFxrSDra7Q"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "08f1f15a-122e-4502-b112-6ee2817dda32",
      "metadata": {
        "id": "08f1f15a-122e-4502-b112-6ee2817dda32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "5c76d8de-5590-443c-ce2b-3c2ac844d14e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://03a35f0232567251e4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://03a35f0232567251e4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase the Size of Textbox"
      ],
      "metadata": {
        "id": "_pSv_aQMpKbz"
      },
      "id": "_pSv_aQMpKbz"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c9a359a4-685c-4c99-891c-bb4d1cb7f426",
      "metadata": {
        "id": "c9a359a4-685c-4c99-891c-bb4d1cb7f426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "outputId": "183f2bf7-3409-4d5b-fd85-5543bfb052f0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:393: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0ea9356368fd032ad5.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://0ea9356368fd032ad5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shout has been called with input testing out the debugger\n",
            "Shout has been called with input testing out the debugger to show the output in the cell below\n",
            "Shout has been called with input testing out the debugger to show the output in the cell below for a third time\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://03a35f0232567251e4.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://36255b38bbe79f2df3.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://28b45d3e6ee9cd1450.gradio.live\n",
            "Killing tunnel 127.0.0.1:7863 <> https://32937b94d19d3facfc.gradio.live\n",
            "Killing tunnel 127.0.0.1:7864 <> https://0ea9356368fd032ad5.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "view = gr.Interface(\n",
        "    fn=shout,# still using the shout function\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=6)],\n",
        "    outputs=[gr.Textbox(label=\"Response:\", lines=8)],\n",
        "    allow_flagging=\"never\" # removes flag\n",
        ")\n",
        "view.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio & ChatGPT (Chatbot)\n",
        "\n",
        "The key concepts and lessons to take away from this code are how to combine ChatGPT with a Gradio interface to create a simple interactive chatbot, specifically focusing on system and user messages in ChatGPT and how Gradio provides an easy interface for it.\n",
        "\n",
        "### Major Concepts You Should Be Learning:\n",
        "\n",
        "1. **System and User Messages in ChatGPT**:\n",
        "   - **System Messages**: These set the tone or behavior of the assistant (in this case, \"You are a helpful assistant\"). This message is sent as a context to the model to influence its behavior throughout the conversation.\n",
        "   - **User Messages**: These are the actual inputs or prompts from the user, which the model responds to. In this function, the user's message is included as `{\"role\": \"user\", \"content\": prompt}` in the message payload.\n",
        "   - **Message Structure**: You're learning how to build a structured conversation with ChatGPT using roles (system, user) to define different parts of the interaction.\n",
        "\n",
        "2. **Gradio**:\n",
        "   - **What Gradio Does**: Gradio creates an easy-to-use web-based interface to interact with machine learning models. In this case, it is used to create a simple text-based chatbot interface.\n",
        "   - **Major Components**:\n",
        "     - `fn=message_gpt`: This tells Gradio to use your `message_gpt()` function to handle the user input.\n",
        "     - `inputs=[gr.Textbox(...)]`: This creates a text input box where the user enters their message (or prompt).\n",
        "     - `outputs=[gr.Textbox(...)]`: This defines where the AI’s response will be displayed.\n",
        "     - `view.launch(share=True)`: This starts the Gradio interface, and setting `share=True` allows you to share the app via a unique URL that Gradio generates.\n",
        "\n",
        "3. **Combining ChatGPT with Gradio**:\n",
        "   - **How It Works**: You're learning to connect ChatGPT's API (via the `message_gpt()` function) to a web interface using Gradio. When a user types something in the input box, Gradio sends it to the `message_gpt()` function, which sends the message to ChatGPT and returns the response to be displayed in the output box.\n",
        "   - **User Interaction**: The Gradio interface captures user input and displays the AI's response, providing an easy-to-use web interface without having to write complex frontend code.\n",
        "\n",
        "### Major Lessons:\n",
        "1. **System messages control model behavior**: You can adjust the system message to define the \"personality\" or behavior of the assistant (helpful, formal, humorous, etc.).\n",
        "2. **Structured communication with the model**: You're building a structured conversation with roles (system and user) and learning how ChatGPT uses this to generate context-aware responses.\n",
        "3. **Gradio simplifies UI creation**: Gradio is a powerful tool for quickly creating web interfaces for machine learning models, allowing you to easily deploy and share models with others without needing to build a complex interface from scratch.\n",
        "\n",
        "This code teaches how to structure a basic conversational interaction with ChatGPT, connect it to a Gradio interface, and launch it as an interactive application."
      ],
      "metadata": {
        "id": "dR1o1grSsf88"
      },
      "id": "dR1o1grSsf88"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format Response in Markdown"
      ],
      "metadata": {
        "id": "XHyZ87ThqmUi"
      },
      "id": "XHyZ87ThqmUi"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f235288e-63a2-4341-935b-1441f9be969b",
      "metadata": {
        "id": "f235288e-63a2-4341-935b-1441f9be969b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "outputId": "cfa1592f-d693-4221-b9c6-ed62fbed0373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://50a882f6d55240eb47.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50a882f6d55240eb47.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://50a882f6d55240eb47.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# use our function from earlier\n",
        "\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "# Let's wrap a call to GPT-4o-mini in a simple function\n",
        "\n",
        "def message_gpt(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    completion = openai.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "\n",
        "# add GPT to our Gradio interface\n",
        "view = gr.Interface(\n",
        "    fn=message_gpt, # now we use GPT\n",
        "    inputs=[gr.Textbox(label=\"Your message:\", lines=4)],\n",
        "    # outputs=[gr.Textbox(label=\"Response:\", lines=8)], # respond in plain text\n",
        "    outputs=[gr.Markdown(label=\"Response:\")], # add markdown formatting, # add markdown formatting\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch(share=True, debug=True) # debug=True keeps the cell running ubtil stopped manually\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stream Response\n",
        "\n",
        "This code introduces a new concept: **streaming responses from ChatGPT**. Here's a breakdown of the major lessons and concepts you should focus on when learning from this code:\n",
        "\n",
        "### Major Concepts You Should Be Learning:\n",
        "\n",
        "1. **Streaming Responses from ChatGPT**:\n",
        "   - **What is Streaming?**: Instead of waiting for the entire response to be generated before returning it, this code **streams** the response in real time. This allows the user to see parts of the response as they are being generated, providing faster feedback and a smoother user experience.\n",
        "   - **How Streaming Works**:\n",
        "     - `stream=True`: This enables streaming mode in the ChatGPT API. Instead of returning the entire message all at once, the API sends smaller chunks of the response as they are generated.\n",
        "     - **Chunking the Response**: The response is streamed in **chunks**, which are processed one by one in the loop. Each `chunk` is a part of the overall message being constructed.\n",
        "   \n",
        "2. **The Role of `yield`**:\n",
        "   - **What `yield` Does**: Instead of returning the full response at once, `yield` returns a partial result (the accumulated response so far) each time a new chunk is received. This allows for real-time feedback in the interface.\n",
        "   - **Building the Response**: The `result` variable is used to accumulate the chunks of the response. Each new chunk of text is appended to the previous ones, and `yield result` sends this cumulative text back to the user interface (or caller) as it is updated.\n",
        "\n",
        "3. **System and User Messages**:\n",
        "   - As with the previous examples, this function still uses **system** and **user messages** to structure the interaction. The **system message** defines the assistant's behavior, while the **user message** contains the prompt.\n",
        "\n",
        "4. **Combining Streaming with the User Interface**:\n",
        "   - You’re learning a key concept here: how to stream AI responses back to the user in real time, which enhances the user experience. When incorporated with Gradio or any web interface, this creates a **live, dynamic interaction** where users don’t have to wait for the entire response to be generated before seeing any output.\n",
        "\n",
        "### Major Lessons:\n",
        "\n",
        "1. **Streaming Responses Improves User Experience**:\n",
        "   - Streaming can provide faster feedback, particularly useful for longer responses where you want users to see results as they are generated.\n",
        "2. **Yielding Responses Allows Incremental Output**:\n",
        "   - The use of `yield` makes the function return intermediate results, allowing the UI (like Gradio) to update in real time, providing a more dynamic user experience.\n",
        "3. **Streaming Mode in OpenAI API**:\n",
        "   - You're learning how to activate and manage **streaming** in OpenAI's API by setting `stream=True`. This can be particularly useful for chatbots, customer support, or any application where response speed is important.\n",
        "\n",
        "### Practical Application:\n",
        "- In the context of Gradio, this function can be integrated to show the response being typed out in real time as users interact with the model, improving the responsiveness of the chatbot interface.\n",
        "\n",
        "\n",
        "### Delta\n",
        "\n",
        "The `delta` in the code refers to the incremental updates (or partial content) that are streamed from the OpenAI API when `stream=True` is enabled.\n",
        "\n",
        "In streaming mode, the API sends parts of the response as they are generated, rather than waiting to return the full response all at once. Each part, or \"chunk,\" is received as a message containing the **difference** (or **delta**) between the last part of the response and the current part.\n",
        "\n",
        "### Key Concepts:\n",
        "1. **Delta**:\n",
        "   - The term \"delta\" comes from mathematics, where it represents the change or difference between values. In this case, `delta` represents the difference between the response's current state and the next chunk of content generated.\n",
        "   - The OpenAI API uses `delta` to represent the partial text being added to the ongoing response. Each new `chunk` contains only the newly generated text that is being streamed at that moment.\n",
        "\n",
        "2. **How it Works**:\n",
        "   - In streaming mode, the API sends multiple \"chunks\" of data, and each chunk contains the new text that was generated since the previous chunk.\n",
        "   - `chunk.choices[0].delta.content` accesses the newly generated text from the `delta` field of the chunk, and `result += ...` appends this partial content to the overall result.\n",
        "   \n",
        "3. **Building the Response**:\n",
        "   - As the streaming response progresses, each new `delta.content` is added to `result`, which gradually constructs the full response from the model. By doing this in real-time, users can see the response being generated incrementally.\n",
        "\n",
        "### In the Code:\n",
        "```python\n",
        "result += chunk.choices[0].delta.content or \"\"\n",
        "```\n",
        "- This line adds the new content from the current chunk (`chunk.choices[0].delta.content`) to the `result`.\n",
        "- The `or \"\"` part ensures that if `delta.content` is `None` or empty in any chunk, it doesn’t cause an error.\n",
        "\n",
        "### Summary:\n",
        "- **Delta** refers to the new, incremental part of the response generated by the model in streaming mode.\n",
        "- The code is appending these incremental updates (deltas) to `result`, allowing the response to be streamed back to the user in real time."
      ],
      "metadata": {
        "id": "XWEqWwadwMz1"
      },
      "id": "XWEqWwadwMz1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stream GPT"
      ],
      "metadata": {
        "id": "CDY0Ve6Mztl5"
      },
      "id": "CDY0Ve6Mztl5"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f",
      "metadata": {
        "id": "88c04ebf-0671-4fea-95c9-bc1565d4bb4f"
      },
      "outputs": [],
      "source": [
        "# Let's create a call that streams back results\n",
        "system_message = \"You are a helpful assistant with an Australian accent and sunny demeanor\"\n",
        "\n",
        "def stream_gpt(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "      ]\n",
        "    stream = openai.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "        stream=True\n",
        "    )\n",
        "    result = \"\"\n",
        "    for chunk in stream:\n",
        "        result += chunk.choices[0].delta.content or \"\"\n",
        "        yield result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "0bb1f789-ff11-4cba-ac67-11b815e29d09",
      "metadata": {
        "id": "0bb1f789-ff11-4cba-ac67-11b815e29d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "8cd47886-4f7d-4d4b-823e-9e912a784f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e4ca70b4fd17defb8f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e4ca70b4fd17defb8f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7864 <> https://d358016b146ab362a2.gradio.live\n",
            "Killing tunnel 127.0.0.1:7865 <> https://d80ca0f2db48318394.gradio.live\n",
            "Killing tunnel 127.0.0.1:7866 <> https://e4ca70b4fd17defb8f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "view = gr.Interface(\n",
        "    fn=stream_gpt,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stream Claude\n",
        "\n",
        "When working with the ChatGPT API (from OpenAI) and the Claude API (from Anthropic), there are a few key differences in how you interact with them. Let’s break down the differences using your code examples:\n",
        "\n",
        "### 1. **API Structure and Method Calls**:\n",
        "   - **ChatGPT (OpenAI)**:\n",
        "     - You are using `openai.chat.completions.create()` to send a prompt and get the response.\n",
        "     - The API call is structured around the roles of **system** and **user**, and you pass a list of message objects (with roles like `\"system\"` and `\"user\"`).\n",
        "     - **Streaming**: The `stream=True` parameter allows the API to return the response in chunks (delta updates), which you handle in a loop.\n",
        "\n",
        "   - **Claude (Anthropic)**:\n",
        "     - You are using `claude.messages.stream()` to send a prompt and stream the response.\n",
        "     - The Claude API also supports roles, but in your case, the system message is passed as a separate parameter (`system=system_message`), and the messages only include the user’s message.\n",
        "     - **Streaming**: Claude’s streaming response is accessed via `stream.text_stream`, and chunks of text are processed similarly to how ChatGPT works, but through a `with result as stream` block, which handles the stream lifecycle.\n",
        "\n",
        "### 2. **System Messages**:\n",
        "   - **ChatGPT**: The system message is included as part of the `messages` list with the role `\"system\"`. This defines the assistant’s behavior, tone, or persona (in your example, an Australian accent and sunny demeanor).\n",
        "   - **Claude**: The system message is passed separately via the `system` parameter, rather than being embedded in the messages array.\n",
        "\n",
        "### 3. **Streaming the Response**:\n",
        "   - **ChatGPT**:\n",
        "     - You iterate over `stream` directly, and each chunk contains partial content from the model’s response in `chunk.choices[0].delta.content`.\n",
        "     - You construct the result incrementally by appending each chunk of content.\n",
        "   - **Claude**:\n",
        "     - You handle streaming by using `with result as stream`, which is more explicit in handling the stream's lifecycle.\n",
        "     - Inside this block, you access `stream.text_stream`, which is used to yield the response in chunks (similar to ChatGPT’s delta updates).\n",
        "\n",
        "### 4. **Model Names**:\n",
        "   - **ChatGPT**: You specify the model via the `model` parameter (e.g., `'gpt-4o-mini'`). OpenAI has different models (GPT-3, GPT-4, etc.), and you select which one to use.\n",
        "   - **Claude**: You specify the model with the `model` parameter (e.g., `'claude-3-haiku-20240307'`). Anthropic has its own set of models, such as Claude, and different versions might exist for performance variations.\n",
        "\n",
        "### 5. **Token and Temperature Control**:\n",
        "   - **ChatGPT**: You haven’t specified it here, but you can control the number of tokens, temperature, and other settings similar to how you would with Claude.\n",
        "   - **Claude**: In your code, you're specifying `max_tokens=1000` and `temperature=0.7`. These parameters control how long the response can be (in tokens) and how creative or random the model's output should be (temperature).\n",
        "\n",
        "### Summary of Key Differences:\n",
        "- **System Message Handling**: ChatGPT uses the system message inside the `messages` list, while Claude uses a separate `system` parameter.\n",
        "- **Streaming**: ChatGPT streams chunks using `stream=True` with direct iteration, while Claude uses `with result as stream` to handle text streaming.\n",
        "- **API Calls**: The APIs use slightly different method calls (`openai.chat.completions.create()` vs. `claude.messages.stream()`), but both are designed to process and stream large responses in chunks.\n",
        "- **Model Naming**: Each API has its own model names and versioning system.\n",
        "- **Token and Temperature Control**: Both allow for controlling token length and output randomness, though you’ve only specified this in the Claude call.\n",
        "\n",
        "### What You Should Focus On:\n",
        "- **Streaming Behavior**: Understand how both APIs stream data in real-time and how the delta or text chunks are processed.\n",
        "- **System Message and Prompt Structure**: Both ChatGPT and Claude allow you to control the assistant’s behavior via system messages, but they handle this differently.\n",
        "- **API-Specific Syntax**: Each API has its own method and parameter requirements, so knowing the syntax and available options for each is important when switching between them."
      ],
      "metadata": {
        "id": "jMMS6K7Hzzgz"
      },
      "id": "jMMS6K7Hzzgz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbc8e930-ba2a-4194-8f7c-044659150626",
      "metadata": {
        "id": "bbc8e930-ba2a-4194-8f7c-044659150626"
      },
      "outputs": [],
      "source": [
        "def stream_claude(prompt):\n",
        "    result = claude.messages.stream(\n",
        "        model=\"claude-3-haiku-20240307\",\n",
        "        max_tokens=1000,\n",
        "        temperature=0.7,\n",
        "        system=system_message,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "    )\n",
        "    response = \"\"\n",
        "    with result as stream:\n",
        "        for text in stream.text_stream:\n",
        "            response += text or \"\"\n",
        "            yield response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "view = gr.Interface(\n",
        "    fn=stream_claude,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "view.launch()"
      ],
      "metadata": {
        "id": "Id_C7oOTDrRt"
      },
      "id": "Id_C7oOTDrRt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5",
      "metadata": {
        "id": "0087623a-4e31-470b-b2e6-d8d16fc7bcf5"
      },
      "outputs": [],
      "source": [
        "def stream_model(prompt, model):\n",
        "    if model==\"GPT\":\n",
        "        result = stream_gpt(prompt)\n",
        "    elif model==\"Claude\":\n",
        "        result = stream_claude(prompt)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "    for chunk in result:\n",
        "        yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855",
      "metadata": {
        "id": "8d8ce810-997c-4b6a-bc4f-1fc847ac8855"
      },
      "outputs": [],
      "source": [
        "view = gr.Interface(\n",
        "    fn=stream_model,\n",
        "    inputs=[gr.Textbox(label=\"Your message:\"), gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\")],\n",
        "    outputs=[gr.Markdown(label=\"Response:\")],\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "view.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d933865b-654c-4b92-aa45-cf389f1eda3d",
      "metadata": {
        "id": "d933865b-654c-4b92-aa45-cf389f1eda3d"
      },
      "source": [
        "## Build a Brochure Generator\n",
        "\n",
        "- start with out Website class to extract links and important content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1626eb2e-eee8-4183-bda5-1591b58ae3cf",
      "metadata": {
        "id": "1626eb2e-eee8-4183-bda5-1591b58ae3cf"
      },
      "outputs": [],
      "source": [
        "# A class to represent a Webpage\n",
        "\n",
        "class Website:\n",
        "    url: str\n",
        "    title: str\n",
        "    text: str\n",
        "\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        response = requests.get(url)\n",
        "        self.body = response.content\n",
        "        soup = BeautifulSoup(self.body, 'html.parser')\n",
        "        self.title = soup.title.string if soup.title else \"No title found\"\n",
        "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "            irrelevant.decompose()\n",
        "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "    def get_contents(self):\n",
        "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c701ec17-ecd5-4000-9f68-34634c8ed49d",
      "metadata": {
        "id": "c701ec17-ecd5-4000-9f68-34634c8ed49d"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"You are a helpful assistant that analyzes the contents of a company website landing page\"\n",
        "system_prompt += \"You use the website landing page contents to create a short brochure about the company \\\n",
        "                  for prospective customers, investors and recruits.\"\n",
        "system_prompt += \"Respond in markdown.\"\n",
        "\n",
        "# define brochuer function\n",
        "def stream_brochure(company_name, url, model):\n",
        "    prompt = f\"Please generate a company brochure for {company_name}. Here is their landing page:\\n\"\n",
        "    prompt += Website(url).get_contents()\n",
        "    if model==\"GPT\":\n",
        "        result = stream_gpt(prompt)\n",
        "    elif model==\"Claude\":\n",
        "        result = stream_claude(prompt)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "    for chunk in result:\n",
        "        yield chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "66399365-5d67-4984-9d47-93ed26c0bd3d",
      "metadata": {
        "id": "66399365-5d67-4984-9d47-93ed26c0bd3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "464f7871-20c8-425d-8ca3-54d9e6db86c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/interface.py:393: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8ed339152c30647f0c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8ed339152c30647f0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7866 <> https://8ed339152c30647f0c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "view = gr.Interface(\n",
        "    fn=stream_brochure,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Company name:\"),\n",
        "        gr.Textbox(label=\"Landing page URL:\"),\n",
        "        gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\")],\n",
        "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "view.launch(share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}