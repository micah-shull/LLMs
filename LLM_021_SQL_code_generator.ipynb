{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNopIiCQwp4ArjurYKDiCyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_021_SQL_code_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several strategies to make the English to SQL process more efficient, accurate, and context-aware when using LLMs to generate SQL code. Here are some improvements and best practices that can enhance the workflow:\n",
        "\n",
        "### 1. **Schema Embedding for Context-Awareness**\n",
        "\n",
        "Instead of just including the table structure in the prompt, you can dynamically retrieve and include more detailed schema information, such as data types, primary keys, and relationships between tables. This way, GPT has more context to generate accurate queries.\n",
        "\n",
        "#### Example Code\n",
        "\n",
        "```python\n",
        "def get_detailed_table_schema(df):\n",
        "    columns_info = \", \".join([f\"{col} ({str(df[col].dtype)})\" for col in df.columns])\n",
        "    return f\"### Table structure: Sales ({columns_info})\\n\"\n",
        "\n",
        "def create_full_prompt(df, user_query):\n",
        "    schema_info = get_detailed_table_schema(df)\n",
        "    prompt = f\"{schema_info}\\n### User Request: {user_query}\\n### SQL Query:\\nSELECT\"\n",
        "    return prompt\n",
        "```\n",
        "\n",
        "Including data types helps GPT understand columns better, such as whether `date` columns need date-based operations or numeric fields should be aggregated.\n",
        "\n",
        "### 2. **Example Queries in System Prompt for Few-Shot Learning**\n",
        "\n",
        "You can \"train\" GPT to produce better results by giving it examples of SQL queries based on similar table structures and natural language requests. This approach is called few-shot learning, and it helps the model understand what kind of SQL syntax and patterns are expected.\n",
        "\n",
        "#### Example Code\n",
        "\n",
        "```python\n",
        "example_queries = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an assistant that writes SQL queries.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Get total sales by product.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"SELECT product_id, SUM(sales) AS total_sales FROM Sales GROUP BY product_id;\"},\n",
        "    {\"role\": \"user\", \"content\": \"Calculate the average sales by city.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"SELECT city, AVG(sales) AS avg_sales FROM Sales GROUP BY city;\"}\n",
        "]\n",
        "\n",
        "def get_sql_query_from_gpt(df, user_query):\n",
        "    prompt = create_full_prompt(df, user_query)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=MODEL,\n",
        "        messages=example_queries + [{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "```\n",
        "\n",
        "This method uses example queries as part of the conversation history, guiding GPT to produce similar responses.\n",
        "\n",
        "### 3. **Prompt Refinement and Iterative Query Refinement**\n",
        "\n",
        "If the initial response from GPT is not correct, you can iteratively refine the query by analyzing the initial response and adjusting the prompt. For example, if the LLM didn’t group data correctly, you could add feedback to the prompt, such as \"Ensure the query includes a GROUP BY clause.\"\n",
        "\n",
        "### 4. **Using Specialized SQL Models**\n",
        "\n",
        "Some LLMs are fine-tuned specifically for SQL or programming tasks. Using a specialized model, such as OpenAI Codex or similar models from other platforms, can improve accuracy and reduce the need for prompt engineering.\n",
        "\n",
        "### 5. **Use Structured Data to Automate Column Extraction and Formatting**\n",
        "\n",
        "Automate the process of extracting column names and formatting them as SQL-friendly prompts. You can integrate this with SQLAlchemy or other database libraries to inspect the schema directly from a live database and format it dynamically.\n",
        "\n",
        "### 6. **Multi-Step Reasoning for Complex Queries**\n",
        "\n",
        "For more complex queries involving joins, subqueries, or conditional logic, you can use multi-step reasoning. This involves breaking down a complex natural language query into parts, generating partial SQL code, and combining the parts.\n",
        "\n",
        "For example:\n",
        "1. Generate a basic query to retrieve specific columns.\n",
        "2. Generate additional clauses (e.g., `WHERE`, `GROUP BY`).\n",
        "3. Combine each part into a final query.\n",
        "\n",
        "### Example: Putting It All Together\n",
        "\n",
        "Here’s a more comprehensive example that includes a schema extraction function, prompt engineering, and few-shot examples for a complete workflow.This setup uses few-shot learning, structured schema information, and dynamic prompt generation to maximize the accuracy of SQL code generated by the LLM."
      ],
      "metadata": {
        "id": "DtxV5WPNtbUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv openai"
      ],
      "metadata": {
        "id": "fqx6JG_vuJdp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential imports\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import pandas as pd  # For data loading and manipulation\n",
        "import openai         # For accessing the OpenAI API to generate SQL queries\n",
        "\n",
        "# Optional, for database schema inspection if connected to a database\n",
        "from sqlalchemy import create_engine, inspect  # To connect to a database and inspect schema"
      ],
      "metadata": {
        "id": "K8qRutC-uQ0P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')\n",
        "\n",
        "# Get the API key from the environment\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Print the API key to confirm it's loaded correctly\n",
        "print(f\"API Key loaded from .env: {api_key[0:40]}\")\n",
        "\n",
        "# Set the API key for the OpenAI library\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgQ_wcatt8-s",
        "outputId": "1a903a58-cddd-4206-f37b-458528aa9980"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded from .env: sk-proj-e1GUWruINPRnrozmiakkRMQEnFiEbthN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa64WtCftFdg",
        "outputId": "6140d213-ebf0-4d1a-b5aa-5fa97b7ded86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated SQL Query:\n",
            " SELECT CITY, AVG(SALES) AS avg_sales FROM Sales GROUP BY CITY\n"
          ]
        }
      ],
      "source": [
        "MODEL = 'gpt-4'  # Use a model that supports code or SQL generation effectively\n",
        "\n",
        "def get_schema_info(df):\n",
        "    columns_info = \", \".join([f\"{col} ({str(df[col].dtype)})\" for col in df.columns])\n",
        "    return f\"### Table structure: Sales ({columns_info})\\n\"\n",
        "\n",
        "def create_prompt_with_examples(df, user_query):\n",
        "    schema_info = get_schema_info(df)\n",
        "    examples = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an assistant that generates SQL queries.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Get total sales by product.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"SELECT product_id, SUM(sales) AS total_sales FROM Sales GROUP BY product_id;\"},\n",
        "        {\"role\": \"user\", \"content\": \"Calculate the average sales by city.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"SELECT city, AVG(sales) AS avg_sales FROM Sales GROUP BY city;\"}\n",
        "    ]\n",
        "    prompt = f\"{schema_info}\\n### User Request: {user_query}\\n### SQL Query:\\nSELECT\"\n",
        "    return examples + [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "def get_sql_query(df, user_query):\n",
        "    \"\"\"\n",
        "    Generates a SQL query using GPT based on the provided table structure and natural language query.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): DataFrame to define the SQL table structure.\n",
        "        user_query (str): The user's natural language query.\n",
        "\n",
        "    Returns:\n",
        "        str: The SQL query generated by GPT, ensuring it starts with 'SELECT'.\n",
        "    \"\"\"\n",
        "    messages = create_prompt_with_examples(df, user_query)\n",
        "\n",
        "    try:\n",
        "        # Send the prompt to GPT and retrieve the response\n",
        "        response = openai.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=150,\n",
        "            top_p=1.0,\n",
        "            frequency_penalty=0.0,\n",
        "            presence_penalty=0.0,\n",
        "            stop=[\"#\", \";\"]\n",
        "        )\n",
        "\n",
        "        # Retrieve and clean the generated query\n",
        "        query = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Check if 'SELECT' is at the start of the query; if not, add it\n",
        "        if not query.upper().startswith(\"SELECT\"):\n",
        "            query = \"SELECT \" + query\n",
        "\n",
        "        return query\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred while generating the SQL query:\", e)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load data and test query generation\n",
        "df = pd.read_csv(\"/content/sales_data_sample.csv\")\n",
        "query = \"Calculate the average sales by city.\"\n",
        "generated_sql = get_sql_query(df, query)\n",
        "print(\"Generated SQL Query:\\n\", generated_sql)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9r95w4Kuoc2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}