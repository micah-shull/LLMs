{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfxUXwEhWvDouYu0g+1iYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_047_huggingFace_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face **pipelines** are the perfect starting point—they're simple yet powerful. Let’s break it down in a beginner-friendly way so you understand **what they are**, **why they're useful**, **when you need them**, and **how to use them effectively**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 What Are Hugging Face Pipelines?\n",
        "\n",
        "Think of a **pipeline** as a ready-to-use, pre-built tool that wraps together:\n",
        "1. **A tokenizer** – turns text into numbers the model can understand\n",
        "2. **A pretrained model** – performs a specific task (e.g., sentiment analysis)\n",
        "3. **Post-processing** – converts model output into human-readable results\n",
        "\n",
        "Instead of setting all that up manually, pipelines let you **skip the boilerplate** and run complex models with just a few lines of code.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Are Pipelines *Necessary* to Use LLMs?\n",
        "\n",
        "No, **they're not necessary**, but they’re **incredibly helpful** when:\n",
        "- You want a **quick result** with minimal setup.\n",
        "- You’re doing a **standard task** like classification, translation, summarization, etc.\n",
        "- You're **prototyping** or just learning.\n",
        "\n",
        "If you're building **custom workflows**, **fine-tuning models**, or needing **more control**, you'll bypass pipelines and use the raw models/tokenizers instead.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ How to Use Pipelines\n",
        "\n",
        "### 1. **Basic Syntax**\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Run it on a sentence\n",
        "result = classifier(\"I love learning about Hugging Face!\")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.9998}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Popular Pipeline Tasks**\n",
        "\n",
        "| Task Name           | Pipeline Task                 | Description                        |\n",
        "|---------------------|-------------------------------|------------------------------------|\n",
        "| `\"sentiment-analysis\"` | Text classification          | Positive/Negative                   |\n",
        "| `\"text-generation\"`    | Language model completion    | Like ChatGPT-style generation      |\n",
        "| `\"summarization\"`      | Text summarization            | Shorten long documents             |\n",
        "| `\"translation_xx_to_yy\"` | Translate between languages | E.g., `\"translation_en_to_fr\"`     |\n",
        "| `\"question-answering\"` | QA on a given context         | SQuAD-style question answering     |\n",
        "| `\"zero-shot-classification\"` | Classify text into custom labels | Without training on them          |\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Example: Text Generation**\n",
        "\n",
        "```python\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "result = generator(\"Once upon a time\", max_length=30, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Example: Summarization**\n",
        "\n",
        "```python\n",
        "summarizer = pipeline(\"summarization\")\n",
        "text = \"Hugging Face provides powerful libraries for natural language processing, making state-of-the-art models accessible to everyone.\"\n",
        "summary = summarizer(text)\n",
        "print(summary[0]['summary_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Example: Customizing the Model**\n",
        "\n",
        "You can use a **specific model** from the Hugging Face Hub:\n",
        "\n",
        "```python\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "- **Pipelines** = easy, high-level API to run ML models.\n",
        "- Great for **standard NLP tasks** and **quick prototyping**.\n",
        "- You can switch to the **low-level APIs** when you need more control.\n"
      ],
      "metadata": {
        "id": "4qyKMdlueRe-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s dive deeper into each part of what a Hugging Face **pipeline** wraps for you, and why it’s so helpful—especially when you're getting started with large language models (LLMs).\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 The 3 Core Components of a Hugging Face Pipeline\n",
        "\n",
        "When you use a pipeline like this:\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I love this product!\")\n",
        "```\n",
        "\n",
        "You're *actually* doing a lot under the hood! Here's what happens:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Tokenizer – Prepares the Input**\n",
        "> 🔧 *\"Turns text into numbers the model can understand\"*\n",
        "\n",
        "- LLMs don’t understand raw text like we do. They only work with **tokens**, which are pieces of text converted into **numeric IDs**.\n",
        "- The tokenizer handles:\n",
        "  - Breaking text into subword tokens (e.g., \"playing\" → \"play\", \"##ing\")\n",
        "  - Mapping those tokens to numeric IDs\n",
        "  - Adding special tokens like `[CLS]` and `[SEP]` required by models like BERT\n",
        "  - Creating attention masks so the model knows which parts of the input are real words vs. padding\n",
        "\n",
        "Without a pipeline, you’d write something like this manually:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "inputs = tokenizer(\"I love this!\", return_tensors=\"pt\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Pretrained Model – Runs the Prediction**\n",
        "> 🧠 *\"Performs a specific task, like sentiment analysis or summarization\"*\n",
        "\n",
        "- After tokenization, the data is sent into a **deep neural network** (transformer-based model) that’s already been **trained on millions of examples**.\n",
        "- The model processes the numeric input and outputs raw **logits** or **embeddings** depending on the task.\n",
        "\n",
        "Example of manual model usage:\n",
        "```python\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "```\n",
        "\n",
        "But pipelines automatically do this part for you behind the scenes.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Post-Processing – Interprets the Output**\n",
        "> 🧾 *\"Converts raw model output into human-readable results\"*\n",
        "\n",
        "- The model returns **raw numbers**, usually in the form of logits (scores before softmax).\n",
        "- The pipeline automatically:\n",
        "  - Applies softmax (for classification tasks)\n",
        "  - Picks the label with the highest probability (e.g., `'POSITIVE'`)\n",
        "  - Returns a clean dictionary with `label` and `score`\n",
        "\n",
        "Manual example:\n",
        "```python\n",
        "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "label = torch.argmax(probs)\n",
        "```\n",
        "\n",
        "But the pipeline saves you all that and just gives you:\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.999}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Summary: Why This Matters\n",
        "\n",
        "Instead of writing **15+ lines of code** to:\n",
        "1. Tokenize your input\n",
        "2. Run inference with the model\n",
        "3. Process the output and interpret the results\n",
        "\n",
        "…you get it done in **1-2 lines** with pipelines.\n",
        "\n",
        "It’s like using a **coffee machine** instead of roasting beans, grinding them, and boiling water manually. You push a button, and get results—fast.\n",
        "\n"
      ],
      "metadata": {
        "id": "FmpoqXbjeamC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-sRtxu4eHRN"
      },
      "outputs": [],
      "source": []
    }
  ]
}