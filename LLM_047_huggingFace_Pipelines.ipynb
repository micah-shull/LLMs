{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwNf0l6gxvtKv9wK1EIMSV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_047_huggingFace_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hugging Face **pipelines** are the perfect starting point‚Äîthey're simple yet powerful. Let‚Äôs break it down in a beginner-friendly way so you understand **what they are**, **why they're useful**, **when you need them**, and **how to use them effectively**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© What Are Hugging Face Pipelines?\n",
        "\n",
        "Think of a **pipeline** as a ready-to-use, pre-built tool that wraps together:\n",
        "1. **A tokenizer** ‚Äì turns text into numbers the model can understand\n",
        "2. **A pretrained model** ‚Äì performs a specific task (e.g., sentiment analysis)\n",
        "3. **Post-processing** ‚Äì converts model output into human-readable results\n",
        "\n",
        "Instead of setting all that up manually, pipelines let you **skip the boilerplate** and run complex models with just a few lines of code.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Are Pipelines *Necessary* to Use LLMs?\n",
        "\n",
        "No, **they're not necessary**, but they‚Äôre **incredibly helpful** when:\n",
        "- You want a **quick result** with minimal setup.\n",
        "- You‚Äôre doing a **standard task** like classification, translation, summarization, etc.\n",
        "- You're **prototyping** or just learning.\n",
        "\n",
        "If you're building **custom workflows**, **fine-tuning models**, or needing **more control**, you'll bypass pipelines and use the raw models/tokenizers instead.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è How to Use Pipelines\n",
        "\n",
        "### 1. **Basic Syntax**\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Run it on a sentence\n",
        "result = classifier(\"I love learning about Hugging Face!\")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.9998}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Popular Pipeline Tasks**\n",
        "\n",
        "| Task Name           | Pipeline Task                 | Description                        |\n",
        "|---------------------|-------------------------------|------------------------------------|\n",
        "| `\"sentiment-analysis\"` | Text classification          | Positive/Negative                   |\n",
        "| `\"text-generation\"`    | Language model completion    | Like ChatGPT-style generation      |\n",
        "| `\"summarization\"`      | Text summarization            | Shorten long documents             |\n",
        "| `\"translation_xx_to_yy\"` | Translate between languages | E.g., `\"translation_en_to_fr\"`     |\n",
        "| `\"question-answering\"` | QA on a given context         | SQuAD-style question answering     |\n",
        "| `\"zero-shot-classification\"` | Classify text into custom labels | Without training on them          |\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Example: Text Generation**\n",
        "\n",
        "```python\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "result = generator(\"Once upon a time\", max_length=30, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Example: Summarization**\n",
        "\n",
        "```python\n",
        "summarizer = pipeline(\"summarization\")\n",
        "text = \"Hugging Face provides powerful libraries for natural language processing, making state-of-the-art models accessible to everyone.\"\n",
        "summary = summarizer(text)\n",
        "print(summary[0]['summary_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Example: Customizing the Model**\n",
        "\n",
        "You can use a **specific model** from the Hugging Face Hub:\n",
        "\n",
        "```python\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "- **Pipelines** = easy, high-level API to run ML models.\n",
        "- Great for **standard NLP tasks** and **quick prototyping**.\n",
        "- You can switch to the **low-level APIs** when you need more control.\n"
      ],
      "metadata": {
        "id": "4qyKMdlueRe-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-sRtxu4eHRN"
      },
      "outputs": [],
      "source": []
    }
  ]
}