{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_007_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
      "metadata": {
        "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2"
      },
      "source": [
        "# Conversational AI - aka Chatbot!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "xYtTpEjjA8hD"
      },
      "id": "xYtTpEjjA8hD"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "# !pip install google-generativeai\n",
        "# !pip install anthropic\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "ifw0J2E4A2Ex"
      },
      "id": "ifw0J2E4A2Ex",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "m7TZSjRQA517"
      },
      "id": "m7TZSjRQA517"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
      "metadata": {
        "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write API Keys to .env file"
      ],
      "metadata": {
        "id": "EHhq-AQdBDAI"
      },
      "id": "EHhq-AQdBDAI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the .env file\n",
        "env_file_path = '/content/API_KEYS.env' # naming file makes it visible, no name makes it hidden\n",
        "\n",
        "# Your OpenAI API key\n",
        "OPEN_API_KEY = \"sk-proj-e1GUWruINPRnro###\"\n",
        "ANTHROPIC_API_KEY = \"sk-ant-api03-NX48###\"\n",
        "GOOGLE_API_KEY = \"AIzaS###\"\n",
        "\n",
        "# Create the .env file and write the API keys\n",
        "with open(env_file_path, 'w') as f:\n",
        "    f.write(f\"OPENAI_API_KEY={OPEN_API_KEY}\\n\")\n",
        "    f.write(f\"ANTHROPIC_API_KEY={ANTHROPIC_API_KEY}\\n\")\n",
        "    f.write(f\"GOOGLE_API_KEY={GOOGLE_API_KEY}\\n\")\n",
        "\n",
        "print(f\".env file created at: {env_file_path}\")\n",
        "# List all files (including hidden ones) in the /content/ folder\n",
        "!ls -la /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdyTxGfrAuAL",
        "outputId": "774aa2e1-e2a7-41d0-c9d5-02e9ad646c91"
      },
      "id": "DdyTxGfrAuAL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".env file created at: /content/API_KEYS.env\n",
            "total 20\n",
            "drwxr-xr-x 1 root root 4096 Oct 22 21:35 .\n",
            "drwxr-xr-x 1 root root 4096 Oct 22 21:29 ..\n",
            "-rw-r--r-- 1 root root  362 Oct 22 21:35 API_KEYS.env\n",
            "drwxr-xr-x 4 root root 4096 Oct 21 13:22 .config\n",
            "drwxr-xr-x 1 root root 4096 Oct 21 13:22 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "33QZ6MgqBGbd"
      },
      "id": "33QZ6MgqBGbd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')  # Ensure this is the correct path to your file\n",
        "\n",
        "# Get the API keys from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Check if the keys are loaded correctly and print a portion of them\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key loaded: {openai_api_key[0:10]}...\")  # Only print part of the key\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded correctly.\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key loaded: {anthropic_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Anthropic API key not loaded correctly.\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key loaded: {google_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Google API key not loaded correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVd-S6DdAyNK",
        "outputId": "ef19cb8f-00ee-4a5d-b501-977bbedac52f"
      },
      "id": "nVd-S6DdAyNK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded: sk-proj-e1...\n",
            "Anthropic API Key loaded: sk-ant-api...\n",
            "Google API Key loaded: AIzaSyDh3a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai\n",
        "\n",
        "# Connect to OpenAI\n",
        "openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "\n",
        "# Connect to Anthropic (Claude)\n",
        "claude = anthropic.Anthropic(api_key=anthropic_api_key)  # Set Anthropic API key\n",
        "\n",
        "# Connect to Google Generative AI\n",
        "google.generativeai.configure(api_key=google_api_key)  # Set Google API key"
      ],
      "metadata": {
        "id": "_tvSBQa3BSGB"
      },
      "id": "_tvSBQa3BSGB",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
      "metadata": {
        "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7"
      },
      "source": [
        "## Message Structure for OpenAI\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
        "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
        "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
        "]\n",
        "```\n",
        "\n",
        "We will write a function `chat(message, history)` where:\n",
        "**message** is the prompt to use\n",
        "**history** is a list of pairs of user message with assistant's reply\n",
        "\n",
        "```\n",
        "[\n",
        "    [\"user said this\", \"assistant replied\"],\n",
        "    [\"then user said this\", \"and assistant replied again],\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "### Major Concepts You Should Be Learning:\n",
        "\n",
        "1. **Conversation History in ChatGPT**:\n",
        "   - **How history is handled**: The `history` parameter contains the conversation history between the user and the assistant. This history is crucial because ChatGPT doesn't maintain state across calls. So, the full conversation context needs to be passed each time.\n",
        "   - **Rebuilding the conversation**: The `for` loop iterates over the history, adding past user messages (`\"role\": \"user\"`) and assistant responses (`\"role\": \"assistant\"`) to the `messages` list. This maintains the continuity of the conversation by passing the entire message history to the model in each API call.\n",
        "\n",
        "2. **System and User Messages**:\n",
        "   - **System message**: As before, the system message sets the tone for the assistant's behavior. It’s added first in the `messages` list.\n",
        "   - **User messages**: The current user message (along with the conversation history) is added to the `messages` list. This helps provide context to the model about the ongoing interaction.\n",
        "\n",
        "3. **Streaming Responses**:\n",
        "   - **Streamed output**: The `stream=True` parameter enables response streaming. Instead of waiting for the entire response, you get real-time chunks of the model's output.\n",
        "   - **Yielding the response**: The response is constructed incrementally by appending each chunk of text to `response` and then yielding the partial response. This allows you to update the UI (or any interface) in real time as the model generates the response.\n",
        "\n",
        "4. **Handling Role-based Messages**:\n",
        "   - **Assistant and user roles**: The API expects messages to have a role. This function adds `\"role\": \"user\"` for user inputs and `\"role\": \"assistant\"` for the model's previous outputs. By doing this, the API understands the flow of conversation and can generate an appropriate response based on both the assistant's past replies and the user’s queries.\n",
        "\n",
        "### Key Differences from Previous Code:\n",
        "- **Conversation History**: This function introduces conversation history, which is a key feature in creating more dynamic and context-aware interactions with the model. The history is structured in pairs (user message, assistant message) and is passed along to give context to the assistant for every new input.\n",
        "- **Role Assignment**: In previous examples, we only passed the system and user messages. Here, the function handles both user and assistant roles, ensuring the model can respond in context to previous interactions.\n",
        "\n",
        "### What You Should Focus On:\n",
        "\n",
        "1. **Maintaining Conversation State**:\n",
        "   - You need to understand how the conversation history is passed in and rebuilt for every new request. This is important because ChatGPT doesn't remember conversations from previous API calls unless you include the history explicitly.\n",
        "   \n",
        "2. **Streaming with Real-time Updates**:\n",
        "   - Streaming is used here to yield partial responses from the model as they are generated. This is useful for creating a more responsive and dynamic user experience, especially for longer outputs.\n",
        "\n",
        "3. **Role-based Messages**:\n",
        "   - The model needs to know who is saying what. The `\"role\"` field ensures the model can differentiate between the user’s inputs and the assistant’s responses, which is critical for maintaining the conversational flow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Function"
      ],
      "metadata": {
        "id": "7Dc897GiKHAZ"
      },
      "id": "7Dc897GiKHAZ"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
        "outputId": "f3a6b2ef-d149-4f85-8be7-7490497013ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a58f7531221647e7a9.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a58f7531221647e7a9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "MODEL = 'gpt-4o-mini'\n",
        "\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    print(\"History is:\")\n",
        "    print(history)\n",
        "    print(\"And messages is:\")\n",
        "    print(messages)\n",
        "\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "# And then enter Gradio's magic!\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### History Dictionary vs List\n",
        "\n",
        "The deprecation warning you're seeing indicates that Gradio has updated the way it handles messages in its chatbot interface. Specifically, Gradio no longer supports passing conversation history as tuples (like `(\"user message\", \"assistant message\")`). Instead, it now expects messages in a format similar to OpenAI's API: a list of dictionaries with `\"role\"` and `\"content\"` keys.\n",
        "\n",
        "### Key Action:\n",
        "\n",
        "Make sure the history is structured as a list of dictionaries, each with `\"role\"` and `\"content\"`. Gradio expects this format going forward.\n",
        "\n",
        "\n",
        "### Changes:\n",
        "1. **History Format**: Ensure `history` is a list of dictionaries, where each message has the `\"role\"` (either `\"user\"` or `\"assistant\"`) and `\"content\"` (the actual message). This aligns with both OpenAI's API and Gradio's new expected message format.\n",
        "2. **No Tuples**: Avoid using tuples like `(\"user message\", \"assistant message\")`. Instead, each message should be structured with the keys `\"role\"` and `\"content\"`, as in `{\"role\": \"user\", \"content\": user_message}`.\n",
        "\n",
        "### Example of History Format:\n",
        "Here’s an example of how the conversation history should now look:\n",
        "```python\n",
        "history = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"I'm doing great! How can I assist you today?\"},\n",
        "]\n",
        "```\n",
        "\n",
        "The main difference in the code is how the **conversation history** is structured and processed. In the previous version of the code, the **history** was expected to be a list of tuples (e.g., `(\"user message\", \"assistant message\")`), whereas now Gradio expects the history to be passed as a list of dictionaries with `role` and `content` keys, matching OpenAI’s message structure.\n",
        "\n",
        "### Previous Code (Using Tuples for History):\n",
        "\n",
        "```python\n",
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        # Adding tuples directly, user-assistant pairs\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "    # Streaming response logic...\n",
        "```\n",
        "\n",
        "### Updated Code (Using `role` and `content` Keys in History):\n",
        "\n",
        "```python\n",
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for msg in history:\n",
        "        # Now history is a list of dictionaries with role and content\n",
        "        messages.append(msg)\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "    # Streaming response logic...\n",
        "```\n",
        "\n",
        "### Key Code Difference:\n",
        "- **Previous Code**: The history was processed as **tuples**: `(user_message, assistant_message)`, requiring two `append()` calls (one for the user and one for the assistant).\n",
        "- **Updated Code**: The history is now processed as a **list of dictionaries** with `role` and `content` fields, so each dictionary is directly appended to the `messages` list in one go.\n",
        "\n"
      ],
      "metadata": {
        "id": "f6UVCfxyFB6q"
      },
      "id": "f6UVCfxyFB6q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Function with Shopping Assistant"
      ],
      "metadata": {
        "id": "3JYKjXmRNf_Q"
      },
      "id": "3JYKjXmRNf_Q"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
        "outputId": "b2b781d8-931b-4465-c87e-d477d9344ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5388d39abcbe39c7fb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5388d39abcbe39c7fb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding user and assistant messages using OpenAI-style format\n",
        "    for msg in history:\n",
        "        # Expecting history in OpenAI-style with role and content\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    print(\"History is:\")\n",
        "    print(history)\n",
        "    print(\"And messages is:\")\n",
        "    print(messages)\n",
        "\n",
        "    # Send the message to the OpenAI API with streaming\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "# More detailed system message for the assistant behavior\n",
        "system_message = (\n",
        "    \"You are a helpful assistant in a clothing store. Your goal is to gently encourage \"\n",
        "    \"the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \"\n",
        "    \"For example, if the customer says, 'I'm looking to buy a hat,' you could reply with something like, \"\n",
        "    \"'Wonderful - we have lots of hats, including several that are part of our sales event.' \"\n",
        "    \"Encourage the customer to buy hats if they are unsure what to get.\"\n",
        ")\n",
        "\n",
        "# Specify type='messages' to use OpenAI-style role/content format\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
      "metadata": {
        "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternate System Role"
      ],
      "metadata": {
        "id": "wn8LgkJFOkM9"
      },
      "id": "wn8LgkJFOkM9"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d75f0ffa-55c8-4152-b451-945021676837",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "d75f0ffa-55c8-4152-b451-945021676837",
        "outputId": "adc37879-c5d9-44c5-85e9-b01d07a0583d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://631ec93f56457e6092.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://631ec93f56457e6092.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
        "but remind the customer to look at hats!\"\n",
        "\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
      "metadata": {
        "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Context"
      ],
      "metadata": {
        "id": "zOVnBSASPTLc"
      },
      "id": "zOVnBSASPTLc"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
        "outputId": "fef89492-90e0-4713-fd6f-6cc525c8b51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e82b3990fef5554d51.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e82b3990fef5554d51.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "    if 'belt' in message: # add context about belts\n",
        "        messages.append({\"role\": \"system\", \"content\": \"For added context, the store does not sell belts, \\\n",
        "                        but be sure to point out other items on sale\"})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
      "metadata": {
        "id": "20570de2-eaad-42cc-a92c-c779d71b48b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035",
      "metadata": {
        "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}