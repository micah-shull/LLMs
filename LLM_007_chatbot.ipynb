{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_007_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
      "metadata": {
        "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2"
      },
      "source": [
        "# Conversational AI - aka Chatbot!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Libraries"
      ],
      "metadata": {
        "id": "xYtTpEjjA8hD"
      },
      "id": "xYtTpEjjA8hD"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install openai\n",
        "# !pip install google-generativeai\n",
        "# !pip install anthropic\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "ifw0J2E4A2Ex"
      },
      "id": "ifw0J2E4A2Ex",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "m7TZSjRQA517"
      },
      "id": "m7TZSjRQA517"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
      "metadata": {
        "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Environment Variables"
      ],
      "metadata": {
        "id": "33QZ6MgqBGbd"
      },
      "id": "33QZ6MgqBGbd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables from the .env file\n",
        "load_dotenv('/content/API_KEYS.env')  # Ensure this is the correct path to your file\n",
        "\n",
        "# Get the API keys from the environment\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
        "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Check if the keys are loaded correctly and print a portion of them\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key loaded: {openai_api_key[0:10]}...\")  # Only print part of the key\n",
        "else:\n",
        "    print(\"OpenAI API key not loaded correctly.\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key loaded: {anthropic_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Anthropic API key not loaded correctly.\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key loaded: {google_api_key[0:10]}...\")\n",
        "else:\n",
        "    print(\"Google API key not loaded correctly.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVd-S6DdAyNK",
        "outputId": "932763a2-e2b9-4736-e5f1-6e771dc40f5c"
      },
      "id": "nVd-S6DdAyNK",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API Key loaded: sk-proj-e1...\n",
            "Anthropic API Key loaded: sk-ant-api...\n",
            "Google API Key loaded: AIzaSyDh3a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import anthropic\n",
        "import google.generativeai\n",
        "\n",
        "# Connect to OpenAI\n",
        "openai.api_key = openai_api_key  # Set OpenAI API key\n",
        "\n",
        "# Connect to Anthropic (Claude)\n",
        "claude = anthropic.Anthropic(api_key=anthropic_api_key)  # Set Anthropic API key\n",
        "\n",
        "# Connect to Google Generative AI\n",
        "google.generativeai.configure(api_key=google_api_key)  # Set Google API key"
      ],
      "metadata": {
        "id": "_tvSBQa3BSGB"
      },
      "id": "_tvSBQa3BSGB",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
      "metadata": {
        "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7"
      },
      "source": [
        "## Message Structure and History\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
        "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
        "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
        "]\n",
        "```\n",
        "\n",
        "We will write a function `chat(message, history)` where:\n",
        "**message** is the prompt to use\n",
        "**history** is a list of pairs of user message with assistant's reply\n",
        "\n",
        "```\n",
        "[\n",
        "    [\"user said this\", \"assistant replied\"],\n",
        "    [\"then user said this\", \"and assistant replied again],\n",
        "    ...\n",
        "]\n",
        "```\n",
        "\n",
        "### Major Concepts You Should Be Learning:\n",
        "\n",
        "1. **Conversation History in ChatGPT**:\n",
        "   - **How history is handled**: The `history` parameter contains the conversation history between the user and the assistant. This history is crucial because ChatGPT doesn't maintain state across calls. So, the full conversation context needs to be passed each time.\n",
        "   - **Rebuilding the conversation**: The `for` loop iterates over the history, adding past user messages (`\"role\": \"user\"`) and assistant responses (`\"role\": \"assistant\"`) to the `messages` list. This maintains the continuity of the conversation by passing the entire message history to the model in each API call.\n",
        "\n",
        "2. **System and User Messages**:\n",
        "   - **System message**: As before, the system message sets the tone for the assistant's behavior. It’s added first in the `messages` list.\n",
        "   - **User messages**: The current user message (along with the conversation history) is added to the `messages` list. This helps provide context to the model about the ongoing interaction.\n",
        "\n",
        "3. **Streaming Responses**:\n",
        "   - **Streamed output**: The `stream=True` parameter enables response streaming. Instead of waiting for the entire response, you get real-time chunks of the model's output.\n",
        "   - **Yielding the response**: The response is constructed incrementally by appending each chunk of text to `response` and then yielding the partial response. This allows you to update the UI (or any interface) in real time as the model generates the response.\n",
        "\n",
        "4. **Handling Role-based Messages**:\n",
        "   - **Assistant and user roles**: The API expects messages to have a role. This function adds `\"role\": \"user\"` for user inputs and `\"role\": \"assistant\"` for the model's previous outputs. By doing this, the API understands the flow of conversation and can generate an appropriate response based on both the assistant's past replies and the user’s queries.\n",
        "\n",
        "### Key Differences from Previous Code:\n",
        "- **Conversation History**: This function introduces conversation history, which is a key feature in creating more dynamic and context-aware interactions with the model. The history is structured in pairs (user message, assistant message) and is passed along to give context to the assistant for every new input.\n",
        "- **Role Assignment**: In previous examples, we only passed the system and user messages. Here, the function handles both user and assistant roles, ensuring the model can respond in context to previous interactions.\n",
        "\n",
        "### What You Should Focus On:\n",
        "\n",
        "1. **Maintaining Conversation State**:\n",
        "   - You need to understand how the conversation history is passed in and rebuilt for every new request. This is important because ChatGPT doesn't remember conversations from previous API calls unless you include the history explicitly.\n",
        "   \n",
        "2. **Streaming with Real-time Updates**:\n",
        "   - Streaming is used here to yield partial responses from the model as they are generated. This is useful for creating a more responsive and dynamic user experience, especially for longer outputs.\n",
        "\n",
        "3. **Role-based Messages**:\n",
        "   - The model needs to know who is saying what. The `\"role\"` field ensures the model can differentiate between the user’s inputs and the assistant’s responses, which is critical for maintaining the conversational flow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Function with Chat History\n",
        "\n",
        "### Example of `history` and `messages`\n",
        "\n",
        "Suppose the user and assistant have had the following conversation:\n",
        "1. **User**: \"Hello, can you help me with my homework?\"\n",
        "2. **Assistant**: \"Of course! What subject are you working on?\"\n",
        "3. **User**: \"Math. I'm stuck on algebra.\"\n",
        "4. **Assistant**: \"Got it. Let's work on it together.\"\n",
        "\n",
        "The `history` would look like this:\n",
        "```python\n",
        "history = [\n",
        "    (\"Hello, can you help me with my homework?\", \"Of course! What subject are you working on?\"),\n",
        "    (\"Math. I'm stuck on algebra.\", \"Got it. Let's work on it together.\")\n",
        "]\n",
        "```\n",
        "\n",
        "#### Resulting `messages`:\n",
        "If the current user message is `\"How do I solve quadratic equations?\"`, then `messages` will be:\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Hello, can you help me with my homework?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Of course! What subject are you working on?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Math. I'm stuck on algebra.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Got it. Let's work on it together.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I solve quadratic equations?\"}\n",
        "]\n",
        "```\n",
        "\n",
        "### Why Learning How Chat History Works Is Important\n",
        "\n",
        "1. **Maintaining Context**:\n",
        "   - Chat history allows the assistant to maintain context across multiple user interactions.\n",
        "   - Without this, the model would treat each new user message as a standalone request, losing continuity and providing responses that may not make sense in an ongoing conversation.\n",
        "\n",
        "2. **Creating Coherent Conversations**:\n",
        "   - By appending previous user and assistant messages, the assistant can refer back to past messages, making the conversation feel more natural and engaging.\n",
        "   - It allows for more meaningful responses, as the assistant knows what has already been discussed.\n",
        "\n",
        "3. **Personalization**:\n",
        "   - If the assistant can remember and refer to earlier parts of a conversation, it can provide more personalized assistance.\n",
        "   - For example, remembering that a user was working on algebra allows the assistant to continue helping without re-explaining everything.\n",
        "\n",
        "4. **Debugging and Improving Conversations**:\n",
        "   - Understanding how history is built and used can help debug issues where the assistant forgets context or provides irrelevant responses.\n",
        "   - It can also help improve prompt engineering to ensure the assistant has the best possible understanding of the entire conversation.\n",
        "\n",
        "In summary, the **chat history** feature is fundamental to building more interactive and engaging AI-driven conversations. Learning how to maintain, construct, and pass this history to the LLM is key to creating chatbots that deliver coherent and context-aware responses, making the interaction more useful for users."
      ],
      "metadata": {
        "id": "7Dc897GiKHAZ"
      },
      "id": "7Dc897GiKHAZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# works"
      ],
      "metadata": {
        "id": "YOzoe7OYeJUN"
      },
      "id": "YOzoe7OYeJUN"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding user and assistant messages using OpenAI-style format\n",
        "    for msg in history:\n",
        "        # Expecting history in OpenAI-style with role and content\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Send the message to the OpenAI API with streaming\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "# Specify type='messages' to use OpenAI-style role/content format\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "a2ZV9a9uVS9c",
        "outputId": "53597f76-f2b7-49d2-a835-bb8ef2954030"
      },
      "id": "a2ZV9a9uVS9c",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4d1f68fbd0a108375e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4d1f68fbd0a108375e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Chatbot\n"
      ],
      "metadata": {
        "id": "fER20U0peGpZ"
      },
      "id": "fER20U0peGpZ"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import gradio as gr\n",
        "import json\n",
        "import os\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful assistant\"\n",
        "history_file = \"chat_history.json\"\n",
        "\n",
        "# Function to save chat history to a JSON file\n",
        "def save_chat_history(history, file_path):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(history, file, indent=4)\n",
        "\n",
        "# Modified chat function\n",
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding user and assistant messages using OpenAI-style format\n",
        "    for msg in history:\n",
        "        # Expecting history in OpenAI-style with role and content\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Send the message to the OpenAI API with streaming\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "    # Add the current user message and assistant response to the history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Save updated history to a JSON file\n",
        "    save_chat_history(history, history_file)\n",
        "\n",
        "# Specify type='messages' to use OpenAI-style role/content format\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "mfXIwQ2lejbi",
        "outputId": "f3172493-77ba-4200-ae07-15e19398bb81"
      },
      "id": "mfXIwQ2lejbi",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7d9e01f3ef67027116.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7d9e01f3ef67027116.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Chat History"
      ],
      "metadata": {
        "id": "O-c6PCD8f3_k"
      },
      "id": "O-c6PCD8f3_k"
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "def print_chat_history(file_path):\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, \"r\") as file:\n",
        "            history = json.load(file)\n",
        "\n",
        "        print(\"\\nChat History:\\n\" + \"=\" * 50)\n",
        "        turn = 1\n",
        "        for msg in history:\n",
        "            if msg[\"role\"] == \"user\":\n",
        "                # print(f\"Turn {turn}:\")\n",
        "                print(f\"User: {msg['content']}\")\n",
        "            elif msg[\"role\"] == \"assistant\":\n",
        "                print(f\"Assistant: {msg['content']}\")\n",
        "                print(\"-\" * 50)\n",
        "                turn += 1\n",
        "    else:\n",
        "        print(\"No chat history found.\")\n",
        "\n",
        "print_chat_history(\"/content/chat_history.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XppcLPjMfeWT",
        "outputId": "dc8a2728-f935-4e49-a92b-daadbfbf6331"
      },
      "id": "XppcLPjMfeWT",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat History:\n",
            "==================================================\n",
            "User: hello there\n",
            "Assistant: Hello! How can I assist you today?\n",
            "--------------------------------------------------\n",
            "User: what is the world largest mammal?\n",
            "Assistant: The world’s largest mammal is the blue whale (*Balaenoptera musculus*). Blue whales can reach lengths of up to 100 feet (30 meters) and can weigh as much as 200 tons (approximately 181 metric tonnes) or more. They are found in oceans around the world and are known for their distinct blue-gray coloration and incredible size.\n",
            "--------------------------------------------------\n",
            "User: What is the world longest river?\n",
            "Assistant: The title of the world’s longest river is subject to some debate, primarily between the Nile River and the Amazon River. \n",
            "\n",
            "1. **Nile River**: Traditionally, the Nile has been considered the longest river in the world, measuring about 6,650 kilometers (4,130 miles) in length. It flows north through northeastern Africa and drains into the Mediterranean Sea.\n",
            "\n",
            "2. **Amazon River**: Recent studies suggest that the Amazon River may actually be longer, measuring approximately 7,000 kilometers (4,345 miles), although measurements can vary based on the selected source or the definition of the river's length. The Amazon flows through South America, primarily in Brazil, and drains into the Atlantic Ocean.\n",
            "\n",
            "While both rivers are incredibly significant and long, the answer may depend on the criteria and methodology used to measure them.\n",
            "--------------------------------------------------\n",
            "User: When was the peak of the Roman Empire?\n",
            "Assistant: The peak of the Roman Empire is often considered to have occurred during the 2nd century AD, particularly around the year 117 AD. This period is known as the Pax Romana, or \"Roman Peace,\" which lasted for about two centuries from 27 BC to around 180 AD. \n",
            "\n",
            "During the reign of Emperor Trajan, who ruled from 98 to 117 AD, the Roman Empire reached its greatest territorial extent. Trajan's conquests expanded the empire to its furthest boundaries, including territories in Europe, North Africa, and parts of the Middle East. \n",
            "\n",
            "After this period, the empire began to experience various challenges, including political instability, economic troubles, and external invasions, leading to a gradual decline that culminated in the eventual fall of the Western Roman Empire in 476 AD.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code provides a good example of how to use OpenAI's chatbot API with Gradio, illustrating how to manage **conversation history** and **message formatting** effectively for an interactive chat experience. Let’s break down the **message structure** and **history management**, focusing on key takeaways for understanding how to work with chatbots.\n",
        "\n",
        "### Key Takeaways for Learning Chatbots with OpenAI:\n",
        "\n",
        "1. **Message and History Structure**:\n",
        "   - **Message Format**:\n",
        "     - The messages used in the chat follow OpenAI's required format for conversations, where each message is a dictionary containing:\n",
        "       - `\"role\"`: This specifies the **role** of the message sender, which can be `\"system\"`, `\"user\"`, or `\"assistant\"`.\n",
        "       - `\"content\"`: This is the **content** of the message.\n",
        "     - For example:\n",
        "       ```python\n",
        "       {\"role\": \"user\", \"content\": \"Hello, what can you do?\"}\n",
        "       {\"role\": \"assistant\", \"content\": \"I can help you with answering questions, giving advice, etc.\"}\n",
        "       ```\n",
        "     - The `\"role\"` and `\"content\"` structure is crucial because it helps the model understand **who said what** during the conversation.\n",
        "  \n",
        "   - **System Message**:\n",
        "     - A **system message** is added at the start of the conversation to establish the context, tone, or behavior of the assistant. In this case:\n",
        "       ```python\n",
        "       {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}\n",
        "       ```\n",
        "     - The system message is like a guide that sets the assistant's behavior throughout the conversation.\n",
        "\n",
        "2. **Maintaining Conversation Context**:\n",
        "   - **Iterating Through the History**:\n",
        "     - The `history` parameter holds the conversation history as a **list of dictionaries.**\n",
        "     - The loop `for msg in history:` is used to add each previous message (both user and assistant) to the `messages` list.\n",
        "     - This maintains the full context of the conversation, which is crucial for OpenAI's model to generate relevant responses.\n",
        "     - History format:\n",
        "       ```python\n",
        "       [\n",
        "           {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "           {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
        "           ...\n",
        "       ]\n",
        "       ```\n",
        "     - Keeping track of the entire conversation allows the model to generate coherent and contextually aware responses. Without context, the assistant might not understand the relevance of the user's questions.\n",
        "\n",
        "3. **Adding User Messages and Generating Responses**:\n",
        "   - **Adding the Latest User Message**:\n",
        "     - After the `messages` list has been populated with all prior conversation history, the latest user message is added:\n",
        "       ```python\n",
        "       messages.append({\"role\": \"user\", \"content\": message})\n",
        "       ```\n",
        "     - This makes sure that the assistant has the latest user input to respond to.\n",
        "\n",
        "   - **Generating a Response**:\n",
        "     - The `messages` list is then sent to the OpenAI API:\n",
        "       ```python\n",
        "       stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "       ```\n",
        "     - The `stream=True` option is used to generate responses incrementally, which can make the interaction feel faster and more conversational.\n",
        "\n",
        "  \n",
        "4. **Updating History**:\n",
        "   - **Appending the Conversation to History**:\n",
        "     - Once a response has been generated, the user message and assistant response are added to `history`:\n",
        "       ```python\n",
        "       history.append({\"role\": \"user\", \"content\": message})\n",
        "       history.append({\"role\": \"assistant\", \"content\": response})\n",
        "       ```\n",
        "     - This allows the conversation to be preserved and used for the next turn, providing context for further interactions.\n",
        "\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Message Format Is Crucial**:\n",
        "   - Use OpenAI’s format for messages: dictionaries with `\"role\"` and `\"content\"` as a **list of dictionaries.**\n",
        "   - Always include a **system message** to set the assistant's behavior, and maintain user and assistant messages for context.\n",
        "\n",
        "2. **Maintaining Context with History**:\n",
        "   - Maintaining a complete conversation history helps the model understand the context and respond appropriately.\n",
        "   - The `history` parameter is passed by Gradio and is updated after every turn to ensure the conversation continues logically.\n",
        "\n",
        "3. **Gradio Simplifies Chat UI**:\n",
        "   - **Gradio** provides an easy way to create an interactive user interface, handling the complexities of UI development.\n",
        "   - By using `type=\"messages\"`, Gradio maintains the expected format and state of the conversation history, which makes integration with OpenAI seamless.\n"
      ],
      "metadata": {
        "id": "9F9KldqKhI9i"
      },
      "id": "9F9KldqKhI9i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code that does not work"
      ],
      "metadata": {
        "id": "aTsFGRDniVNE"
      },
      "id": "aTsFGRDniVNE"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding previous user and assistant messages\n",
        "    if history is None:\n",
        "        history = []\n",
        "    for msg in history:\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Generate response from the OpenAI API\n",
        "    response = openai.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=messages\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # Add the current user message and assistant response to the history\n",
        "    history.append({\"role\": \"user\", \"content\": message})\n",
        "    history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    # Return the response and the updated history\n",
        "    return response, history\n",
        "\n",
        "# Launch the Gradio interface\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "3jVTp0ozWHRn",
        "outputId": "cb96b1d5-1c9e-4435-96d1-f3b6ffae1699"
      },
      "id": "3jVTp0ozWHRn",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bc9df88cd20f010f65.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bc9df88cd20f010f65.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### History is Maintained by Gradio\n",
        "\n",
        "The key differences between the **working code** and the **non-working code** that are causing the issue are mainly related to how the **history** is initialized and managed, as well as **consistency** in how the `history` list is updated and used across different interactions.\n",
        "\n",
        "Let me explain these differences step-by-step:\n",
        "\n",
        "### 1. **History Initialization (`if history is None:`)**\n",
        "- **Non-working code**:\n",
        "  ```python\n",
        "  if history is None:\n",
        "      history = []\n",
        "  ```\n",
        "  - This conditional check attempts to initialize the `history` to an empty list if it is `None`. This can lead to issues if `history` is managed improperly across different function calls.\n",
        "  - The **Gradio state** should be handled automatically between calls, and trying to manually initialize `history` can create inconsistencies.\n",
        "  - **Potential Issue**: If `history` is incorrectly reset to an empty list or improperly passed, it might not retain the previous conversation, causing the assistant to lose context.\n",
        "\n",
        "- **Working code**:\n",
        "  - In the working version, `history` is assumed to be correctly passed by Gradio each time, and there’s no manual initialization of `history` to an empty list.\n",
        "  - Gradio manages the state and handles `history` automatically, so manually setting it can lead to unexpected behavior.\n",
        "\n",
        "### 2. **History Structure Consistency**\n",
        "- **Non-working code**:\n",
        "  ```python\n",
        "  # Add the current user message and assistant response to the history\n",
        "  history.append({\"role\": \"user\", \"content\": message})\n",
        "  history.append({\"role\": \"assistant\", \"content\": response})\n",
        "  ```\n",
        "  - The **history** in this version is updated manually by appending the new user message and assistant response.\n",
        "  - If there is an inconsistency in the data structure or if `history` is modified in an unexpected way, this can lead to errors when trying to access or use `history` in subsequent function calls.\n",
        "  - **Key Problem**: If `history` is not passed correctly or if its format changes between function calls, it can result in the error you’re seeing.\n",
        "\n",
        "- **Working code**:\n",
        "  - In the working code, there is no explicit initialization of `history` and no direct check for `None`.\n",
        "  - The `history` is directly iterated over, and every message is appended to the `messages` list in the correct format, ensuring consistency.\n",
        "\n",
        "### 3. **Role of Gradio State Management (`type=\"messages\"`)**\n",
        "- When using `gr.ChatInterface(fn=chat, type=\"messages\")`, Gradio expects `history` to be passed as a list of dictionaries, where each dictionary contains `\"role\"` and `\"content\"`.\n",
        "- In the **working version**, the code directly uses `history` as it is provided by Gradio, assuming it is in the correct format (`list of dictionaries with \"role\" and \"content\"`).\n",
        "- In the **non-working version**, manually manipulating `history` (`if history is None: history = []`) might be leading to inconsistencies in how Gradio expects the data.\n",
        "\n",
        "### Summary of Key Differences:\n",
        "\n",
        "1. **Manual Initialization of `history`**:\n",
        "   - In the non-working version, `history` is manually initialized (`if history is None: history = []`).\n",
        "   - This can lead to losing previous messages or having an empty history, disrupting the conversation flow.\n",
        "\n",
        "2. **State Handling by Gradio**:\n",
        "   - In the working version, Gradio manages the state automatically and keeps `history` in the proper format.\n",
        "   - By avoiding manual state initialization, the working version ensures the correct state is used across each interaction.\n",
        "\n",
        "3. **Consistency in Message Format**:\n",
        "   - **Working Version**: Simply iterates through `history` as-is, adding each entry to the `messages` list, keeping the format consistent throughout.\n",
        "   - **Non-working Version**: Attempts to initialize `history` and then modify it manually, which can lead to inconsistencies in the format expected by Gradio or OpenAI.\n",
        "\n",
        "### Key Takeaway:\n",
        "- Let **Gradio** handle the **conversation state** (`history`). It automatically provides the history to the `chat` function, and this history is correctly formatted (`list of dictionaries`).\n",
        "- Avoid manually initializing or changing `history` within the `chat` function, as it can lead to discrepancies between how Gradio manages the state and how your function modifies it.\n",
        "- Always maintain a consistent format for `history` to ensure smooth conversation flow. Gradio will pass `history` in the required format, and as long as you append new entries using the same format (`{\"role\": \"user\" or \"assistant\", \"content\": \"...\"}`), it will work properly.\n",
        "\n",
        "The working code keeps everything straightforward and consistent, whereas the non-working code introduces manual steps that can disrupt Gradio's internal state handling and lead to errors."
      ],
      "metadata": {
        "id": "ydrYVgsEiKV0"
      },
      "id": "ydrYVgsEiKV0"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UuG5V2u4WHNH"
      },
      "id": "UuG5V2u4WHNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYOXGaMPWHKQ"
      },
      "id": "yYOXGaMPWHKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kh6G3iVYWHH6"
      },
      "id": "Kh6G3iVYWHH6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxVHZqRjWHFl"
      },
      "id": "YxVHZqRjWHFl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0jYpGbHZWHBM"
      },
      "id": "0jYpGbHZWHBM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `history` variable is not explicitly instantiated within the function or in the script itself. This can lead to confusion as it seems like `history` appears out of nowhere. Let me explain what's happening and why the `history` variable is functional in the context of this code.\n",
        "\n",
        "### Where `history` Comes From\n",
        "- The **`history`** parameter in the `chat` function is provided by **Gradio** when it calls the function.\n",
        "- **Gradio** keeps track of the conversation's history and passes it to the function as an argument during each user interaction.\n",
        "\n",
        "### Explanation:\n",
        "1. **Gradio's Handling of Chat History**:\n",
        "   - **Gradio** is the one managing the chat history.\n",
        "   - When you use `gr.ChatInterface(fn=chat, type=\"messages\")`, Gradio provides the `history` to the `chat()` function automatically.\n",
        "   - The `history` parameter in this context is maintained by Gradio across multiple interactions between the user and the chatbot.\n",
        "\n",
        "2. **How `history` Is Populated**:\n",
        "   - In the first interaction, **`history`** will be an empty list because there is no prior conversation.\n",
        "   - After the first user message, the response is generated by the model and appended to the `history` list.\n",
        "   - This updated history is passed back to Gradio, which in turn passes it to the function on subsequent interactions.\n",
        "\n",
        "3. **How Gradio Manages It**:\n",
        "   - When a new user message is entered, Gradio will call the `chat()` function again.\n",
        "   - It will pass the accumulated history as an argument, allowing the assistant to continue from where the conversation left off.\n",
        "   - Essentially, **Gradio** persists the `history` state between different user inputs, which is why it seems like `history` is never instantiated explicitly in the code.\n",
        "\n",
        "### Example Flow:\n",
        "1. **First User Message**:\n",
        "   - The `chat()` function is called by Gradio.\n",
        "   - `history` is initially an empty list (`[]`).\n",
        "   - After generating the assistant's response, the pair `(user_message, response)` is appended to `history`.\n",
        "   - `history` is saved by Gradio for future interactions.\n",
        "\n",
        "2. **Second User Message**:\n",
        "   - Gradio calls `chat()` again, but this time it provides the `history` list from the previous interaction.\n",
        "   - `history` now contains the previous message-response pairs, allowing the assistant to maintain context.\n",
        "\n",
        "### Key Takeaways for Learning:\n",
        "- **`history` Is Managed by Gradio**: The `history` variable is not defined manually in your script because it is managed and passed by **Gradio**. It retains the state of the conversation across multiple calls to the `chat` function.\n",
        "\n",
        "\n",
        "### Example with Different Parameter Names:\n",
        "You could rename `message` and `history` to something else, and it would still work with Gradio, as long as they are in the correct order.\n",
        "\n",
        "- You **don't need to use the exact keywords** like `chat(message, history)`.\n",
        "- What matters is that you pass **two arguments** to your function: one for the **user’s message** and one for the **conversation history**.\n",
        "- You can rename the function and parameters to anything descriptive, as long as they match the expected inputs for Gradio.\n",
        "\n",
        "This flexibility helps you make your code more readable and descriptive without being bound to specific parameter names.\n",
        "\n"
      ],
      "metadata": {
        "id": "f6UVCfxyFB6q"
      },
      "id": "f6UVCfxyFB6q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Function with Shopping Assistant"
      ],
      "metadata": {
        "id": "3JYKjXmRNf_Q"
      },
      "id": "3JYKjXmRNf_Q"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
        "outputId": "e4a4e0bc-49b5-4d61-e3c1-d1f1993d49ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://04317974f1695a4d32.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://04317974f1695a4d32.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "MODEL = 'gpt-4o-mini'\n",
        "system_message = \"You are a helpful assistant\"\n",
        "\n",
        "def chat(message, history):\n",
        "    # Start the message list with the system message\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "\n",
        "    # Iterate through the history, adding user and assistant messages using OpenAI-style format\n",
        "    for msg in history:\n",
        "        # Expecting history in OpenAI-style with role and content\n",
        "        messages.append(msg)\n",
        "\n",
        "    # Add the latest user message to the conversation\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    print(\"History is:\")\n",
        "    print(history)\n",
        "    print(\"And messages is:\")\n",
        "    print(messages)\n",
        "\n",
        "    # Send the message to the OpenAI API with streaming\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "# More detailed system message for the assistant behavior\n",
        "system_message = (\n",
        "    \"You are a helpful assistant in a clothing store. Your goal is to gently encourage \"\n",
        "    \"the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \"\n",
        "    \"For example, if the customer says, 'I'm looking to buy a hat,' you could reply with something like, \"\n",
        "    \"'Wonderful - we have lots of hats, including several that are part of our sales event.' \"\n",
        "    \"Encourage the customer to buy hats if they are unsure what to get.\"\n",
        ")\n",
        "\n",
        "# Specify type='messages' to use OpenAI-style role/content format\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
      "metadata": {
        "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternate System Role"
      ],
      "metadata": {
        "id": "wn8LgkJFOkM9"
      },
      "id": "wn8LgkJFOkM9"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d75f0ffa-55c8-4152-b451-945021676837",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "d75f0ffa-55c8-4152-b451-945021676837",
        "outputId": "af1523fa-cd42-4c02-fb0f-b76b1f15c2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6d652f58bacb83fc3e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6d652f58bacb83fc3e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
        "but remind the customer to look at hats!\"\n",
        "\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
      "metadata": {
        "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Context"
      ],
      "metadata": {
        "id": "zOVnBSASPTLc"
      },
      "id": "zOVnBSASPTLc"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
        "outputId": "4115b537-9400-4918-bf33-a32b88c6c9dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://594e9f0b0b747f64fe.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://594e9f0b0b747f64fe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_message}]\n",
        "    for user_message, assistant_message in history:\n",
        "        messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "\n",
        "    if 'belt' in message: # add context about belts\n",
        "        messages.append({\"role\": \"system\", \"content\": \"For added context, the store does not sell belts, \\\n",
        "                        but be sure to point out other items on sale\"})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
        "\n",
        "    response = \"\"\n",
        "    for chunk in stream:\n",
        "        response += chunk.choices[0].delta.content or ''\n",
        "        yield response\n",
        "\n",
        "\n",
        "gr.ChatInterface(fn=chat, type=\"messages\").launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
      "metadata": {
        "id": "20570de2-eaad-42cc-a92c-c779d71b48b6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035",
      "metadata": {
        "id": "887fd6c1-2db0-4dc4-bc53-49399af8e035"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}