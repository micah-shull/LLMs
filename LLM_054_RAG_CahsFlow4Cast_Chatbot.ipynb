{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOZ8xBykTpdGGp3eMrTWOZF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_054_RAG_CahsFlow4Cast_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ The Goal:  \n",
        "Run a chatbot that uses your **FAISS + blog chunks** to answer questions 24/7, likely via your website or app.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Key Components of a 24/7 Chatbot\n",
        "\n",
        "| Component         | Purpose |\n",
        "|------------------|---------|\n",
        "| üîç **Vector Index (FAISS)** | Fast document retrieval |\n",
        "| üìö **Blog Chunks**          | Your knowledge base |\n",
        "| ü§ñ **LLM** (like Falcon or OpenAI) | Generates the response |\n",
        "| üí¨ **Chat UI / API**        | User-facing interface |\n",
        "| ‚öôÔ∏è **Backend App** (e.g., FastAPI) | Runs RAG logic and serves responses |\n",
        "| ‚òÅÔ∏è **Hosting Platform**     | Keeps your app running 24/7 |\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Step-by-Step Plan\n",
        "\n",
        "#### 1. **Start with a Simple Chatbot (Gradio + LLM)**\n",
        "- Use `Gradio` to set up a basic chatbot interface\n",
        "- Use a **helpful, instruction-tuned model** (like `tiiuae/falcon-7b-instruct`, `mistralai/Mistral-7B-Instruct`, or even `google/flan-t5-base`)\n",
        "- Let the chatbot answer general questions naturally\n",
        "- Test the tone, quality, and response clarity\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Add Your Brand Voice or Domain Focus**\n",
        "- Adjust prompts: \"You are a helpful assistant for small businesses trying to forecast revenue.\"\n",
        "- Add example questions to guide response style\n",
        "- Validate that it feels like *your* bot\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Integrate RAG for Accuracy + Depth**\n",
        "- Once basic chat is solid:\n",
        "  - Load your FAISS index\n",
        "  - On user query, retrieve top chunks\n",
        "  - Add those chunks as **context** to the prompt\n",
        "  - Let the LLM answer with that grounded knowledge\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Why This Works Better\n",
        "\n",
        "| üö´ All-at-once RAG Bot       | ‚úÖ Build-Then-Integrate |\n",
        "|-----------------------------|-------------------------|\n",
        "| Hard to debug               | Easy to test components |\n",
        "| Can feel robotic or brittle | Lets you iterate naturally |\n",
        "| Context injection can break | You know what \"normal\" output looks like first |\n",
        "| Slower to deploy            | Can get to a working version in minutes |\n",
        "\n",
        "---\n",
        "\n",
        "Great call ‚Äî planning ahead will save us from frustration and make this smooth. Here's a list of **common issues** we might encounter when setting up your Gradio chatbot, along with **proactive solutions**:\n",
        "\n",
        "---\n",
        "\n",
        "### üöß Potential Problems & ‚úÖ Proactive Fixes\n",
        "\n",
        "| üõë Issue | üß† What It Means | ‚úÖ How to Prevent or Fix |\n",
        "|--------|----------------|-------------------------|\n",
        "| **Model loading fails** | Large models like `falcon-7b` might be too heavy for free-tier Spaces or Colab | ‚úÖ Use a lightweight instruction-tuned model like `google/flan-t5-base`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct` with caution (test locally first) |\n",
        "| **Long load times** | First-time model loading can take minutes | ‚úÖ Print a \"loading model...\" message, and cache in Colab to test performance |\n",
        "| **Memory crashes (OOM)** | Large models on limited RAM environments crash | ‚úÖ Try smaller models (like `flan-t5-base`), or test in Colab before deploying |\n",
        "| **Chatbot doesn't respond naturally** | Prompting may not guide the model well | ‚úÖ Use clear system prompts like: `\"You are a helpful assistant for small business owners...\"` |\n",
        "| **API rate limits / Hugging Face issues** | Too many requests or expired token | ‚úÖ Log in once with `huggingface_hub.login()` and monitor API usage. Use `.env` for token safety. |\n",
        "| **Gradio UI issues (formatting, text overflow)** | Output is too long or hard to read | ‚úÖ Use `Textbox`, add a line-wrap or max length, and validate layout in Colab |\n",
        "| **Chatbot resets every time** | No memory between turns | ‚úÖ This is expected unless we add memory ‚Äî which we can layer in later |\n",
        "| **Hugging Face Spaces deployment errors** | Missing files or bad requirements.txt | ‚úÖ Include all files: `app.py`, `requirements.txt`, index files, metadata, and test the app locally before uploading |\n",
        "| **RAG integration breaks** | When we add chunked context, prompt gets too long | ‚úÖ Start simple, test with 1 chunk, and truncate carefully with `tokenizer.encode(..., truncation=True)` when we get there |\n",
        "\n",
        "---\n",
        "\n",
        "### üîí Best Practices (Now)\n",
        "\n",
        "- ‚úÖ **Use Colab to test locally** before pushing to Hugging Face Spaces\n",
        "- ‚úÖ **Pick a small model first** (`flan-t5-base`, `mistral-7b` if GPU available)\n",
        "- ‚úÖ **Start with a single-turn chatbot**\n",
        "- ‚úÖ **Save your `.env` token in your Colab securely**\n",
        "- ‚úÖ **Install dependencies cleanly** (`!pip install gradio transformers`)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkzkZtnP3Sri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Dependencies"
      ],
      "metadata": {
        "id": "fKuiMEWWB0nh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zxo5FcsYxsSG"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio transformers dotenv huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load the Model and Set Up Chat Function"
      ],
      "metadata": {
        "id": "2PrOjSOdBx8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*The secret.*\")\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n",
        "\n",
        "# Load a small, instruction-tuned model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def respond(message):\n",
        "    prompt = f\"Answer the following question in a helpful and clear way:\\n\\n{message}\"\n",
        "    result = chatbot(prompt, max_new_tokens=150)[0][\"generated_text\"]\n",
        "\n",
        "    chat_history.append({\n",
        "        \"user\": message,\n",
        "        \"bot\": result\n",
        "    })\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBFuiIcqBGRa",
        "outputId": "ba2e8f0f-3d49-4a2e-a112-e9ebc09e19c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üí¨ Small Business Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot_output = gr.Textbox(lines=6, label=\"Assistant\", interactive=False)\n",
        "\n",
        "    user_input = gr.Textbox(placeholder=\"Ask a question...\", label=\"You\")\n",
        "\n",
        "    def handle_message(message):\n",
        "        return respond(message)\n",
        "\n",
        "    user_input.submit(fn=handle_message, inputs=user_input, outputs=chatbot_output)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "wh7TWTOwBGOk",
        "outputId": "86cb7037-2850-43f6-cea0-d45f582d218a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a6ebd29c827383c3c3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6ebd29c827383c3c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a6ebd29c827383c3c3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Chat Logs"
      ],
      "metadata": {
        "id": "0hNEltxkECza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, turn in enumerate(chat_history):\n",
        "    print(f\"üîπ Q{i+1}: {turn['user']}\")\n",
        "    print(f\"üí¨ A{i+1}: {turn['bot']}\\n{'-'*60}\")\n"
      ],
      "metadata": {
        "id": "Q7wwleXqBGLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "### üß† Option 1: **Fine-Tune the Chat Prompt First**\n",
        "This gives you **stronger answers** *immediately* and helps validate your use case before adding complexity.\n",
        "\n",
        "#### Why it‚Äôs valuable:\n",
        "- Clarifies your assistant‚Äôs **tone, role, and style**\n",
        "- Prevents vague or generic answers\n",
        "- Easier to test how well the base model works *before* adding RAG\n",
        "\n",
        "#### What we can do:\n",
        "- Add a **clear system-style prompt** (like: \"You are a helpful forecasting assistant for small business owners.\")\n",
        "- Preload common topics (economic indicators, forecasting accuracy, etc.)\n",
        "- Add examples of ideal responses as in-context few-shot learning\n",
        "\n",
        "‚úÖ **Recommended next if you want better answers right now.**\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Option 2: **Add RAG (Retrieval-Augmented Generation)**\n",
        "This helps you **pull in real knowledge** from your blog and business content.\n",
        "\n",
        "#### Why it‚Äôs valuable:\n",
        "- Gives the model *real substance* from your blog\n",
        "- Helps it answer questions that aren't in the base model's training\n",
        "- Makes your chatbot **truly yours**\n",
        "\n",
        "#### What we can do:\n",
        "- Embed the question\n",
        "- Retrieve top-k chunks from your blog using FAISS\n",
        "- Concatenate the chunks and send them as context with the question\n",
        "\n",
        "‚úÖ **Recommended next if you're ready to scale to real business Q&A.**\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Best Practice Order:\n",
        "1. ‚úÖ **Fine-tune your prompt** to get the tone, role, and helpfulness right  \n",
        "2. ‚úÖ Then **add RAG** to feed it real data for more grounded answers  \n",
        "3. ‚úÖ Later: add multi-turn memory, logging, and deployment (e.g., Gradio Spaces)\n"
      ],
      "metadata": {
        "id": "GnckQFRSGIps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 1: Create a Strong System Prompt\n",
        "\n",
        "Adding **guardrails through a strong system prompt** is **one of the most effective ways** to:\n",
        "\n",
        "‚úÖ Keep the chatbot focused  \n",
        "‚úÖ Avoid wandering into unsafe or off-topic territory  \n",
        "‚úÖ Increase user trust and experience  \n",
        "\n",
        "---\n",
        "\n",
        "### üîí Why Guardrails Matter\n",
        "\n",
        "Even small models like `flan-t5-base` will try to answer **anything** you throw at them unless you tell them:\n",
        "\n",
        "> ‚ùå \"That's not within my expertise.\"  \n",
        "> ‚úÖ \"Let me help you with something related to forecasting or business planning.\"\n",
        "\n",
        "And in public-facing tools, this **prevents confusion**, **content liability**, and **weird answers** that can harm credibility.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ How to Add These Guardrails (Prompt Snippet)\n",
        "\n",
        "Here's how you might extend your system prompt:\n",
        "\n",
        "```txt\n",
        "You are a helpful assistant for small business owners.\n",
        "\n",
        "Your expertise includes:\n",
        "- Cash flow forecasting\n",
        "- Economic indicators\n",
        "- Business decision-making\n",
        "- Data-driven growth strategies\n",
        "\n",
        "Only answer questions related to these topics. If a question is outside your domain (e.g. politics, health, personal advice), politely respond:\n",
        "\n",
        "\"I'm trained to assist with business forecasting and strategy. Let me know how I can help in that area!\"\n",
        "\n",
        "Here are a few examples of questions you can help with:\n",
        "Q: What economic indicators matter most for retail?\n",
        "A: ...\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üõ°Ô∏è Bonus Techniques You Can Add Later\n",
        "\n",
        "- **Classify user questions** before answering (zero-shot or intent classifier)\n",
        "- **Reject or redirect** with: `\"I'm only able to assist with...\"`  \n",
        "- **Log off-topic queries** to improve your examples over time\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to help you update your current prompt with this restriction built in? We can also add a fallback message for out-of-scope topics."
      ],
      "metadata": {
        "id": "kpK5U-rdHP8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a helpful, friendly assistant who helps small business owners improve their forecasting and understand key economic indicators.\n",
        "\n",
        "You specialize in:\n",
        "- Explaining economic indicators like CPI, consumer confidence, and retail sales\n",
        "- Helping owners interpret forecasting accuracy\n",
        "- Recommending how to adjust based on business conditions\n",
        "- Using real-world examples and simple language\n",
        "\n",
        "Respond clearly and conversationally. Avoid overly technical jargon.\"\"\"\n"
      ],
      "metadata": {
        "id": "L0PawoGoBGJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ Step 2: Add In-Context Few-Shot Examples\n",
        "\n",
        "\n",
        "You're performing what's called **prompt engineering** or **in-context learning**, which is a powerful way to *steer the model* without needing to retrain it.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What‚Äôs Happening Behind the Scenes\n",
        "\n",
        "- When you send a message like  \n",
        "  > ‚ÄúWhat is CPI and why does it matter?‚Äù  \n",
        "  you're not just sending that alone.\n",
        "\n",
        "- You're sending the entire crafted prompt:\n",
        "\n",
        "```plaintext\n",
        "You are a helpful, friendly assistant who helps small business owners...\n",
        "[+ examples of great Q&A]\n",
        "Now answer this new question:\n",
        "Q: What is CPI and why does it matter?\n",
        "A:\n",
        "```\n",
        "\n",
        "- The model uses the **patterns** in your examples to generate a consistent, on-brand, and topic-aware response.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why This Works So Well\n",
        "\n",
        "‚úÖ No fine-tuning required  \n",
        "‚úÖ Super flexible ‚Äî just update your examples  \n",
        "‚úÖ You can A/B test prompts easily  \n",
        "‚úÖ It's cheaper and faster than model training  \n",
        "‚úÖ Perfect for early-stage prototypes or small business chatbots\n",
        "\n",
        "---\n",
        "\n",
        "If you're up for it, we can also:\n",
        "- Add **topic-aware branching** (e.g. different examples depending on whether it's about inflation or sales)\n",
        "- Store previous prompts to experiment with **prompt refinement**\n",
        "- Add a slider for *response length or helpfulness*\n",
        "\n",
        "üí° You nailed it ‚Äî and you're thinking like an AI architect now.\n",
        "\n",
        "There‚Äôs a **spectrum** between **in-context learning** and **RAG**, and you‚Äôre absolutely right: if you keep feeding the model more and more context manually, you‚Äôre slowly building a *manual RAG system* without calling it that.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† In-Context Learning vs RAG\n",
        "\n",
        "| Feature | **In-Context Learning** | **RAG (Retrieval-Augmented Generation)** |\n",
        "|--------|-------------------------|-----------------------------------------|\n",
        "| üîß Setup | Handcrafted prompt with examples | Dynamic retrieval of relevant info |\n",
        "| üì¶ Data | Static and hardcoded | Stored in an index (like FAISS) |\n",
        "| üìà Scaling | Manual ‚Äî hits token limit fast | Automatic ‚Äî fetches only what‚Äôs needed |\n",
        "| üîÅ Adaptability | Rigid, needs manual updates | Flexible, can grow as your docs grow |\n",
        "| üí° Use Case | Small fixed topics | Large, evolving knowledge bases |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ So What Are the Limits of In-Context Learning?\n",
        "\n",
        "1. **Token Limits**  \n",
        "   Most models like `flan-t5-base` cap around 512‚Äì1024 tokens. You‚Äôll run out of room fast if you add too many examples.\n",
        "\n",
        "2. **Relevance Weakens**  \n",
        "   The more you pack in, the harder it is for the model to focus. Unlike RAG, it won‚Äôt *retrieve* the best examples ‚Äî it just reads all of them equally.\n",
        "\n",
        "3. **Hard to Update**  \n",
        "   If your blog changes or you want to update one fact, you have to manually rewrite your prompt examples.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Where RAG Begins to Shine\n",
        "\n",
        "RAG steps in **exactly when**:\n",
        "- You have too many examples to fit in context\n",
        "- Your content changes often\n",
        "- You want to scale without prompt clutter\n",
        "\n",
        "So yes ‚Äî in-context learning is great for:\n",
        "> ‚ÄúSmall, focused, handcrafted expertise.‚Äù\n",
        "\n",
        "RAG is great for:\n",
        "> ‚ÄúScaleable, flexible knowledge bases.‚Äù\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ll3Ftr3Hhjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES = \"\"\"\n",
        "Q: What is CPI and why does it matter for my store?\n",
        "A: CPI stands for Consumer Price Index. It tracks how much prices are rising for everyday goods. If CPI is going up, it means inflation is rising ‚Äî and your customers may start cutting back on spending. Watching CPI helps you plan for slower sales and adjust your pricing.\n",
        "\n",
        "Q: How can I tell if my forecasts are accurate?\n",
        "A: One good way is to look at your forecasting error. If your forecast says you‚Äôll sell $10,000 but you only sell $8,000, your error is 20%. A well-tuned model should get you under 10% error most of the time.\n",
        "\n",
        "Q: What economic indicators are most useful?\n",
        "A: Start with local unemployment, consumer spending, and inflation. These show how confident your customers are and how much they can afford. If those numbers shift, your sales probably will too.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "AZDIDiQ0HN45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 3: Combine Into a Prompt Template"
      ],
      "metadata": {
        "id": "CJ7hD5u8Hr7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(message):\n",
        "    return f\"\"\"{SYSTEM_PROMPT}\n",
        "\n",
        "Here are some examples of helpful answers:\n",
        "\n",
        "{EXAMPLES}\n",
        "\n",
        "Now answer this new question:\n",
        "\n",
        "Q: {message}\n",
        "A:\"\"\"\n"
      ],
      "metadata": {
        "id": "g58BaSD4HN2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Final Updated Chatbot Function"
      ],
      "metadata": {
        "id": "VqZJ164kHxQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(message):\n",
        "    prompt = build_prompt(message)\n",
        "    result = chatbot(prompt, max_new_tokens=200)[0][\"generated_text\"]\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "RrDCEMtQHNzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HM1YiDpGHNw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hpHsF6TXHNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLijSQjHHNr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oy8aQCkWHNpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Clean Up & Remove Widgets from Notebook to Save to Github"
      ],
      "metadata": {
        "id": "ejLijtALBRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your current notebook file (adjust if different)\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_053_RAG_CahsFlow4Cast_Embeddings.ipynb\"\n",
        "\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove the widget metadata if it exists\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook metadata cleaned. Try saving to GitHub again.\")\n"
      ],
      "metadata": {
        "id": "CgU1QYqZBGGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5mRtUYgBGDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}