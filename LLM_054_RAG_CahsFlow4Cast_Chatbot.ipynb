{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgOSpavWI57lNmVhXJv+p7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_054_RAG_CahsFlow4Cast_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ Your Goal:  \n",
        "Run a chatbot that uses your **FAISS + blog chunks** to answer questions 24/7, likely via your website or app.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Key Components of a 24/7 Chatbot\n",
        "\n",
        "| Component         | Purpose |\n",
        "|------------------|---------|\n",
        "| üîç **Vector Index (FAISS)** | Fast document retrieval |\n",
        "| üìö **Blog Chunks**          | Your knowledge base |\n",
        "| ü§ñ **LLM** (like Falcon or OpenAI) | Generates the response |\n",
        "| üí¨ **Chat UI / API**        | User-facing interface |\n",
        "| ‚öôÔ∏è **Backend App** (e.g., FastAPI) | Runs RAG logic and serves responses |\n",
        "| ‚òÅÔ∏è **Hosting Platform**     | Keeps your app running 24/7 |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Deployment Blueprint\n",
        "\n",
        "#### 1. **Convert Your Colab Workflow Into a Python Script**\n",
        "\n",
        "Your notebook logic (embedding, loading FAISS, RAG function, etc.) should be refactored into:\n",
        "- `rag_pipeline.py` ‚Äî all logic\n",
        "- `app.py` ‚Äî web app interface (e.g., FastAPI or Flask)\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Set Up a Simple API Server (e.g., FastAPI)**\n",
        "\n",
        "```python\n",
        "# app.py\n",
        "from fastapi import FastAPI, Request\n",
        "from rag_pipeline import search_and_generate\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/ask\")\n",
        "async def ask(request: Request):\n",
        "    data = await request.json()\n",
        "    question = data.get(\"question\")\n",
        "    answer = search_and_generate(question)\n",
        "    return {\"question\": question, \"answer\": answer}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Host It Somewhere (Always On)**\n",
        "\n",
        "| Option         | Cost       | Notes |\n",
        "|----------------|------------|-------|\n",
        "| üîÑ **Replit**  | Free       | Easy to set up, always-on with ping |\n",
        "| üåç **Render**  | Free tier  | Deploy FastAPI/Flask easily |\n",
        "| ‚òÅÔ∏è **Hugging Face Spaces** | Free (CPU) | Host a Gradio UI directly |\n",
        "| üî• **Modal/Replicate** | Pay-per-use | Scale up dynamically |\n",
        "| üöÄ **Docker + VPS (Advanced)** | ~$5/mo | Host it yourself |\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Optional: Add a UI (Gradio or Chat Widget)**\n",
        "\n",
        "To make it interactive:\n",
        "- Use **Gradio** (easiest)\n",
        "- Or integrate with your **blog site** using a chat widget and fetch responses via API\n",
        "\n",
        "Yes ‚Äî **you‚Äôre thinking in exactly the right way**! üëè\n",
        "\n",
        "### ‚úÖ Recommended Strategy: Start Simple, Then Add RAG\n",
        "\n",
        "> It‚Äôs much smarter (and more fun) to **first get a chatbot working cleanly with basic behavior**, then plug in your **RAG system** once the core is stable.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Step-by-Step Plan\n",
        "\n",
        "#### 1. **Start with a Simple Chatbot (Gradio + LLM)**\n",
        "- Use `Gradio` to set up a basic chatbot interface\n",
        "- Use a **helpful, instruction-tuned model** (like `tiiuae/falcon-7b-instruct`, `mistralai/Mistral-7B-Instruct`, or even `google/flan-t5-base`)\n",
        "- Let the chatbot answer general questions naturally\n",
        "- Test the tone, quality, and response clarity\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Add Your Brand Voice or Domain Focus**\n",
        "- Adjust prompts: \"You are a helpful assistant for small businesses trying to forecast revenue.\"\n",
        "- Add example questions to guide response style\n",
        "- Validate that it feels like *your* bot\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Integrate RAG for Accuracy + Depth**\n",
        "- Once basic chat is solid:\n",
        "  - Load your FAISS index\n",
        "  - On user query, retrieve top chunks\n",
        "  - Add those chunks as **context** to the prompt\n",
        "  - Let the LLM answer with that grounded knowledge\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Why This Works Better\n",
        "\n",
        "| üö´ All-at-once RAG Bot       | ‚úÖ Build-Then-Integrate |\n",
        "|-----------------------------|-------------------------|\n",
        "| Hard to debug               | Easy to test components |\n",
        "| Can feel robotic or brittle | Lets you iterate naturally |\n",
        "| Context injection can break | You know what \"normal\" output looks like first |\n",
        "| Slower to deploy            | Can get to a working version in minutes |\n",
        "\n",
        "---\n",
        "\n",
        "Great call ‚Äî planning ahead will save us from frustration and make this smooth. Here's a list of **common issues** we might encounter when setting up your Gradio chatbot, along with **proactive solutions**:\n",
        "\n",
        "---\n",
        "\n",
        "### üöß Potential Problems & ‚úÖ Proactive Fixes\n",
        "\n",
        "| üõë Issue | üß† What It Means | ‚úÖ How to Prevent or Fix |\n",
        "|--------|----------------|-------------------------|\n",
        "| **Model loading fails** | Large models like `falcon-7b` might be too heavy for free-tier Spaces or Colab | ‚úÖ Use a lightweight instruction-tuned model like `google/flan-t5-base`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct` with caution (test locally first) |\n",
        "| **Long load times** | First-time model loading can take minutes | ‚úÖ Print a \"loading model...\" message, and cache in Colab to test performance |\n",
        "| **Memory crashes (OOM)** | Large models on limited RAM environments crash | ‚úÖ Try smaller models (like `flan-t5-base`), or test in Colab before deploying |\n",
        "| **Chatbot doesn't respond naturally** | Prompting may not guide the model well | ‚úÖ Use clear system prompts like: `\"You are a helpful assistant for small business owners...\"` |\n",
        "| **API rate limits / Hugging Face issues** | Too many requests or expired token | ‚úÖ Log in once with `huggingface_hub.login()` and monitor API usage. Use `.env` for token safety. |\n",
        "| **Gradio UI issues (formatting, text overflow)** | Output is too long or hard to read | ‚úÖ Use `Textbox`, add a line-wrap or max length, and validate layout in Colab |\n",
        "| **Chatbot resets every time** | No memory between turns | ‚úÖ This is expected unless we add memory ‚Äî which we can layer in later |\n",
        "| **Hugging Face Spaces deployment errors** | Missing files or bad requirements.txt | ‚úÖ Include all files: `app.py`, `requirements.txt`, index files, metadata, and test the app locally before uploading |\n",
        "| **RAG integration breaks** | When we add chunked context, prompt gets too long | ‚úÖ Start simple, test with 1 chunk, and truncate carefully with `tokenizer.encode(..., truncation=True)` when we get there |\n",
        "\n",
        "---\n",
        "\n",
        "### üîí Best Practices (Now)\n",
        "\n",
        "- ‚úÖ **Use Colab to test locally** before pushing to Hugging Face Spaces\n",
        "- ‚úÖ **Pick a small model first** (`flan-t5-base`, `mistral-7b` if GPU available)\n",
        "- ‚úÖ **Start with a single-turn chatbot**\n",
        "- ‚úÖ **Save your `.env` token in your Colab securely**\n",
        "- ‚úÖ **Install dependencies cleanly** (`!pip install gradio transformers`)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkzkZtnP3Sri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Dependencies"
      ],
      "metadata": {
        "id": "fKuiMEWWB0nh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zxo5FcsYxsSG"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio transformers dotenv huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load the Model and Set Up Chat Function"
      ],
      "metadata": {
        "id": "2PrOjSOdBx8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*The secret.*\")\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n",
        "\n",
        "# Load a small, instruction-tuned model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "chat_history = []\n",
        "\n",
        "def respond(message):\n",
        "    prompt = f\"Answer the following question in a helpful and clear way:\\n\\n{message}\"\n",
        "    result = chatbot(prompt, max_new_tokens=150)[0][\"generated_text\"]\n",
        "\n",
        "    chat_history.append({\n",
        "        \"user\": message,\n",
        "        \"bot\": result\n",
        "    })\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBFuiIcqBGRa",
        "outputId": "ba2e8f0f-3d49-4a2e-a112-e9ebc09e19c9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üí¨ Small Business Chatbot\")\n",
        "\n",
        "    with gr.Row():\n",
        "        chatbot_output = gr.Textbox(lines=6, label=\"Assistant\", interactive=False)\n",
        "\n",
        "    user_input = gr.Textbox(placeholder=\"Ask a question...\", label=\"You\")\n",
        "\n",
        "    def handle_message(message):\n",
        "        return respond(message)\n",
        "\n",
        "    user_input.submit(fn=handle_message, inputs=user_input, outputs=chatbot_output)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "wh7TWTOwBGOk",
        "outputId": "86cb7037-2850-43f6-cea0-d45f582d218a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a6ebd29c827383c3c3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6ebd29c827383c3c3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a6ebd29c827383c3c3.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View Chat Logs"
      ],
      "metadata": {
        "id": "0hNEltxkECza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, turn in enumerate(chat_history):\n",
        "    print(f\"üîπ Q{i+1}: {turn['user']}\")\n",
        "    print(f\"üí¨ A{i+1}: {turn['bot']}\\n{'-'*60}\")\n"
      ],
      "metadata": {
        "id": "Q7wwleXqBGLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "### üß† Option 1: **Fine-Tune the Chat Prompt First**\n",
        "This gives you **stronger answers** *immediately* and helps validate your use case before adding complexity.\n",
        "\n",
        "#### Why it‚Äôs valuable:\n",
        "- Clarifies your assistant‚Äôs **tone, role, and style**\n",
        "- Prevents vague or generic answers\n",
        "- Easier to test how well the base model works *before* adding RAG\n",
        "\n",
        "#### What we can do:\n",
        "- Add a **clear system-style prompt** (like: \"You are a helpful forecasting assistant for small business owners.\")\n",
        "- Preload common topics (economic indicators, forecasting accuracy, etc.)\n",
        "- Add examples of ideal responses as in-context few-shot learning\n",
        "\n",
        "‚úÖ **Recommended next if you want better answers right now.**\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Option 2: **Add RAG (Retrieval-Augmented Generation)**\n",
        "This helps you **pull in real knowledge** from your blog and business content.\n",
        "\n",
        "#### Why it‚Äôs valuable:\n",
        "- Gives the model *real substance* from your blog\n",
        "- Helps it answer questions that aren't in the base model's training\n",
        "- Makes your chatbot **truly yours**\n",
        "\n",
        "#### What we can do:\n",
        "- Embed the question\n",
        "- Retrieve top-k chunks from your blog using FAISS\n",
        "- Concatenate the chunks and send them as context with the question\n",
        "\n",
        "‚úÖ **Recommended next if you're ready to scale to real business Q&A.**\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Best Practice Order:\n",
        "1. ‚úÖ **Fine-tune your prompt** to get the tone, role, and helpfulness right  \n",
        "2. ‚úÖ Then **add RAG** to feed it real data for more grounded answers  \n",
        "3. ‚úÖ Later: add multi-turn memory, logging, and deployment (e.g., Gradio Spaces)\n"
      ],
      "metadata": {
        "id": "GnckQFRSGIps"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0PawoGoBGJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memory Clean Up & Remove Widgets from Notebook to Save to Github"
      ],
      "metadata": {
        "id": "ejLijtALBRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your current notebook file (adjust if different)\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_053_RAG_CahsFlow4Cast_Embeddings.ipynb\"\n",
        "\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove the widget metadata if it exists\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook metadata cleaned. Try saving to GitHub again.\")\n"
      ],
      "metadata": {
        "id": "CgU1QYqZBGGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5mRtUYgBGDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}