{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOABbHuAvb7kHmv1V6Nuz04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_054_RAG_CahsFlow4Cast_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ The Goal:  \n",
        "Run a chatbot that uses your **FAISS + blog chunks** to answer questions 24/7, likely via your website or app.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Key Components of a 24/7 Chatbot\n",
        "\n",
        "| Component         | Purpose |\n",
        "|------------------|---------|\n",
        "| 🔍 **Vector Index (FAISS)** | Fast document retrieval |\n",
        "| 📚 **Blog Chunks**          | Your knowledge base |\n",
        "| 🤖 **LLM** (like Falcon or OpenAI) | Generates the response |\n",
        "| 💬 **Chat UI / API**        | User-facing interface |\n",
        "| ⚙️ **Backend App** (e.g., FastAPI) | Runs RAG logic and serves responses |\n",
        "| ☁️ **Hosting Platform**     | Keeps your app running 24/7 |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧱 Step-by-Step Plan\n",
        "\n",
        "#### 1. **Start with a Simple Chatbot (Gradio + LLM)**\n",
        "- Use `Gradio` to set up a basic chatbot interface\n",
        "- Use a **helpful, instruction-tuned model** (like `tiiuae/falcon-7b-instruct`, `mistralai/Mistral-7B-Instruct`, or even `google/flan-t5-base`)\n",
        "- Let the chatbot answer general questions naturally\n",
        "- Test the tone, quality, and response clarity\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Add Your Brand Voice or Domain Focus**\n",
        "- Adjust prompts: \"You are a helpful assistant for small businesses trying to forecast revenue.\"\n",
        "- Add example questions to guide response style\n",
        "- Validate that it feels like *your* bot\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Integrate RAG for Accuracy + Depth**\n",
        "- Once basic chat is solid:\n",
        "  - Load your FAISS index\n",
        "  - On user query, retrieve top chunks\n",
        "  - Add those chunks as **context** to the prompt\n",
        "  - Let the LLM answer with that grounded knowledge\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 Why This Works Better\n",
        "\n",
        "| 🚫 All-at-once RAG Bot       | ✅ Build-Then-Integrate |\n",
        "|-----------------------------|-------------------------|\n",
        "| Hard to debug               | Easy to test components |\n",
        "| Can feel robotic or brittle | Lets you iterate naturally |\n",
        "| Context injection can break | You know what \"normal\" output looks like first |\n",
        "| Slower to deploy            | Can get to a working version in minutes |\n",
        "\n",
        "---\n",
        "\n",
        "Great call — planning ahead will save us from frustration and make this smooth. Here's a list of **common issues** we might encounter when setting up your Gradio chatbot, along with **proactive solutions**:\n",
        "\n",
        "---\n",
        "\n",
        "### 🚧 Potential Problems & ✅ Proactive Fixes\n",
        "\n",
        "| 🛑 Issue | 🧠 What It Means | ✅ How to Prevent or Fix |\n",
        "|--------|----------------|-------------------------|\n",
        "| **Model loading fails** | Large models like `falcon-7b` might be too heavy for free-tier Spaces or Colab | ✅ Use a lightweight instruction-tuned model like `google/flan-t5-base`, `mistralai/Mistral-7B-Instruct`, or `tiiuae/falcon-7b-instruct` with caution (test locally first) |\n",
        "| **Long load times** | First-time model loading can take minutes | ✅ Print a \"loading model...\" message, and cache in Colab to test performance |\n",
        "| **Memory crashes (OOM)** | Large models on limited RAM environments crash | ✅ Try smaller models (like `flan-t5-base`), or test in Colab before deploying |\n",
        "| **Chatbot doesn't respond naturally** | Prompting may not guide the model well | ✅ Use clear system prompts like: `\"You are a helpful assistant for small business owners...\"` |\n",
        "| **API rate limits / Hugging Face issues** | Too many requests or expired token | ✅ Log in once with `huggingface_hub.login()` and monitor API usage. Use `.env` for token safety. |\n",
        "| **Gradio UI issues (formatting, text overflow)** | Output is too long or hard to read | ✅ Use `Textbox`, add a line-wrap or max length, and validate layout in Colab |\n",
        "| **Chatbot resets every time** | No memory between turns | ✅ This is expected unless we add memory — which we can layer in later |\n",
        "| **Hugging Face Spaces deployment errors** | Missing files or bad requirements.txt | ✅ Include all files: `app.py`, `requirements.txt`, index files, metadata, and test the app locally before uploading |\n",
        "| **RAG integration breaks** | When we add chunked context, prompt gets too long | ✅ Start simple, test with 1 chunk, and truncate carefully with `tokenizer.encode(..., truncation=True)` when we get there |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔒 Best Practices (Now)\n",
        "\n",
        "- ✅ **Use Colab to test locally** before pushing to Hugging Face Spaces\n",
        "- ✅ **Pick a small model first** (`flan-t5-base`, `mistral-7b` if GPU available)\n",
        "- ✅ **Start with a single-turn chatbot**\n",
        "- ✅ **Save your `.env` token in your Colab securely**\n",
        "- ✅ **Install dependencies cleanly** (`!pip install gradio transformers`)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkzkZtnP3Sri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Dependencies"
      ],
      "metadata": {
        "id": "fKuiMEWWB0nh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxo5FcsYxsSG",
        "outputId": "070ede07-04c3-407c-9f4e-d2b16d5c8941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio transformers dotenv huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Load the Model and Set Up Chat Function\n",
        "\n",
        "### ✅ Key Code Blocks\n",
        "\n",
        "#### 1. **Model Setup**\n",
        "```python\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "```\n",
        "- **What it does:** Loads an instruction-tuned model (FLAN-T5) that can generate text responses.\n",
        "- **Why it matters:** The core intelligence of your chatbot. You can swap this out for other models later.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Prompt Engineering**\n",
        "```python\n",
        "prompt = f\"Answer the following question clearly and helpfully:\\n\\n{message}\"\n",
        "```\n",
        "- **What it does:** Gives the model context to shape its response.\n",
        "- **Why it matters:** This is where you can guide behavior, tone, focus, and even safety. This is one of the most important skills when building chatbots.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Gradio UI**\n",
        "```python\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(...),\n",
        "    outputs=\"text\",\n",
        "    title=\"💬 Simple Small Business Chatbot\"\n",
        ")\n",
        "```\n",
        "- **What it does:** Creates a front-end interface with a textbox input and text output.\n",
        "- **Why it matters:** Gradio handles the interaction between user and model, so you can focus on the model’s logic.\n",
        "\n",
        "---\n",
        "\n",
        "### 💡 What Is an **Instruction-Tuned Model**?\n",
        "\n",
        "An **instruction-tuned model** is a language model that has been **trained to follow human instructions**.\n",
        "\n",
        "#### 🔍 Normal vs. Instruction-Tuned:\n",
        "\n",
        "| Type               | Behavior                                                                 |\n",
        "|--------------------|--------------------------------------------------------------------------|\n",
        "| **Standard model** | Predicts the next word in a sentence (language modeling objective)       |\n",
        "| **Instruction-tuned** | Learns to follow **explicit human instructions** and complete tasks        |\n",
        "\n",
        "#### 🧠 Example:\n",
        "\n",
        "If you ask a **standard model**:\n",
        "> \"What are economic indicators?\"\n",
        "\n",
        "It might just continue the sentence or generate random facts.\n",
        "\n",
        "But if you ask an **instruction-tuned model**:\n",
        "> \"Explain economic indicators in simple terms.\"\n",
        "\n",
        "It knows you're asking for a **helpful explanation** and will give a more structured, user-focused answer.\n",
        "\n",
        "#### ✅ FLAN-T5 is instruction-tuned.\n",
        "That’s why your prompt like:\n",
        "```python\n",
        "\"Answer the following question clearly and helpfully:\\n\\n{message}\"\n",
        "```\n",
        "actually works!\n",
        "\n",
        "\n",
        "### 📌 What to Learn Next\n",
        "\n",
        "| Topic                        | Why It’s Important                                           |\n",
        "|-----------------------------|---------------------------------------------------------------|\n",
        "| **Prompt Engineering**       | Crafting effective instructions is key to good responses.     |\n",
        "| **State and History Handling** | Allows multi-turn conversations that feel natural.           |\n",
        "| **System Prompt Design**     | Sets tone, focus, safety boundaries.                         |\n",
        "| **Model Selection**          | Smaller = faster, larger = smarter (usually).                |\n",
        "| **RAG Integration**          | Adds real-world knowledge your model wasn’t trained on.       |\n",
        "| **Deployment Options**       | Hugging Face Spaces, Flask, FastAPI, etc., for 24/7 hosting.  |\n",
        "\n"
      ],
      "metadata": {
        "id": "2PrOjSOdBx8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*The secret.*\")\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n",
        "\n",
        "# Load the model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Define a basic response function\n",
        "def respond(message):\n",
        "    prompt = f\"Answer the following question clearly and helpfully:\\n\\n{message}\"\n",
        "    response = chatbot(prompt, max_new_tokens=150)[0][\"generated_text\"]\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a question...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"💬 Simple Small Business Chatbot\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "TBFuiIcqBGRa",
        "outputId": "ee4c0d63-564f-400e-d6c1-9bb06ef3abd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a95020aae5cd43e22d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a95020aae5cd43e22d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 | Prompt 1\n",
        "\n",
        "### Updated Prompt with a Business Forecasting Persona"
      ],
      "metadata": {
        "id": "3yFXNdh-u64x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Store chat history for debugging\n",
        "chat_log = []\n",
        "\n",
        "# Define a smarter response function\n",
        "def respond(message):\n",
        "    prompt = f\"\"\"You are a helpful assistant for small business owners.\n",
        "\n",
        "Your job is to answer questions related to:\n",
        "- Business forecasting\n",
        "- Economic indicators (like inflation, unemployment, sales data)\n",
        "- Improving cash flow\n",
        "- Making data-driven decisions\n",
        "\n",
        "Always explain in plain English and give clear, helpful advice.\n",
        "\n",
        "Question: {message}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Log this interaction\n",
        "    chat_log.append((message, response))\n",
        "\n",
        "    # Print entire chat history (for you, not the user)\n",
        "    print(\"\\n🧾 Chat Log:\")\n",
        "    for i, (q, a) in enumerate(chat_log, 1):\n",
        "        print(f\"{i}. Q: {q}\\n   A: {a}\\n{'-'*50}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about forecasting, economics, or small business strategy...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"📊 Small Business Forecasting Assistant\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "QiE1KlNWg5kB",
        "outputId": "9e6273aa-2bd1-4012-9044-7a5e097be16f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3975d6d24f5552a66a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3975d6d24f5552a66a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Logs"
      ],
      "metadata": {
        "id": "Q-FDgjc1ioCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧾 Chat Log\\n\" + \"=\" * 40)\n",
        "for i, (user_msg, bot_msg) in enumerate(chat_log, start=1):\n",
        "    print(f\"\\n🧑‍💼 User {i}: {user_msg}\")\n",
        "    print(f\"  🤖 Bot  {i}: {bot_msg}\")\n",
        "    # print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTQc5Hm5iA6u",
        "outputId": "30d337ad-b888-462c-dfca-e35cd4cdd01d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 Chat Log\n",
            "========================================\n",
            "\n",
            "🧑‍💼 User 1: hi can you help me optimize my business?\n",
            "\n",
            "  🤖 Bot  1: Improving cash flow\n",
            "\n",
            "🧑‍💼 User 2: hi can you help me optimize my business?\n",
            "What else would you recommend?\n",
            "\n",
            "  🤖 Bot  2: Improving cash flow\n",
            "\n",
            "🧑‍💼 User 3: \n",
            "What else would you recommend?\n",
            "\n",
            "  🤖 Bot  3: Improving cash flow\n",
            "\n",
            "🧑‍💼 User 4: what are economic indicators?\n",
            "\n",
            "  🤖 Bot  4: inflation, unemployment, sales data\n",
            "\n",
            "🧑‍💼 User 5: what do they tell me about my business?\n",
            "\n",
            "  🤖 Bot  5: Making data-driven decisions\n",
            "\n",
            "🧑‍💼 User 6: can you give me a more elaborate answer?\n",
            "\n",
            "  🤖 Bot  6: Making data-driven decisions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "### 🧠 Option 1: **Fine-Tune the Chat Prompt First**\n",
        "This gives you **stronger answers** *immediately* and helps validate your use case before adding complexity.\n",
        "\n",
        "#### Why it’s valuable:\n",
        "- Clarifies your assistant’s **tone, role, and style**\n",
        "- Prevents vague or generic answers\n",
        "- Easier to test how well the base model works *before* adding RAG\n",
        "\n",
        "#### What we can do:\n",
        "- Add a **clear system-style prompt** (like: \"You are a helpful forecasting assistant for small business owners.\")\n",
        "- Preload common topics (economic indicators, forecasting accuracy, etc.)\n",
        "- Add examples of ideal responses as in-context few-shot learning\n",
        "\n",
        "✅ **Recommended next if you want better answers right now.**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Option 2: **Add RAG (Retrieval-Augmented Generation)**\n",
        "This helps you **pull in real knowledge** from your blog and business content.\n",
        "\n",
        "#### Why it’s valuable:\n",
        "- Gives the model *real substance* from your blog\n",
        "- Helps it answer questions that aren't in the base model's training\n",
        "- Makes your chatbot **truly yours**\n",
        "\n",
        "#### What we can do:\n",
        "- Embed the question\n",
        "- Retrieve top-k chunks from your blog using FAISS\n",
        "- Concatenate the chunks and send them as context with the question\n",
        "\n",
        "✅ **Recommended next if you're ready to scale to real business Q&A.**\n",
        "\n",
        "---\n",
        "\n",
        "### 🚀 Best Practice Order:\n",
        "1. ✅ **Fine-tune your prompt** to get the tone, role, and helpfulness right  \n",
        "2. ✅ Then **add RAG** to feed it real data for more grounded answers  \n",
        "3. ✅ Later: add multi-turn memory, logging, and deployment (e.g., Gradio Spaces)\n"
      ],
      "metadata": {
        "id": "GnckQFRSGIps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 1: Create a Strong System Prompt\n",
        "\n",
        "Adding **guardrails through a strong system prompt** is **one of the most effective ways** to:\n",
        "\n",
        "✅ Keep the chatbot focused  \n",
        "✅ Avoid wandering into unsafe or off-topic territory  \n",
        "✅ Increase user trust and experience  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔒 Why Guardrails Matter\n",
        "\n",
        "Even small models like `flan-t5-base` will try to answer **anything** you throw at them unless you tell them:\n",
        "\n",
        "> ❌ \"That's not within my expertise.\"  \n",
        "> ✅ \"Let me help you with something related to forecasting or business planning.\"\n",
        "\n",
        "And in public-facing tools, this **prevents confusion**, **content liability**, and **weird answers** that can harm credibility.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ How to Add These Guardrails (Prompt Snippet)\n",
        "\n",
        "Here's how you might extend your system prompt:\n",
        "\n",
        "```txt\n",
        "You are a helpful assistant for small business owners.\n",
        "\n",
        "Your expertise includes:\n",
        "- Cash flow forecasting\n",
        "- Economic indicators\n",
        "- Business decision-making\n",
        "- Data-driven growth strategies\n",
        "\n",
        "Only answer questions related to these topics. If a question is outside your domain (e.g. politics, health, personal advice), politely respond:\n",
        "\n",
        "\"I'm trained to assist with business forecasting and strategy. Let me know how I can help in that area!\"\n",
        "\n",
        "Here are a few examples of questions you can help with:\n",
        "Q: What economic indicators matter most for retail?\n",
        "A: ...\n",
        "...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🛡️ Bonus Techniques You Can Add Later\n",
        "\n",
        "- **Classify user questions** before answering (zero-shot or intent classifier)\n",
        "- **Reject or redirect** with: `\"I'm only able to assist with...\"`  \n",
        "- **Log off-topic queries** to improve your examples over time\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to help you update your current prompt with this restriction built in? We can also add a fallback message for out-of-scope topics."
      ],
      "metadata": {
        "id": "kpK5U-rdHP8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1 | Prompt 2"
      ],
      "metadata": {
        "id": "_nc5RsJouuST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "# Store chat history for debugging\n",
        "chat_log = []\n",
        "\n",
        "# Define a smarter response function\n",
        "def respond(message):\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant for small business owners.\n",
        "\n",
        "Your job is to answer questions related to:\n",
        "- Business forecasting\n",
        "- Economic indicators (like inflation, unemployment, sales data)\n",
        "- Improving cash flow\n",
        "- Making data-driven decisions\n",
        "\n",
        "Always explain in plain English and give clear, helpful advice.\n",
        "\n",
        "Examples:\n",
        "Q: What are economic indicators?\n",
        "A: Economic indicators like inflation, consumer confidence, and employment rates help you anticipate changes in customer spending and business risks.\n",
        "\n",
        "Q: How can I improve cash flow?\n",
        "A: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste. These steps can increase your available cash.\n",
        "\n",
        "Now answer this question:\n",
        "\n",
        "Q: {message}\n",
        "A:\"\"\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Log this interaction\n",
        "    chat_log.append((message, response))\n",
        "\n",
        "    # Print entire chat history (for debugging in notebook)\n",
        "    print(\"\\n🧾 Chat Log:\")\n",
        "    for i, (q, a) in enumerate(chat_log, 1):\n",
        "        print(f\"{i}. Q: {q}\\n   A: {a}\\n{'-'*50}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about forecasting, economics, or small business strategy...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"📊 Small Business Forecasting Assistant\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "U7Ldr_aVjCZY",
        "outputId": "6ef4b4a8-0c65-4493-cced-d732caa362ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ccc952365652dd8be.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ccc952365652dd8be.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chat Logs"
      ],
      "metadata": {
        "id": "eVE8HM4IkiPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧾 Chat Log\\n\" + \"=\" * 40)\n",
        "for i, (user_msg, bot_msg) in enumerate(chat_log, start=1):\n",
        "    print(f\"\\n🧑‍💼 User {i}: {user_msg}\")\n",
        "    print(f\"  🤖 Bot  {i}: {bot_msg}\")\n",
        "    # print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dvwnzUrjCXU",
        "outputId": "26b75d77-b2ae-49eb-8b57-9ea3bc66b5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 Chat Log\n",
            "========================================\n",
            "\n",
            "🧑‍💼 User 1: hi can you help me optimize by small business?\n",
            "  🤖 Bot  1: Improving cash flow\n",
            "\n",
            "🧑‍💼 User 2: how do i improve my cash flow?\n",
            "  🤖 Bot  2: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste. These steps can increase your available cash.\n",
            "\n",
            "🧑‍💼 User 3: What about forecasting? an i iprve my cash flow with more accurate  cash flow forecasting?\n",
            "  🤖 Bot  3: Improving cash flow\n",
            "\n",
            "🧑‍💼 User 4: Can you teach me about econimic indicators?\n",
            "  🤖 Bot  4: Economic indicators like inflation, unemployment, and sales data help you anticipate changes in customer spending and business risks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 | Try a Bigger Instruction-Tuned Model\n",
        "\n",
        "\n",
        "\n",
        "### 🔍 What We're Seeing in the Logs:\n",
        "\n",
        "| Prompt | Bot Response | Feedback |\n",
        "|-------|---------------|---------|\n",
        "| \"hi can you help me optimize by small business?\" | \"Improving cash flow\" | 🟥 **Too short and generic** — looks like it's picking a single keyword. |\n",
        "| \"how do i improve my cash flow?\" | ✅ Full, helpful response | ✅ Great! The model picked up on your in-context example. |\n",
        "| \"What about forecasting? Can I improve...\" | \"Improving cash flow\" | 🟥 Weak again — it dropped context and defaulted to short text. |\n",
        "| \"Can you teach me about economic indicators?\" | ✅ Great explanation | ✅ Another success — matches few-shot example. |\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 Likely Cause: Model Limitation\n",
        "\n",
        "You're using **`flan-t5-base`** — a fantastic small model for quick prototyping, but:\n",
        "- It's **small (~250M parameters)** and may struggle with nuance or retaining context across varying phrasing.\n",
        "- It does well with **questions that match your examples**, but falters when you phrase things differently.\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Recommendation: Try a Bigger Instruction-Tuned Model\n",
        "\n",
        "To really test your prompt design and get better completions, try one of these:\n",
        "\n",
        "| Model | Size | Why Use It |\n",
        "|-------|------|------------|\n",
        "| `google/flan-t5-large` | ~780M | Same family, better quality, works in Colab with GPU |\n",
        "| `tiiuae/falcon-7b-instruct` | 7B | Very capable, good instruction tuning, larger GPU required |\n",
        "| `mistralai/Mistral-7B-Instruct-v0.2` | 7B | Excellent overall performance, Hugging Face compatible |\n",
        "\n",
        "You can try switching to:\n",
        "\n",
        "```python\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Tip: Bigger Models Need More Time and GPU\n",
        "\n",
        "Try `flan-t5-large` first — it's a safe step up and works well in Colab.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGEVEtzdlN4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "chatbot = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "79736d1df5494d8880bf2d99bbd6ca50",
            "f4b440216449483596eb879806201f37",
            "cb7f5077619d4e82a5485f55fcd29869",
            "21e584d859fd49f5b1ae7b740cbd2972",
            "7d0f23c58abe4fc4af3e623a7fcce8ee",
            "54b9b3c038204fbca6fb53fd0ff86223",
            "532e0d96bec140499d6a0479f183da89",
            "482b2fa1116d40eca3ad6ec4786acdb6",
            "b745d78eb9754d19aec5ddc66bfed445",
            "06f2d0ffa16c4c9bab911b1fdd85338f",
            "7a33815e7cd74cc4a5d17fc60f6b1104",
            "cc82ca4eb72d41eb8bd8a7712a5d04aa",
            "4809c0d659724f5ba9638dbd1dcd4b7f",
            "2c75f5a6bda244e8b4539e9bfdd7d677",
            "179bf5e883794a32b01e055a0a5885b9",
            "aad415851a58483c9f7ddd4350fc0a29",
            "9e20dc0e285a40a6b16adfb21c649ba6",
            "35982bf9a5f7430aa7cdfcc3a11f316e",
            "a44ed54a7de443be8bc80656820f02ea",
            "4842bffe5dac4a5fbdeac90dceaed560",
            "55f838cbd8e545439029489050ead1ec",
            "8c5d37889fdb4e02900abb5be42b173d",
            "681cf563dcc14fc28c40f69e545bf276",
            "ba51dd20cccb444baf0eea126cee245a",
            "5b9934333c8f4564bcfdb18e67e68a78",
            "b80bf2ca6f7040f9960736d3dadf9fe7",
            "c7fc2a39214d491aa7978cefc1d5e156",
            "10d1dcdf705a4237be926432733d39a5",
            "68cd234d98274383a8a4b2fb206b9c59",
            "00ec88b704894394bd11eb9a2cecbca1",
            "f5292f675a044472865a3861255cb208",
            "cb57e104a880455bbef8a570c836116c",
            "a7418812306f4c109d3750eb9dc39f9a",
            "d6a5b1bd467e4b08acbce7e23c189a30",
            "632d56993c874eb184e8638aef44a5bd",
            "c3fb3635a6fb41e2bed4eab7f8b9e5f8",
            "91835e1a8bbe42f4bcd5e27593e7534d",
            "47f752a2ee5846b7a8abdc0db94d5264",
            "bb59ef77ee284a16abfcc6d33d254278",
            "6e712d518de347ab81fa84c70b6e116d",
            "91fe167304164f619f8eca78269f3897",
            "9b3d0b0170114ec58db10e2460a9f698",
            "6e591ad443f04effaeeb003ab523bdfb",
            "4c9d4c4963dc457b850ed3bf9dd2814d",
            "1579370126794c72affb0ffbc59ac953",
            "ca390b0f284d46cb8523a29c1a5304a1",
            "3bdea8ed7de04bbb8910804d255a52aa",
            "59ac356eaa9c47a8be3d3b0a814495a7",
            "e73e369d685a4cd783a86847e300109a",
            "fd8c8155203741848df4389704b18d41",
            "f52cc3728a9f4cb8aab13ede371747e0",
            "a24f7eee08e4473790d5087e876f77ca",
            "0ab0551a116846d3aa916bbddb2686b1",
            "de1317f201ef48b59b3e6066d5bd91f2",
            "b5612f277aaa43f5b9db9d17cbf75461",
            "36edf22181d941d2a60abb70b3ff874c",
            "b226f87ddd1c441d802ff7f9f0d37a9b",
            "6f3c4937a0c54ac48c7f180e0e01f534",
            "b5870f4187e842dc8caa18eb73c05339",
            "564397ee109d4a93a7a14b3396ba0831",
            "a083ddaff2c54c7d8658d845e013c53b",
            "cc9057c6c4bf40a68751bb31132b0345",
            "f7265aaaa4914d13b4cdf397fefe6a59",
            "ab3e2e2b30044b618832f8790a57667b",
            "0e466d5064244b839f0bcc96fc4efdfd",
            "65f7b358bd034eb383850d70c8b5dece",
            "98a69bd54cc64117b4961161e17b6d44",
            "d43aa1d077b54c53978e30aa2278e87c",
            "23a06e0669514cbab761f8f59ab0ef4d",
            "034399a4bf0a42d38b6f9e6a0d5bd23c",
            "22b23850ce2f4cd9908c680af624f033",
            "1deebc7500d24f7eaf9a966ae3436ded",
            "a8c1c7c5a6764637acb11ae4e1fec073",
            "17f8ce86f3a342ba912a514e37f24f7a",
            "0e6fb587d997468f9494b755e0ea417f",
            "8c8f4f690e594d2cb6c169a4f7492032",
            "79f95bbbdca5476ca2ac15662d44e2a5"
          ]
        },
        "id": "3RsJ1zYzjCUr",
        "outputId": "339e5727-1e3e-4051-e4d3-8e792957a624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79736d1df5494d8880bf2d99bbd6ca50"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc82ca4eb72d41eb8bd8a7712a5d04aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "681cf563dcc14fc28c40f69e545bf276"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6a5b1bd467e4b08acbce7e23c189a30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1579370126794c72affb0ffbc59ac953"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36edf22181d941d2a60abb70b3ff874c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98a69bd54cc64117b4961161e17b6d44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store chat history for debugging\n",
        "chat_log = []\n",
        "\n",
        "# Define a smarter response function\n",
        "def respond(message):\n",
        "    prompt = f\"\"\"\n",
        "You are a helpful assistant for small business owners.\n",
        "\n",
        "Your job is to answer questions related to:\n",
        "- Business forecasting\n",
        "- Economic indicators (like inflation, unemployment, sales data)\n",
        "- Improving cash flow\n",
        "- Making data-driven decisions\n",
        "\n",
        "Always explain in plain English and give clear, helpful advice.\n",
        "\n",
        "Examples:\n",
        "Q: What are economic indicators?\n",
        "A: Economic indicators like inflation, consumer confidence, and employment rates help you anticipate changes in customer spending and business risks.\n",
        "\n",
        "Q: How can I improve cash flow?\n",
        "A: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste. These steps can increase your available cash.\n",
        "\n",
        "Now answer this question:\n",
        "\n",
        "Q: {message}\n",
        "A:\"\"\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=200)[0][\"generated_text\"].strip()\n",
        "\n",
        "    # Log this interaction\n",
        "    chat_log.append((message, response))\n",
        "\n",
        "    # Print entire chat history (for debugging in notebook)\n",
        "    print(\"\\n🧾 Chat Log:\")\n",
        "    for i, (q, a) in enumerate(chat_log, 1):\n",
        "        print(f\"{i}. Q: {q}\\n   A: {a}\\n{'-'*50}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about forecasting, economics, or small business strategy...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"📊 Small Business Forecasting Assistant\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "XlVCvteAjCSF",
        "outputId": "574b25f5-c82a-4408-b2cc-6e11fce7fd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f1f525f5ce25e4c121.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f1f525f5ce25e4c121.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chat Logs"
      ],
      "metadata": {
        "id": "-Vh58zEgtjuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧾 Chat Log\\n\" + \"=\" * 40)\n",
        "for i, (user_msg, bot_msg) in enumerate(chat_log, start=1):\n",
        "    print(f\"\\n🧑‍💼 User {i}: {user_msg}\")\n",
        "    print(f\"  🤖 Bot  {i}: {bot_msg}\")\n",
        "    # print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZvpBtMsjCP6",
        "outputId": "78fc0ac6-cc0e-400c-8b5d-53a6bfa90f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 Chat Log\n",
            "========================================\n",
            "\n",
            "🧑‍💼 User 1: hi can you help me optimize by small business?\n",
            "  🤖 Bot  1: Can you help me optimize by small business?\n",
            "\n",
            "🧑‍💼 User 2: how do i improve my cash flow?\n",
            "  🤖 Bot  2: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste. These steps can increase your available cash.\n",
            "\n",
            "🧑‍💼 User 3: What about forecasting? an i iprve my cash flow with more accurate  cash flow forecasting?\n",
            "  🤖 Bot  3: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste. These steps can increase your available cash.\n",
            "\n",
            "🧑‍💼 User 4: Can you teach me about econimic indicators?\n",
            "  🤖 Bot  4: Economic indicators like inflation, consumer confidence, and employment rates help you anticipate changes in customer spending and business risks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2 | Prompt 2 : Extensive Prompt\n",
        "\n",
        "\n",
        "### 🧠 Key Takeaway for Learning\n",
        "\n",
        "The most important things to understand here if you're learning how chatbots work:\n",
        "\n",
        "| 🔍 Concept | 💡 What to Learn |\n",
        "|-----------|------------------|\n",
        "| **System Prompt** | Shapes the assistant’s personality, domain, and rules. This is the *heart* of in-context learning. |\n",
        "| **Prompt Formatting** | Using `Q:` and `A:` makes it easier for the model to follow expected patterns. |\n",
        "| **Inference Parameters** | `max_new_tokens`, `temperature`, `top_k`, etc., control output length and creativity. |\n",
        "| **Pipeline Type** | You're using `text2text-generation`, which works well for task-based, formatted outputs. |\n",
        "| **User Interface** | Gradio is your frontend — it makes testing and deploying fast, easy, and interactive. |\n"
      ],
      "metadata": {
        "id": "SehIbwEZuZZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store chat history for debugging\n",
        "chat_log = []\n",
        "\n",
        "# Define a smarter response function with a detailed system prompt\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful, knowledgeable assistant for small business owners.\n",
        "\n",
        "Your role is to answer questions related to:\n",
        "- Business forecasting\n",
        "- Economic indicators (local, state, national)\n",
        "- Improving cash flow\n",
        "- Reducing uncertainty in business decisions\n",
        "- Data-driven strategy for small businesses\n",
        "\n",
        "You are trained using content from the Cashflow 4Cast blog, which covers:\n",
        "- Forecasting accuracy\n",
        "- Interpreting inflation, unemployment, and sales data\n",
        "- How local and national economics affect small businesses\n",
        "- How to reduce forecasting errors\n",
        "\n",
        "You specialize in:\n",
        "- Explaining economic indicators like CPI, consumer confidence, and retail sales\n",
        "- Helping owners interpret forecasting accuracy\n",
        "- Recommending how to adjust based on business conditions\n",
        "- Using real-world examples and simple language\n",
        "\n",
        "📌 Always keep your answers focused on business forecasting, cash flow, or economic strategy.\n",
        "📌 If a question is outside your domain (e.g. health, politics, personal advice), respond with:\n",
        "\n",
        "\"I'm here to help with forecasting, business planning, and data insights. Let me know how I can assist in that area!\"\n",
        "\n",
        "Answer clearly and concisely in plain English, avoid overly technical jargon, and always aim to educate and guide with a friendly tone.\n",
        "\n",
        "---\n",
        "\n",
        "Here are examples of how you answer:\n",
        "\n",
        "Q: What economic indicators matter most for small business?\n",
        "A: Key indicators include inflation (CPI), consumer confidence, retail sales, and local employment trends. These help you anticipate changes in customer spending and adjust your cash flow planning accordingly.\n",
        "\n",
        "Q: How accurate are forecasting models like Prophet or Excel?\n",
        "A: Traditional tools like Excel rely heavily on past averages. In contrast, machine learning models trained on local data can reduce forecasting errors by 50% or more, especially when accounting for current economic conditions.\n",
        "\n",
        "Q: What should I do if my forecast is off?\n",
        "A: Look for patterns — was there a spike in costs? Did a key economic indicator change? Adjust your assumptions and use shorter forecast windows (e.g. 30-day) for better agility.\n",
        "\n",
        "Q: Can you help me understand my recent drop in sales?\n",
        "A: Yes! Let’s explore recent local economic shifts — like employment, inflation, or consumer sentiment — and how they may be affecting your customers.\n",
        "\n",
        "---\n",
        "Always stay on-topic, helpful, and calm.\"\"\"\n",
        "\n",
        "def respond(message):\n",
        "    # Combine system prompt + user message\n",
        "    prompt = f\"{SYSTEM_PROMPT}\\n\\nQ: {message}\\nA:\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=300, temperature=0.7)[0][\"generated_text\"].strip()\n",
        "    chat_log.append((message, response))\n",
        "\n",
        "    # Print the chat log\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n🧾 Chat Log\\n\" + \"=\"*60)\n",
        "    for i, (q, a) in enumerate(chat_log, 1):\n",
        "        print(f\"\\n{i}. Q: {q}\\n   A: {a}\\n{'-'*50}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about forecasting, economics, or small business strategy...\"),\n",
        "    outputs=gr.Textbox(lines=6, label=\"Assistant Response\"),\n",
        "    title=\"📊 Small Business Forecasting Assistant\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "43KTdhAqx7tW",
        "outputId": "1fd17daf-0720-4d3b-8af8-ed3be25d3d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://77ec0a54daab550483.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://77ec0a54daab550483.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧾 Chat Log\\n\" + \"=\" * 40)\n",
        "for i, (user_msg, bot_msg) in enumerate(chat_log, start=1):\n",
        "    print(f\"\\n🧑‍💼 User {i}: {user_msg}\")\n",
        "    print(f\"  🤖 Bot  {i}: {bot_msg}\")\n",
        "    # print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB8zlCFbwuY9",
        "outputId": "2cb5b3f6-8cae-4979-936a-aeacfe449120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 Chat Log\n",
            "========================================\n",
            "\n",
            "🧑‍💼 User 1: hi can you help me optimize by small business?\n",
            "  🤖 Bot  1: I'm here to help with forecasting, business planning, and data insights. Let me know how I can assist in that area! ---\n",
            "\n",
            "🧑‍💼 User 2:  how do i improve my cash flow?\n",
            "\n",
            "  🤖 Bot  2: i'm here to help with forecasting, business planning, and data insights. Let me know how I can assist in that area.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##✅ Step 2: Add In-Context Few-Shot Examples\n",
        "\n",
        "\n",
        "You're performing what's called **prompt engineering** or **in-context learning**, which is a powerful way to *steer the model* without needing to retrain it.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 What’s Happening Behind the Scenes\n",
        "\n",
        "- When you send a message like  \n",
        "  > “What is CPI and why does it matter?”  \n",
        "  you're not just sending that alone.\n",
        "\n",
        "- You're sending the entire crafted prompt:\n",
        "\n",
        "```plaintext\n",
        "You are a helpful, friendly assistant who helps small business owners...\n",
        "[+ examples of great Q&A]\n",
        "Now answer this new question:\n",
        "Q: What is CPI and why does it matter?\n",
        "A:\n",
        "```\n",
        "\n",
        "- The model uses the **patterns** in your examples to generate a consistent, on-brand, and topic-aware response.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔍 Why This Works So Well\n",
        "\n",
        "✅ No fine-tuning required  \n",
        "✅ Super flexible — just update your examples  \n",
        "✅ You can A/B test prompts easily  \n",
        "✅ It's cheaper and faster than model training  \n",
        "✅ Perfect for early-stage prototypes or small business chatbots\n",
        "\n",
        "---\n",
        "\n",
        "If you're up for it, we can also:\n",
        "- Add **topic-aware branching** (e.g. different examples depending on whether it's about inflation or sales)\n",
        "- Store previous prompts to experiment with **prompt refinement**\n",
        "- Add a slider for *response length or helpfulness*\n",
        "\n",
        "💡 You nailed it — and you're thinking like an AI architect now.\n",
        "\n",
        "There’s a **spectrum** between **in-context learning** and **RAG**, and you’re absolutely right: if you keep feeding the model more and more context manually, you’re slowly building a *manual RAG system* without calling it that.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 In-Context Learning vs RAG\n",
        "\n",
        "| Feature | **In-Context Learning** | **RAG (Retrieval-Augmented Generation)** |\n",
        "|--------|-------------------------|-----------------------------------------|\n",
        "| 🔧 Setup | Handcrafted prompt with examples | Dynamic retrieval of relevant info |\n",
        "| 📦 Data | Static and hardcoded | Stored in an index (like FAISS) |\n",
        "| 📈 Scaling | Manual — hits token limit fast | Automatic — fetches only what’s needed |\n",
        "| 🔁 Adaptability | Rigid, needs manual updates | Flexible, can grow as your docs grow |\n",
        "| 💡 Use Case | Small fixed topics | Large, evolving knowledge bases |\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ So What Are the Limits of In-Context Learning?\n",
        "\n",
        "1. **Token Limits**  \n",
        "   Most models like `flan-t5-base` cap around 512–1024 tokens. You’ll run out of room fast if you add too many examples.\n",
        "\n",
        "2. **Relevance Weakens**  \n",
        "   The more you pack in, the harder it is for the model to focus. Unlike RAG, it won’t *retrieve* the best examples — it just reads all of them equally.\n",
        "\n",
        "3. **Hard to Update**  \n",
        "   If your blog changes or you want to update one fact, you have to manually rewrite your prompt examples.\n",
        "\n",
        "---\n",
        "\n",
        "## 🎯 Where RAG Begins to Shine\n",
        "\n",
        "RAG steps in **exactly when**:\n",
        "- You have too many examples to fit in context\n",
        "- Your content changes often\n",
        "- You want to scale without prompt clutter\n",
        "\n",
        "So yes — in-context learning is great for:\n",
        "> “Small, focused, handcrafted expertise.”\n",
        "\n",
        "RAG is great for:\n",
        "> “Scaleable, flexible knowledge bases.”\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ll3Ftr3Hhjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EXAMPLES = \"\"\"\n",
        "Q: What is CPI and why does it matter for my store?\n",
        "A: CPI stands for Consumer Price Index. It tracks how much prices are rising for everyday goods. If CPI is going up, it means inflation is rising — and your customers may start cutting back on spending. Watching CPI helps you plan for slower sales and adjust your pricing.\n",
        "\n",
        "Q: How can I tell if my forecasts are accurate?\n",
        "A: One good way is to look at your forecasting error. If your forecast says you’ll sell $10,000 but you only sell $8,000, your error is 20%. A well-tuned model should get you under 10% error most of the time.\n",
        "\n",
        "Q: What economic indicators are most useful?\n",
        "A: Start with local unemployment, consumer spending, and inflation. These show how confident your customers are and how much they can afford. If those numbers shift, your sales probably will too.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "AZDIDiQ0HN45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2 | Prompt 3"
      ],
      "metadata": {
        "id": "EXsZpYGKXNqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(message):\n",
        "    prompt = f\"\"\"Q: What are economic indicators?\n",
        "A: Economic indicators like inflation, consumer confidence, and employment rates help you anticipate changes in customer spending.\n",
        "\n",
        "Q: How can I improve cash flow?\n",
        "A: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste.\n",
        "\n",
        "Q: How do forecasting models work?\n",
        "A: Traditional tools like Excel rely on past data averages. More advanced models use machine learning and local trends to improve forecast accuracy.\n",
        "\n",
        "Q: {message}\n",
        "A:\"\"\"\n",
        "\n",
        "    response = chatbot(prompt, max_new_tokens=250, temperature=0.7)[0][\"generated_text\"].strip()\n",
        "    chat_log.append((message, response))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n🧾 Chat Log\\n\" + \"=\"*60)\n",
        "    for i, (q, a) in enumerate(chat_log, 1):\n",
        "        print(f\"\\n{i}. Q: {q}\\n   A: {a}\\n{'-'*50}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "# Create the Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=respond,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask about forecasting, economics, or small business strategy...\"),\n",
        "    outputs=gr.Textbox(lines=6, label=\"Assistant Response\"),\n",
        "    title=\"📊 Small Business Forecasting Assistant\"\n",
        ")\n",
        "\n",
        "# Launch the chatbot\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "cBOiEgJ7Z6m8",
        "outputId": "6f1b55ef-4547-428d-c729-c8e53a8cbd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://4427f9d365f12538ee.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4427f9d365f12538ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🧾 Chat Log\\n\" + \"=\" * 40)\n",
        "for i, (user_msg, bot_msg) in enumerate(chat_log, start=1):\n",
        "    print(f\"\\n🧑‍💼 User {i}: {user_msg}\")\n",
        "    print(f\"  🤖 Bot  {i}: {bot_msg}\")\n",
        "    # print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLijSQjHHNr4",
        "outputId": "a860eee6-d96d-4921-90ab-8a00d976802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧾 Chat Log\n",
            "========================================\n",
            "\n",
            "🧑‍💼 User 1: hi can you help me optimize by small business?\n",
            "  🤖 Bot  1: I'm here to help with forecasting, business planning, and data insights. Let me know how I can assist in that area! ---\n",
            "\n",
            "🧑‍💼 User 2:  how do i improve my cash flow?\n",
            "\n",
            "  🤖 Bot  2: i'm here to help with forecasting, business planning, and data insights. Let me know how I can assist in that area.\n",
            "\n",
            "🧑‍💼 User 3: hi can you help me optimize by small business?\n",
            "  🤖 Bot  3: Small business is a category of business.\n",
            "\n",
            "🧑‍💼 User 4: how do i improve my cash flow?\n",
            "  🤖 Bot  4: Track expenses weekly, forecast sales 30 days out, and reduce inventory waste.\n",
            "\n",
            "🧑‍💼 User 5: What about forecasting? an i iprve my cash flow with more accurate  cash flow forecasting?\n",
            "  🤖 Bot  5: Traditional tools like Excel rely on past data averages. More advanced models use machine learning and local trends to improve forecast accuracy.\n",
            "\n",
            "🧑‍💼 User 6: Can you teach me about econimic indicators?\n",
            "  🤖 Bot  6: Economic indicators like inflation, consumer confidence, and employment rates help you anticipate changes in customer spending.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fUwKXBxIaCZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SrPkVNLaCVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D20mWgfoaCQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsqwFpdcaCNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Memory Clean Up & Remove Widgets from Notebook to Save to Github"
      ],
      "metadata": {
        "id": "ejLijtALBRBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_054_RAG_CahsFlow4Cast_Chatbot.ipynb\"\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# 1. Remove widgets from notebook-level metadata\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "    print(\"✅ Removed notebook-level 'widgets' metadata.\")\n",
        "\n",
        "# 2. Remove widgets from each cell's metadata\n",
        "for i, cell in enumerate(nb.get(\"cells\", [])):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "        print(f\"✅ Removed 'widgets' from cell {i}\")\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"✅ Notebook deeply cleaned. Try uploading to GitHub again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgU1QYqZBGGf",
        "outputId": "8fb5e0eb-045e-4a7d-9c1a-4360191d7421"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Notebook deeply cleaned. Try uploading to GitHub again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5mRtUYgBGDZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}