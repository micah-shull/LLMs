{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOyb1Ox6uuNafrAVIUVIqa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_048_huggingFace_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install transformers\n",
        "# !pip install huggingface_hub"
      ],
      "metadata": {
        "id": "G4dClqE9kmbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3406d2a6-1bff-48c7-b986-3d8f0bb8d0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJWGNiI8ipCg",
        "outputId": "7a3cf877-fe86-42c7-bc6b-271ea2a64b1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.999873161315918}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9998546838760376}]\n",
            "[{'label': 'NEGATIVE', 'score': 0.9937392473220825}]\n",
            "[{'label': 'NEGATIVE', 'score': 0.9991015195846558}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9790390133857727}]\n",
            "[{'label': 'POSITIVE', 'score': 0.967018723487854}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9966733455657959}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*The secret.*\")\n",
        "\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n",
        "\n",
        "# Create your pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\",\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "result = classifier(\"I'm really enjoying Hugging Face with a token!\")\n",
        "print(result)\n",
        "\n",
        "# Run it on some text\n",
        "result = classifier(\"I'm really enjoying learning Hugging Face!\")\n",
        "print(result)\n",
        "result = classifier(\"I hate jogging!\")\n",
        "print(result)\n",
        "result = classifier(\"I dont' care either way\")\n",
        "print(result)\n",
        "result = classifier(\"meh\")\n",
        "print(result)\n",
        "result = classifier(\"whatever you say.\")\n",
        "print(result)\n",
        "result = classifier(\"you see awfully sure of yourself\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§î Why Is the Model So Confident?\n",
        "\n",
        "You're seeing results like:\n",
        "```python\n",
        "[{'label': 'NEGATIVE', 'score': 0.999}]\n",
        "[{'label': 'POSITIVE', 'score': 0.996}]\n",
        "```\n",
        "\n",
        "Even for **neutral or sarcastic text** like:\n",
        "- \"meh\"\n",
        "- \"whatever you say\"\n",
        "- \"you seem awfully sure of yourself\"\n",
        "\n",
        "This seems... overly confident, right? Here's why üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What's Actually Happening\n",
        "\n",
        "### 1. **The Model Was Trained for Binary Classification**\n",
        "The model you're using:\n",
        "```python\n",
        "\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "```\n",
        "‚Ä¶was trained on the **Stanford Sentiment Treebank v2 (SST-2)** dataset.\n",
        "\n",
        "**SST-2 only includes:**\n",
        "- `POSITIVE`\n",
        "- `NEGATIVE`\n",
        "\n",
        "There‚Äôs **no \"neutral\" class**, no sarcasm, no subtlety, no \"mixed feelings.\"  \n",
        "So the model **must choose** between just two buckets‚Äîeven when the text is ambiguous or unopinionated.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Softmax Always Picks a Winner**\n",
        "The model outputs **raw scores** (called *logits*) for each label.\n",
        "\n",
        "Then it applies the **softmax function**, which converts logits into **probabilities that always sum to 1.0**.\n",
        "\n",
        "> Even if the model is unsure, it **still picks the \"most likely\" class** with a high confidence value‚Äîespecially in a 2-class scenario.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Pretraining Bias + Overfitting**\n",
        "LLMs like BERT or DistilBERT are pretrained on huge corpora (Wikipedia, books, etc.) and then **fine-tuned** on small datasets like SST-2.\n",
        "\n",
        "Fine-tuning on a limited dataset with polarized opinions can cause:\n",
        "- Overconfidence on short or vague sentences\n",
        "- Misclassification of sarcasm or nuance\n",
        "- Poor generalization to real-world tones (like \"meh\" or \"whatever\")\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Try This Yourself\n",
        "\n",
        "Let‚Äôs inspect the **raw logits** instead of the processed output:\n",
        "\n",
        "This will show you how it always leans toward a class even when unsure.\n",
        "\n",
        "This is a key step toward **understanding how LLMs actually ‚Äúthink‚Äù** under the hood. Let‚Äôs unpack the results of your experiment and why they matter.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What You Just Did:\n",
        "\n",
        "You bypassed the Hugging Face `pipeline` abstraction and used the **raw model and tokenizer directly** to:\n",
        "\n",
        "1. Tokenize the input (`\"meh\"`)\n",
        "2. Run it through the model\n",
        "3. See the raw output scores (called **logits**)\n",
        "4. Apply `softmax` to get **probabilities**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What to Look For\n",
        "\n",
        "### üîπ **1. Logits**:\n",
        "```python\n",
        "tensor([[-1.8109,  2.0331]])\n",
        "```\n",
        "\n",
        "These are the **raw, unnormalized outputs** from the model for each class:\n",
        "- The **first number** corresponds to the model‚Äôs score for the **negative class**\n",
        "- The **second number** is for the **positive class**\n",
        "\n",
        "> Larger numbers = more confident prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **2. Probabilities (Softmax Applied):**\n",
        "```python\n",
        "tensor([[0.0210, 0.9790]])\n",
        "```\n",
        "\n",
        "This means:\n",
        "- 2.1% confidence the text is **negative**\n",
        "- 97.9% confidence the text is **positive**\n",
        "\n",
        "ü§î Even though you typed `\"meh\"` (neutral/indifferent), the model had to **choose** between two labels (POSITIVE or NEGATIVE) and decided it's positive ‚Äî probably because `\"meh\"` isn‚Äôt clearly negative in its limited training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Why This Is Insightful\n",
        "\n",
        "- You now see **how the model scores each class** ‚Äî not just the final label.\n",
        "- You understand **why it looks \"overconfident\"** (softmax forces the highest score to look very strong in a binary setting).\n",
        "- You‚Äôre **not limited to trusting pipelines** ‚Äî you can interpret the actual math behind it!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SL-1ibrzi3_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# text = \"meh\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(**inputs)\n",
        "# logits = outputs.logits\n",
        "# probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "# print(\"Logits:\", logits)\n",
        "# print(\"Probabilities:\", probs)\n",
        "\n",
        "texts = [\"meh\", \"whatever\", \"love it\", \"hate it\", \"this is fine\", \"wow, just wow\", \"I guess it's okay\"]\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Logits: {logits}\")\n",
        "    print(f\"Probabilities: {probs}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcXvll7mi4dK",
        "outputId": "9543ffbb-e6e5-421e-9b99-0682391060d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: meh\n",
            "Logits: tensor([[-1.8109,  2.0331]])\n",
            "Probabilities: tensor([[0.0210, 0.9790]])\n",
            "\n",
            "Text: whatever\n",
            "Logits: tensor([[ 1.8082, -1.6105]])\n",
            "Probabilities: tensor([[0.9683, 0.0317]])\n",
            "\n",
            "Text: love it\n",
            "Logits: tensor([[-4.3101,  4.6796]])\n",
            "Probabilities: tensor([[1.2467e-04, 9.9988e-01]])\n",
            "\n",
            "Text: hate it\n",
            "Logits: tensor([[ 4.5873, -3.6634]])\n",
            "Probabilities: tensor([[9.9974e-01, 2.6101e-04]])\n",
            "\n",
            "Text: this is fine\n",
            "Logits: tensor([[-4.2480,  4.5880]])\n",
            "Probabilities: tensor([[1.4538e-04, 9.9985e-01]])\n",
            "\n",
            "Text: wow, just wow\n",
            "Logits: tensor([[-3.9157,  4.2371]])\n",
            "Probabilities: tensor([[2.8783e-04, 9.9971e-01]])\n",
            "\n",
            "Text: I guess it's okay\n",
            "Logits: tensor([[-3.8674,  4.2151]])\n",
            "Probabilities: tensor([[3.0880e-04, 9.9969e-01]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ HuggingFace Uses PyTorch Under the Hood\n",
        "\n",
        "Hugging Face models are built on top of either:\n",
        "- **PyTorch** (`torch.nn.Module`)\n",
        "- or **TensorFlow** (`tf.keras.Model`)\n",
        "\n",
        "When you ran:\n",
        "```python\n",
        "AutoModelForSequenceClassification.from_pretrained(...)\n",
        "```\n",
        "You were loading the **PyTorch version** of the model by default.\n",
        "\n",
        "This is why your tensors look like this:\n",
        "```python\n",
        "tensor([[-1.8109,  2.0331]])\n",
        "```\n",
        "...and you used:\n",
        "```python\n",
        "torch.nn.functional.softmax(...)\n",
        "```\n",
        "\n",
        "You‚Äôre fully inside the **PyTorch workflow** here.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ You're Bypassing the Pipeline\n",
        "\n",
        "When you use:\n",
        "```python\n",
        "pipeline(\"sentiment-analysis\")\n",
        "```\n",
        "\n",
        "You‚Äôre using a **wrapper** that does all of this:\n",
        "1. Loads the tokenizer\n",
        "2. Loads the model\n",
        "3. Tokenizes your text\n",
        "4. Runs it through the model\n",
        "5. Applies softmax\n",
        "6. Translates scores into human-readable labels (`\"POSITIVE\"`, `\"NEGATIVE\"`)\n",
        "7. Formats the result into a Python-friendly output\n",
        "\n",
        "You just **peeled back that wrapper** and exposed:\n",
        "\n",
        "- The **raw logits** (before softmax)\n",
        "- The **manual softmax application**\n",
        "- The actual **tensor outputs** the model gives you\n",
        "\n",
        "This is **exactly what the pipeline was doing for you**, just hidden behind that 1-liner!\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Is Important\n",
        "\n",
        "- You now know how to **debug**, **customize**, or **interpret** model outputs more deeply.\n",
        "- You‚Äôll be able to **build custom logic**, like multi-label classification, thresholding, or even modifying logits directly.\n",
        "- You‚Äôve taken a huge step toward understanding how to **train or fine-tune your own models** later.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rd7gE_CooiqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ What You Can Do About It\n",
        "\n",
        "- **Use a multi-class sentiment model** with a neutral category  \n",
        "  ‚Üí e.g., `cardiffnlp/twitter-roberta-base-sentiment`  \n",
        "  (has Positive, Neutral, Negative)\n",
        "\n",
        "- **Try zero-shot classification** if you want more nuanced control:\n",
        "```python\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "result = classifier(\n",
        "    \"meh\",\n",
        "    candidate_labels=[\"positive\", \"negative\", \"neutral\", \"sarcastic\"]\n",
        ")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "- **Train your own model** on more subtle or domain-specific sentiment examples."
      ],
      "metadata": {
        "id": "lxEpgw9djWVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! You're running the right structure‚Äîyou‚Äôve got a loop over multiple texts, and you're examining both the logits and probabilities. üéØ\n",
        "\n",
        "Now, let‚Äôs switch that to a **3-class sentiment model** (positive, neutral, negative) so we can capture **more nuance** for ambiguous texts like `\"meh\"` or `\"whatever\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step-by-Step: Use a 3-Class Sentiment Model\n",
        "\n",
        "We'll use the popular:\n",
        "> `cardiffnlp/twitter-roberta-base-sentiment`  \n",
        "A RoBERTa model trained on Twitter data with 3 sentiment labels:\n",
        "- `LABEL_0`: Negative  \n",
        "- `LABEL_1`: Neutral  \n",
        "- `LABEL_2`: Positive\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è This model uses a slightly different tokenizer and label mapping, so let‚Äôs update your code to:\n",
        "\n",
        "‚úÖ Use the correct tokenizer  \n",
        "‚úÖ Apply softmax  \n",
        "‚úÖ Map logits to human-friendly labels\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What to Look For\n",
        "\n",
        "Now, instead of **forcing everything into POSITIVE or NEGATIVE**, you‚Äôll start seeing outputs like:\n",
        "\n",
        "- `\"meh\"` ‚Üí **neutral**\n",
        "- `\"love it\"` ‚Üí **positive**\n",
        "- `\"hate it\"` ‚Üí **negative**\n",
        "- `\"I guess it's okay\"` ‚Üí **neutral or positive (low confidence)**\n",
        "\n",
        "This gives you a **much better understanding of nuanced or lukewarm statements**, which are common in real-world feedback.\n"
      ],
      "metadata": {
        "id": "HbtOV5A8uMst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 3-class sentiment model from Cardiff NLP\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Label mapping (from model card)\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\"meh\", \"whatever\", \"love it\", \"hate it\", \"this is fine\", \"wow, just wow\", \"I guess it's okay\"]\n",
        "\n",
        "# Inference loop\n",
        "for text in texts:\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs_np = probs.numpy()[0]\n",
        "\n",
        "    # Map probabilities to labels\n",
        "    label_probs = {labels[i]: float(probs_np[i]) for i in range(len(labels))}\n",
        "\n",
        "    # Find top prediction\n",
        "    top_label = labels[np.argmax(probs_np)]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"Probabilities:\", label_probs)\n",
        "    print(\"Predicted Sentiment:\", top_label)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802,
          "referenced_widgets": [
            "7768a9b7f3c043a9b39e37fdf21eeb56",
            "9aa1883dcc2045b9886e9b29ea5c815a",
            "21dac1dee8074fba9b51c075c450aebd",
            "5f62aa9b38c142559676f692582942e0",
            "64dbe56d07ff46d9b0d018cc278ce19f",
            "f7e7488d794c4c9b952ad3f8de0272f8",
            "88345c71448a4a32aa81bed88082a793",
            "16028fd0566246738aad6be14541ba97",
            "94222c123ea34ae08cf480eac029acbf",
            "6a022f79468d40e3b05eb0cb92b09a95",
            "f4ea16a6e2584891b6bd991fea3ad9c8",
            "5103b17573984488a24c8583d84cfefb",
            "660ec96e3db44e1bb891dae70086a6bf",
            "c2bd98d997e84d5a9b271ac17f4eec69",
            "704072e1a0be44458c75b79b25d93c6b",
            "c99f3141398c408b89a7077082befe66",
            "8b90e8db240e46819f98307615e409c4",
            "02f732ad563b4b62bbca1ced0324fe11",
            "48195b165c814f02ae1a02b9dce5e0b6",
            "16a999124de140a0b0248bc3dbcaf8c9",
            "6c725d94d6b140ecb93320c3e8e3f432",
            "633a12262b4f4a8d88419b8860bb576d",
            "01b9c1399bc443deb824fbf52d71b6d5",
            "f6c5f95094e74de2b1aa7da4331d45d0",
            "0320700d932f42eca25210cf4de520bd",
            "31ab9d787ca148e194ceee0ad2e8072b",
            "43fa982ba0af496badd59190fcdc1014",
            "b92a8b688daa4c329d779a114a92fb3d",
            "c814d663fdd9434c9977cf328e724084",
            "47d762a6e3df4ceeab3a4f882fb86404",
            "f95d748ee6cd4bc8b43bd924fe2f5bab",
            "5260c948e83445f985f97215c22dfd07",
            "f1ebbc20743f4921968874704e5b6a8e",
            "339677042a0f480c836fd00c31b4f12a",
            "d1b19c770e8547cdb97d66d5ca867fd0",
            "105ed724876a48a5a4820d60fd6dfa49",
            "4cfb5c62537748239de592b9791d7942",
            "f82dca97cfe244fb8c49c15e97e17904",
            "e2f5af78c46d4459a5ed22737851168c",
            "4aaa6a1efe8a4139bd5f05e75f6e94e5",
            "bad43ecf6649409a8ffb404621baa402",
            "b9425af7acc94be78793053f7d9c0e0c",
            "e2e5ba4a2e114138b2ad4d327a88d369",
            "55959d8e12f14c4b8f4ff762d388c4f7",
            "c843490f2540462db05ad90d3e084c54",
            "6ecc73d70a0d48eba01f593658ee8b19",
            "34a57323e3624ab683d291824783ee3b",
            "2ca08ec320114e2c82719e77df4b189d",
            "685c23ba9f5747519bce3e8f73cff0a7",
            "e961fb5366b349c29baa1fa05d4d7c98",
            "c5478598927d44248c22969b39344c53",
            "4386222dddd147388fa03855b78fb4b2",
            "c46d448638ab4f41ad9bb333eb12ab95",
            "a1658730bf9f401db7b7bc36660d0dec",
            "2092a62469a94a82a535d56fa93037cb",
            "01569f35f8b54f0d842bfc6feb87bbc6",
            "a278b9f3ff8e4c71ae68afc1a574bbc3",
            "4f08095200f142858d67cc44418e004f",
            "6f371910420e44d0a9af8f0534ed0d80",
            "c6e141f3a8c64bfab401d4835521e1c6",
            "66efe3c864c94e0593eed2a49cbfc592",
            "1d147bc1ff214f008a402cdf88abe036",
            "b2ef2a446a3c438c80866110fbc47a57",
            "4a2a8a7036b34811b9d1bdb508862ecf",
            "b8c2a69f7cb94043b13a48b20e075212",
            "e4487b40da8f4af3a8686caa47e7a0c7"
          ]
        },
        "id": "_6yA5jRttuh8",
        "outputId": "adb2b7f2-7202-4156-80cc-86748acddef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7768a9b7f3c043a9b39e37fdf21eeb56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5103b17573984488a24c8583d84cfefb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01b9c1399bc443deb824fbf52d71b6d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "339677042a0f480c836fd00c31b4f12a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c843490f2540462db05ad90d3e084c54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01569f35f8b54f0d842bfc6feb87bbc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: meh\n",
            "Probabilities: {'negative': 0.23177234828472137, 'neutral': 0.5303865075111389, 'positive': 0.2378411442041397}\n",
            "Predicted Sentiment: neutral\n",
            "----------------------------------------\n",
            "Text: whatever\n",
            "Probabilities: {'negative': 0.28891637921333313, 'neutral': 0.5565615296363831, 'positive': 0.1545221358537674}\n",
            "Predicted Sentiment: neutral\n",
            "----------------------------------------\n",
            "Text: love it\n",
            "Probabilities: {'negative': 0.024173742160201073, 'neutral': 0.07955925911664963, 'positive': 0.8962669968605042}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: hate it\n",
            "Probabilities: {'negative': 0.8896681666374207, 'neutral': 0.09059063345193863, 'positive': 0.019741209223866463}\n",
            "Predicted Sentiment: negative\n",
            "----------------------------------------\n",
            "Text: this is fine\n",
            "Probabilities: {'negative': 0.01242265198379755, 'neutral': 0.14099746942520142, 'positive': 0.846579909324646}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: wow, just wow\n",
            "Probabilities: {'negative': 0.0195575300604105, 'neutral': 0.17196960747241974, 'positive': 0.8084728717803955}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: I guess it's okay\n",
            "Probabilities: {'negative': 0.020886369049549103, 'neutral': 0.30712148547172546, 'positive': 0.6719921827316284}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Your realization is **exactly right**:\n",
        "\n",
        "> üí° *‚ÄúThere is a lot to understand about the model (and pipeline) you choose to match it to the project you are working on.‚Äù*\n",
        "\n",
        "This is **core to working with Hugging Face effectively** ‚Äî the right model + task + dataset = success. Choosing the wrong one? You get poor results, even if the model is state-of-the-art.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ High-Level Framework for Exploring Hugging Face\n",
        "\n",
        "Here‚Äôs a **structured map** of what‚Äôs available and how to think about it.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± 1. **Pipelines = High-Level Tasks**\n",
        "\n",
        "These are the main categories of work you can do. Each one is tied to a model type behind the scenes.\n",
        "\n",
        "| Pipeline Task               | Description                                           | Example Use Case                              |\n",
        "|----------------------------|-------------------------------------------------------|------------------------------------------------|\n",
        "| `\"sentiment-analysis\"`     | Classify text as positive/negative (or neutral)       | Product reviews, user feedback                |\n",
        "| `\"text-classification\"`    | General category prediction                           | Spam detection, topic tagging                 |\n",
        "| `\"zero-shot-classification\"` | Classify into your own labels (no training required) | Customer intent, content moderation           |\n",
        "| `\"text-generation\"`        | Generate coherent text completions                    | Chatbots, story generation                    |\n",
        "| `\"summarization\"`          | Shorten long text into concise summaries              | News, reports, articles                       |\n",
        "| `\"translation_xx_to_yy\"`   | Translate between languages                           | English ‚Üí French                              |\n",
        "| `\"question-answering\"`     | Answer a question given a passage                     | Customer support, document search             |\n",
        "| `\"fill-mask\"`              | Predict masked word(s)                                | Cloze tasks, language model probing           |\n",
        "| `\"ner\"`                    | Named Entity Recognition (people, places, orgs)       | Info extraction from documents                |\n",
        "| `\"conversational\"`         | Dialogue-style interaction                            | Simple chatbot interaction                    |\n",
        "| `\"image-classification\"`   | Classify images                                       | Object detection, visual tagging              |\n",
        "| `\"audio-classification\"`   | Classify sound files                                  | Music genre, speaker identification           |\n",
        "| `\"automatic-speech-recognition\"` | Convert speech to text                       | Voice transcription, meeting notes            |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 2. **Model Types = Underlying Architectures**\n",
        "\n",
        "The model you choose inside the pipeline matters!\n",
        "\n",
        "| Architecture         | Known For                               | Strengths                                     | Example Models |\n",
        "|----------------------|------------------------------------------|-----------------------------------------------|----------------|\n",
        "| BERT / DistilBERT    | Bidirectional transformers               | Classification, QA, embeddings                | `bert-base-uncased` |\n",
        "| RoBERTa              | Robust BERT variant                      | Sentiment, NER, classification                | `roberta-base`, `cardiffnlp/twitter-roberta-base-sentiment` |\n",
        "| GPT / GPT-2          | Text generation (unidirectional)         | Generating coherent text                     | `gpt2`         |\n",
        "| T5 / BART            | Sequence-to-sequence (text2text) models  | Translation, summarization, QA               | `t5-small`, `facebook/bart-large-cnn` |\n",
        "| XLNet                | General language modeling                | Fill-mask and classification                 | `xlnet-base-cased` |\n",
        "| Whisper              | Audio ‚Üí Text                             | Speech recognition                           | `openai/whisper-base` |\n",
        "| ViT (Vision Transformer) | Image classification                | Computer vision tasks                        | `google/vit-base-patch16-224` |\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 3. **Datasets Matter Too**\n",
        "\n",
        "You also want a model **trained on data similar to your task**:\n",
        "\n",
        "| Dataset              | Domain                        | Impact on Model Behavior                       |\n",
        "|----------------------|-------------------------------|------------------------------------------------|\n",
        "| SST-2                | Movie reviews (binary)        | Overconfident positive/negative predictions   |\n",
        "| TweetEval            | Tweets (multi-label, sentiment, hate speech) | Real-world language, emojis, slang       |\n",
        "| CNN/DailyMail        | News articles                 | Great for summarization                       |\n",
        "| MultiNLI             | Textual entailment (inference) | Used for zero-shot classification             |\n",
        "| SQuAD                | Question answering             | Factual answer extraction                     |\n",
        "\n",
        "---\n",
        "\n",
        "### üõ† 4. **Use This Checklist to Match Project to Model**\n",
        "\n",
        "| Step | Question                                               | Example                                          |\n",
        "|------|--------------------------------------------------------|--------------------------------------------------|\n",
        "| ‚úÖ 1 | What is the task?                                       | \"I want to summarize support tickets.\"          |\n",
        "| ‚úÖ 2 | Is there a pipeline that matches that task?             | ‚Üí `\"summarization\"`                             |\n",
        "| ‚úÖ 3 | What domain is your data in?                            | Tech support, so look for domain-tuned models   |\n",
        "| ‚úÖ 4 | Do you need binary, multi-class, or zero-shot labels?   | Sentiment with neutral = use a 3-class model    |\n",
        "| ‚úÖ 5 | Does the model size fit your hardware?                  | `distilbert` vs `bert-large`                    |\n",
        "| ‚úÖ 6 | Do you want to fine-tune or use off-the-shelf?          | Off-the-shelf = pipeline                          |\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Explore Hugging Face with Purpose\n",
        "\n",
        "You can structure your exploration like this:\n",
        "\n",
        "| Goal                          | Pipeline         | Suggested Model Example                             |\n",
        "|-------------------------------|------------------|------------------------------------------------------|\n",
        "| Understand tone               | `sentiment-analysis` | `cardiffnlp/twitter-roberta-base-sentiment`      |\n",
        "| Build a chatbot               | `text-generation`, `conversational` | `gpt2`, `DialoGPT`          |\n",
        "| Classify open-ended input     | `zero-shot-classification` | `facebook/bart-large-mnli`             |\n",
        "| Translate content             | `translation_en_to_fr` | `Helsinki-NLP/opus-mt-en-fr`           |\n",
        "| Extract key info              | `ner`             | `dbmdz/bert-large-cased-conll03-english`            |\n",
        "| Build a summarizer            | `summarization`   | `facebook/bart-large-cnn`, `t5-small`               |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6ss8QdZEvpuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# clean up cache and memory\n",
        "def clear_memory(*var_names):\n",
        "    for name in var_names:\n",
        "        if name in globals():\n",
        "            del globals()[name]\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Cleared memory and freed GPU cache.\")\n",
        "\n",
        "clear_memory('inputs', 'outputs', 'model')"
      ],
      "metadata": {
        "id": "yduXVfgbtuel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOFipTvEtuZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTSoeJlrtuWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove Widgets from Notebook to save to Github"
      ],
      "metadata": {
        "id": "iSLP6mQ6nVJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your current notebook file (adjust if different)\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_048_huggingFace_SentimentAnalysis.ipynb\"\n",
        "\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove the widget metadata if it exists\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook metadata cleaned. Try saving to GitHub again.\")\n"
      ],
      "metadata": {
        "id": "UILjP50ojZBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean up cache and memory\n",
        "del inputs, outputs, model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "V7XcFvhMlzQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}