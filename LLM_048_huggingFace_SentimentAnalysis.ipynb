{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9GRxDhwhJSA5y84L7hduV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LLMs/blob/main/LLM_048_huggingFace_SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-dotenv\n",
        "# !pip install transformers\n",
        "# !pip install huggingface_hub"
      ],
      "metadata": {
        "id": "G4dClqE9kmbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJWGNiI8ipCg",
        "outputId": "7b61ca0b-6133-48e6-84dd-588c50a01cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'POSITIVE', 'score': 0.999873161315918}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9998546838760376}]\n",
            "[{'label': 'NEGATIVE', 'score': 0.9937392473220825}]\n",
            "[{'label': 'NEGATIVE', 'score': 0.9991015195846558}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9790390133857727}]\n",
            "[{'label': 'POSITIVE', 'score': 0.967018723487854}]\n",
            "[{'label': 'POSITIVE', 'score': 0.9966733455657959}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(\"/content/HUGGINGFACE_HUB_TOKEN.env\")\n",
        "\n",
        "# Login using the token\n",
        "login(token=os.environ[\"HUGGINGFACE_HUB_TOKEN\"])\n",
        "\n",
        "# Create your pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\",\n",
        "                      model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "result = classifier(\"I'm really enjoying Hugging Face with a token!\")\n",
        "print(result)\n",
        "\n",
        "# Run it on some text\n",
        "result = classifier(\"I'm really enjoying learning Hugging Face!\")\n",
        "print(result)\n",
        "result = classifier(\"I hate jogging!\")\n",
        "print(result)\n",
        "result = classifier(\"I dont' care either way\")\n",
        "print(result)\n",
        "result = classifier(\"meh\")\n",
        "print(result)\n",
        "result = classifier(\"whatever you say.\")\n",
        "print(result)\n",
        "result = classifier(\"you see awfully sure of yourself\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§î Why Is the Model So Confident?\n",
        "\n",
        "You're seeing results like:\n",
        "```python\n",
        "[{'label': 'NEGATIVE', 'score': 0.999}]\n",
        "[{'label': 'POSITIVE', 'score': 0.996}]\n",
        "```\n",
        "\n",
        "Even for **neutral or sarcastic text** like:\n",
        "- \"meh\"\n",
        "- \"whatever you say\"\n",
        "- \"you seem awfully sure of yourself\"\n",
        "\n",
        "This seems... overly confident, right? Here's why üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What's Actually Happening\n",
        "\n",
        "### 1. **The Model Was Trained for Binary Classification**\n",
        "The model you're using:\n",
        "```python\n",
        "\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "```\n",
        "‚Ä¶was trained on the **Stanford Sentiment Treebank v2 (SST-2)** dataset.\n",
        "\n",
        "**SST-2 only includes:**\n",
        "- `POSITIVE`\n",
        "- `NEGATIVE`\n",
        "\n",
        "There‚Äôs **no \"neutral\" class**, no sarcasm, no subtlety, no \"mixed feelings.\"  \n",
        "So the model **must choose** between just two buckets‚Äîeven when the text is ambiguous or unopinionated.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Softmax Always Picks a Winner**\n",
        "The model outputs **raw scores** (called *logits*) for each label.\n",
        "\n",
        "Then it applies the **softmax function**, which converts logits into **probabilities that always sum to 1.0**.\n",
        "\n",
        "> Even if the model is unsure, it **still picks the \"most likely\" class** with a high confidence value‚Äîespecially in a 2-class scenario.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Pretraining Bias + Overfitting**\n",
        "LLMs like BERT or DistilBERT are pretrained on huge corpora (Wikipedia, books, etc.) and then **fine-tuned** on small datasets like SST-2.\n",
        "\n",
        "Fine-tuning on a limited dataset with polarized opinions can cause:\n",
        "- Overconfidence on short or vague sentences\n",
        "- Misclassification of sarcasm or nuance\n",
        "- Poor generalization to real-world tones (like \"meh\" or \"whatever\")\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Try This Yourself\n",
        "\n",
        "Let‚Äôs inspect the **raw logits** instead of the processed output:\n",
        "\n",
        "This will show you how it always leans toward a class even when unsure.\n",
        "\n",
        "This is a key step toward **understanding how LLMs actually ‚Äúthink‚Äù** under the hood. Let‚Äôs unpack the results of your experiment and why they matter.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What You Just Did:\n",
        "\n",
        "You bypassed the Hugging Face `pipeline` abstraction and used the **raw model and tokenizer directly** to:\n",
        "\n",
        "1. Tokenize the input (`\"meh\"`)\n",
        "2. Run it through the model\n",
        "3. See the raw output scores (called **logits**)\n",
        "4. Apply `softmax` to get **probabilities**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç What to Look For\n",
        "\n",
        "### üîπ **1. Logits**:\n",
        "```python\n",
        "tensor([[-1.8109,  2.0331]])\n",
        "```\n",
        "\n",
        "These are the **raw, unnormalized outputs** from the model for each class:\n",
        "- The **first number** corresponds to the model‚Äôs score for the **negative class**\n",
        "- The **second number** is for the **positive class**\n",
        "\n",
        "> Larger numbers = more confident prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **2. Probabilities (Softmax Applied):**\n",
        "```python\n",
        "tensor([[0.0210, 0.9790]])\n",
        "```\n",
        "\n",
        "This means:\n",
        "- 2.1% confidence the text is **negative**\n",
        "- 97.9% confidence the text is **positive**\n",
        "\n",
        "ü§î Even though you typed `\"meh\"` (neutral/indifferent), the model had to **choose** between two labels (POSITIVE or NEGATIVE) and decided it's positive ‚Äî probably because `\"meh\"` isn‚Äôt clearly negative in its limited training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üìò Why This Is Insightful\n",
        "\n",
        "- You now see **how the model scores each class** ‚Äî not just the final label.\n",
        "- You understand **why it looks \"overconfident\"** (softmax forces the highest score to look very strong in a binary setting).\n",
        "- You‚Äôre **not limited to trusting pipelines** ‚Äî you can interpret the actual math behind it!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SL-1ibrzi3_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# text = \"meh\"\n",
        "# inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "# with torch.no_grad():\n",
        "#     outputs = model(**inputs)\n",
        "# logits = outputs.logits\n",
        "# probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "# print(\"Logits:\", logits)\n",
        "# print(\"Probabilities:\", probs)\n",
        "\n",
        "texts = [\"meh\", \"whatever\", \"love it\", \"hate it\", \"this is fine\", \"wow, just wow\", \"I guess it's okay\"]\n",
        "for text in texts:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Logits: {logits}\")\n",
        "    print(f\"Probabilities: {probs}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcXvll7mi4dK",
        "outputId": "d9ea8da0-785b-4325-9a97-d1f3c0ecacd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: meh\n",
            "Logits: tensor([[-1.8109,  2.0331]])\n",
            "Probabilities: tensor([[0.0210, 0.9790]])\n",
            "\n",
            "Text: whatever\n",
            "Logits: tensor([[ 1.8082, -1.6105]])\n",
            "Probabilities: tensor([[0.9683, 0.0317]])\n",
            "\n",
            "Text: love it\n",
            "Logits: tensor([[-4.3101,  4.6796]])\n",
            "Probabilities: tensor([[1.2467e-04, 9.9988e-01]])\n",
            "\n",
            "Text: hate it\n",
            "Logits: tensor([[ 4.5873, -3.6634]])\n",
            "Probabilities: tensor([[9.9974e-01, 2.6101e-04]])\n",
            "\n",
            "Text: this is fine\n",
            "Logits: tensor([[-4.2480,  4.5880]])\n",
            "Probabilities: tensor([[1.4538e-04, 9.9985e-01]])\n",
            "\n",
            "Text: wow, just wow\n",
            "Logits: tensor([[-3.9157,  4.2371]])\n",
            "Probabilities: tensor([[2.8783e-04, 9.9971e-01]])\n",
            "\n",
            "Text: I guess it's okay\n",
            "Logits: tensor([[-3.8674,  4.2151]])\n",
            "Probabilities: tensor([[3.0880e-04, 9.9969e-01]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ HuggingFace Uses PyTorch Under the Hood\n",
        "\n",
        "Hugging Face models are built on top of either:\n",
        "- **PyTorch** (`torch.nn.Module`)\n",
        "- or **TensorFlow** (`tf.keras.Model`)\n",
        "\n",
        "When you ran:\n",
        "```python\n",
        "AutoModelForSequenceClassification.from_pretrained(...)\n",
        "```\n",
        "You were loading the **PyTorch version** of the model by default.\n",
        "\n",
        "This is why your tensors look like this:\n",
        "```python\n",
        "tensor([[-1.8109,  2.0331]])\n",
        "```\n",
        "...and you used:\n",
        "```python\n",
        "torch.nn.functional.softmax(...)\n",
        "```\n",
        "\n",
        "You‚Äôre fully inside the **PyTorch workflow** here.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ You're Bypassing the Pipeline\n",
        "\n",
        "When you use:\n",
        "```python\n",
        "pipeline(\"sentiment-analysis\")\n",
        "```\n",
        "\n",
        "You‚Äôre using a **wrapper** that does all of this:\n",
        "1. Loads the tokenizer\n",
        "2. Loads the model\n",
        "3. Tokenizes your text\n",
        "4. Runs it through the model\n",
        "5. Applies softmax\n",
        "6. Translates scores into human-readable labels (`\"POSITIVE\"`, `\"NEGATIVE\"`)\n",
        "7. Formats the result into a Python-friendly output\n",
        "\n",
        "You just **peeled back that wrapper** and exposed:\n",
        "\n",
        "- The **raw logits** (before softmax)\n",
        "- The **manual softmax application**\n",
        "- The actual **tensor outputs** the model gives you\n",
        "\n",
        "This is **exactly what the pipeline was doing for you**, just hidden behind that 1-liner!\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Is Important\n",
        "\n",
        "- You now know how to **debug**, **customize**, or **interpret** model outputs more deeply.\n",
        "- You‚Äôll be able to **build custom logic**, like multi-label classification, thresholding, or even modifying logits directly.\n",
        "- You‚Äôve taken a huge step toward understanding how to **train or fine-tune your own models** later.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rd7gE_CooiqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ What You Can Do About It\n",
        "\n",
        "- **Use a multi-class sentiment model** with a neutral category  \n",
        "  ‚Üí e.g., `cardiffnlp/twitter-roberta-base-sentiment`  \n",
        "  (has Positive, Neutral, Negative)\n",
        "\n",
        "- **Try zero-shot classification** if you want more nuanced control:\n",
        "```python\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "result = classifier(\n",
        "    \"meh\",\n",
        "    candidate_labels=[\"positive\", \"negative\", \"neutral\", \"sarcastic\"]\n",
        ")\n",
        "print(result)\n",
        "```\n",
        "\n",
        "- **Train your own model** on more subtle or domain-specific sentiment examples."
      ],
      "metadata": {
        "id": "lxEpgw9djWVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! You're running the right structure‚Äîyou‚Äôve got a loop over multiple texts, and you're examining both the logits and probabilities. üéØ\n",
        "\n",
        "Now, let‚Äôs switch that to a **3-class sentiment model** (positive, neutral, negative) so we can capture **more nuance** for ambiguous texts like `\"meh\"` or `\"whatever\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step-by-Step: Use a 3-Class Sentiment Model\n",
        "\n",
        "We'll use the popular:\n",
        "> `cardiffnlp/twitter-roberta-base-sentiment`  \n",
        "A RoBERTa model trained on Twitter data with 3 sentiment labels:\n",
        "- `LABEL_0`: Negative  \n",
        "- `LABEL_1`: Neutral  \n",
        "- `LABEL_2`: Positive\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è This model uses a slightly different tokenizer and label mapping, so let‚Äôs update your code to:\n",
        "\n",
        "‚úÖ Use the correct tokenizer  \n",
        "‚úÖ Apply softmax  \n",
        "‚úÖ Map logits to human-friendly labels\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What to Look For\n",
        "\n",
        "Now, instead of **forcing everything into POSITIVE or NEGATIVE**, you‚Äôll start seeing outputs like:\n",
        "\n",
        "- `\"meh\"` ‚Üí **neutral**\n",
        "- `\"love it\"` ‚Üí **positive**\n",
        "- `\"hate it\"` ‚Üí **negative**\n",
        "- `\"I guess it's okay\"` ‚Üí **neutral or positive (low confidence)**\n",
        "\n",
        "This gives you a **much better understanding of nuanced or lukewarm statements**, which are common in real-world feedback.\n"
      ],
      "metadata": {
        "id": "HbtOV5A8uMst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# 3-class sentiment model from Cardiff NLP\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Label mapping (from model card)\n",
        "labels = ['negative', 'neutral', 'positive']\n",
        "\n",
        "# Example texts to analyze\n",
        "texts = [\"meh\", \"whatever\", \"love it\", \"hate it\", \"this is fine\", \"wow, just wow\", \"I guess it's okay\"]\n",
        "\n",
        "# Inference loop\n",
        "for text in texts:\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    probs_np = probs.numpy()[0]\n",
        "\n",
        "    # Map probabilities to labels\n",
        "    label_probs = {labels[i]: float(probs_np[i]) for i in range(len(labels))}\n",
        "\n",
        "    # Find top prediction\n",
        "    top_label = labels[np.argmax(probs_np)]\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Text: {text}\")\n",
        "    print(\"Probabilities:\", label_probs)\n",
        "    print(\"Predicted Sentiment:\", top_label)\n",
        "    print(\"-\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802,
          "referenced_widgets": [
            "a1a829fdd53b484ba3ebf1cb10245add",
            "4900d9cb631d458480b18fbd1bf1dde2",
            "2d9704d50bb9456bb4c3ddc1da4afb2f",
            "4fe64e25a3e843f19115f257a5542920",
            "6592c42cf8824f1e9f98bb09bc0954f3",
            "699e4de90dcb403a88d17a4e690b45a2",
            "0368c2e3a63d4b6784dd9e8b115623a8",
            "5bfc702375564542a9aa2e0cd4665e21",
            "dd0fa172f9e44e7392558994a7be3638",
            "f204d51cfdc8478c8e52ebb936d9798f",
            "0b29b07ae1194b75aae5f53105dbe521",
            "d74c2e7e086a43a98f486f44bbd4bf7e",
            "14e71e9d8d39463f94a83ac5c5a926e0",
            "6c2ea1fd103b43e3b4a60dd021608e98",
            "caf86bc8aaa546669577b33cdee44dfe",
            "9e312e6339bb4c3aa46794f86eba1923",
            "6d951e3bbdaa40d7b26f758973e256ae",
            "30c832cdaaaa429f9fc3bdfab54519cb",
            "020e0f237964411b8876b1e9d9f728f9",
            "7c555b55df0841e0be1310fd29d91448",
            "93d4dcaf037140d9823e59d8da2bb169",
            "9e0bfddfaf364059a956d6ac6c728663",
            "34ab193d26d34309a20ec1a4855b78de",
            "76b69e72853746dab029ec6ae1e28cb7",
            "78b9ab40d7ef4ebebb30bd5c326e3a43",
            "793b439e5bd940bf8878a86d4426dc2c",
            "a181f8484cb24f00af782b5aa2fd54d6",
            "54aba961c65a41d58ce3de6cf8dae719",
            "6895f989414a47eda5a92ede1aa7d4a2",
            "ee8ebcee4290422098dc9e6672c0cdde",
            "2cb53088ab104bacb1397571ee683259",
            "767dccd825ef488397c9818364f3038b",
            "40508952a1e24d0f9cbf5315b35b4b0d",
            "e80b98fe80be43289039329b1de8842b",
            "e27438c37a814ce795d1004a6bf5e852",
            "8f0744be97e948d38ffccd4aa668e453",
            "2542f19357324f7289cbd3de4a48ed59",
            "9928550b425142c79f266fa1167f4ce8",
            "a1055423cfb441cba8ba6f2fc4b5ea55",
            "d2cbb8e3685043d6ad7eee750cc07af5",
            "56b82315be6a4f4183735e1aaa048291",
            "339729c11cb54e20ada9f4eb9a5608ff",
            "d4a10e311000477393b691b0ca310184",
            "82d381a52e934fa183330faeb30c551b",
            "751e45f48fc341a7a3f452e3c1b5df4d",
            "38f9718b990e421680cae36f9da0f8e2",
            "9b57c8f15312495f808ff285fb492b38",
            "1eb1cc8198274704afa82d673726d858",
            "99bad8cc95a143ca9f14425e3ba5182a",
            "632b8ee05cd74c7680ead02fb72a43f9",
            "5dad5cced1d345cb8f26cd8268828591",
            "5688ba78131a484bba96b2e00f71e3b5",
            "4dce216a2daf4ea3b6db409995b1c091",
            "1b6a755a3f6745748ce633c8b7af4ee0",
            "4daef566e7a4471584881cb32f8fb0be",
            "c11c2df18129499b8137fa729dbddafa",
            "f00a4919a78e448f85f080224840ff3f",
            "7be19e18110c4708bcb737ff80cc097b",
            "24ade1b18ed6467b8c7070b5e57c6979",
            "be578ad4a7804165a086253be208c8db",
            "be9c03ebfae84c1885f88aea46fd9016",
            "7db5f5c3613b4877b4836b93510d8404",
            "0471c75817a84ea28ef5ecb0574090e0",
            "8e66d30a65fe43b0a209889767ec26d3",
            "8b89a8326f7d45129fe26eeff750df58",
            "8fc057a4479e442c81a1d1bb1ad0b33f"
          ]
        },
        "id": "_6yA5jRttuh8",
        "outputId": "c6afdb1f-3b64-4495-bed3-02ec2d513bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/747 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1a829fdd53b484ba3ebf1cb10245add"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d74c2e7e086a43a98f486f44bbd4bf7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34ab193d26d34309a20ec1a4855b78de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e80b98fe80be43289039329b1de8842b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "751e45f48fc341a7a3f452e3c1b5df4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: meh\n",
            "Probabilities: {'negative': 0.23177234828472137, 'neutral': 0.5303865075111389, 'positive': 0.2378411442041397}\n",
            "Predicted Sentiment: neutral\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c11c2df18129499b8137fa729dbddafa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: whatever\n",
            "Probabilities: {'negative': 0.28891637921333313, 'neutral': 0.5565615296363831, 'positive': 0.1545221358537674}\n",
            "Predicted Sentiment: neutral\n",
            "----------------------------------------\n",
            "Text: love it\n",
            "Probabilities: {'negative': 0.024173742160201073, 'neutral': 0.07955925911664963, 'positive': 0.8962669968605042}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: hate it\n",
            "Probabilities: {'negative': 0.8896681666374207, 'neutral': 0.09059063345193863, 'positive': 0.019741209223866463}\n",
            "Predicted Sentiment: negative\n",
            "----------------------------------------\n",
            "Text: this is fine\n",
            "Probabilities: {'negative': 0.01242265198379755, 'neutral': 0.14099746942520142, 'positive': 0.846579909324646}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: wow, just wow\n",
            "Probabilities: {'negative': 0.0195575300604105, 'neutral': 0.17196960747241974, 'positive': 0.8084728717803955}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n",
            "Text: I guess it's okay\n",
            "Probabilities: {'negative': 0.020886369049549103, 'neutral': 0.30712148547172546, 'positive': 0.6719921827316284}\n",
            "Predicted Sentiment: positive\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Your realization is **exactly right**:\n",
        "\n",
        "> üí° *‚ÄúThere is a lot to understand about the model (and pipeline) you choose to match it to the project you are working on.‚Äù*\n",
        "\n",
        "This is **core to working with Hugging Face effectively** ‚Äî the right model + task + dataset = success. Choosing the wrong one? You get poor results, even if the model is state-of-the-art.\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ High-Level Framework for Exploring Hugging Face\n",
        "\n",
        "Here‚Äôs a **structured map** of what‚Äôs available and how to think about it.\n",
        "\n",
        "---\n",
        "\n",
        "### üß± 1. **Pipelines = High-Level Tasks**\n",
        "\n",
        "These are the main categories of work you can do. Each one is tied to a model type behind the scenes.\n",
        "\n",
        "| Pipeline Task               | Description                                           | Example Use Case                              |\n",
        "|----------------------------|-------------------------------------------------------|------------------------------------------------|\n",
        "| `\"sentiment-analysis\"`     | Classify text as positive/negative (or neutral)       | Product reviews, user feedback                |\n",
        "| `\"text-classification\"`    | General category prediction                           | Spam detection, topic tagging                 |\n",
        "| `\"zero-shot-classification\"` | Classify into your own labels (no training required) | Customer intent, content moderation           |\n",
        "| `\"text-generation\"`        | Generate coherent text completions                    | Chatbots, story generation                    |\n",
        "| `\"summarization\"`          | Shorten long text into concise summaries              | News, reports, articles                       |\n",
        "| `\"translation_xx_to_yy\"`   | Translate between languages                           | English ‚Üí French                              |\n",
        "| `\"question-answering\"`     | Answer a question given a passage                     | Customer support, document search             |\n",
        "| `\"fill-mask\"`              | Predict masked word(s)                                | Cloze tasks, language model probing           |\n",
        "| `\"ner\"`                    | Named Entity Recognition (people, places, orgs)       | Info extraction from documents                |\n",
        "| `\"conversational\"`         | Dialogue-style interaction                            | Simple chatbot interaction                    |\n",
        "| `\"image-classification\"`   | Classify images                                       | Object detection, visual tagging              |\n",
        "| `\"audio-classification\"`   | Classify sound files                                  | Music genre, speaker identification           |\n",
        "| `\"automatic-speech-recognition\"` | Convert speech to text                       | Voice transcription, meeting notes            |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 2. **Model Types = Underlying Architectures**\n",
        "\n",
        "The model you choose inside the pipeline matters!\n",
        "\n",
        "| Architecture         | Known For                               | Strengths                                     | Example Models |\n",
        "|----------------------|------------------------------------------|-----------------------------------------------|----------------|\n",
        "| BERT / DistilBERT    | Bidirectional transformers               | Classification, QA, embeddings                | `bert-base-uncased` |\n",
        "| RoBERTa              | Robust BERT variant                      | Sentiment, NER, classification                | `roberta-base`, `cardiffnlp/twitter-roberta-base-sentiment` |\n",
        "| GPT / GPT-2          | Text generation (unidirectional)         | Generating coherent text                     | `gpt2`         |\n",
        "| T5 / BART            | Sequence-to-sequence (text2text) models  | Translation, summarization, QA               | `t5-small`, `facebook/bart-large-cnn` |\n",
        "| XLNet                | General language modeling                | Fill-mask and classification                 | `xlnet-base-cased` |\n",
        "| Whisper              | Audio ‚Üí Text                             | Speech recognition                           | `openai/whisper-base` |\n",
        "| ViT (Vision Transformer) | Image classification                | Computer vision tasks                        | `google/vit-base-patch16-224` |\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ 3. **Datasets Matter Too**\n",
        "\n",
        "You also want a model **trained on data similar to your task**:\n",
        "\n",
        "| Dataset              | Domain                        | Impact on Model Behavior                       |\n",
        "|----------------------|-------------------------------|------------------------------------------------|\n",
        "| SST-2                | Movie reviews (binary)        | Overconfident positive/negative predictions   |\n",
        "| TweetEval            | Tweets (multi-label, sentiment, hate speech) | Real-world language, emojis, slang       |\n",
        "| CNN/DailyMail        | News articles                 | Great for summarization                       |\n",
        "| MultiNLI             | Textual entailment (inference) | Used for zero-shot classification             |\n",
        "| SQuAD                | Question answering             | Factual answer extraction                     |\n",
        "\n",
        "---\n",
        "\n",
        "### üõ† 4. **Use This Checklist to Match Project to Model**\n",
        "\n",
        "| Step | Question                                               | Example                                          |\n",
        "|------|--------------------------------------------------------|--------------------------------------------------|\n",
        "| ‚úÖ 1 | What is the task?                                       | \"I want to summarize support tickets.\"          |\n",
        "| ‚úÖ 2 | Is there a pipeline that matches that task?             | ‚Üí `\"summarization\"`                             |\n",
        "| ‚úÖ 3 | What domain is your data in?                            | Tech support, so look for domain-tuned models   |\n",
        "| ‚úÖ 4 | Do you need binary, multi-class, or zero-shot labels?   | Sentiment with neutral = use a 3-class model    |\n",
        "| ‚úÖ 5 | Does the model size fit your hardware?                  | `distilbert` vs `bert-large`                    |\n",
        "| ‚úÖ 6 | Do you want to fine-tune or use off-the-shelf?          | Off-the-shelf = pipeline                          |\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ Explore Hugging Face with Purpose\n",
        "\n",
        "You can structure your exploration like this:\n",
        "\n",
        "| Goal                          | Pipeline         | Suggested Model Example                             |\n",
        "|-------------------------------|------------------|------------------------------------------------------|\n",
        "| Understand tone               | `sentiment-analysis` | `cardiffnlp/twitter-roberta-base-sentiment`      |\n",
        "| Build a chatbot               | `text-generation`, `conversational` | `gpt2`, `DialoGPT`          |\n",
        "| Classify open-ended input     | `zero-shot-classification` | `facebook/bart-large-mnli`             |\n",
        "| Translate content             | `translation_en_to_fr` | `Helsinki-NLP/opus-mt-en-fr`           |\n",
        "| Extract key info              | `ner`             | `dbmdz/bert-large-cased-conll03-english`            |\n",
        "| Build a summarizer            | `summarization`   | `facebook/bart-large-cnn`, `t5-small`               |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6ss8QdZEvpuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yduXVfgbtuel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOFipTvEtuZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JTSoeJlrtuWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Remove Widgets from Notebook to save to Github"
      ],
      "metadata": {
        "id": "iSLP6mQ6nVJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to your current notebook file (adjust if different)\n",
        "notebook_path = \"/content/drive/My Drive/LLM/LLM_048_huggingFace_SentimentAnalysis.ipynb\"\n",
        "\n",
        "\n",
        "# Load the notebook JSON\n",
        "with open(notebook_path, 'r', encoding='utf-8') as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove the widget metadata if it exists\n",
        "if 'widgets' in nb.get('metadata', {}):\n",
        "    del nb['metadata']['widgets']\n",
        "\n",
        "# Save the cleaned notebook\n",
        "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook metadata cleaned. Try saving to GitHub again.\")\n"
      ],
      "metadata": {
        "id": "UILjP50ojZBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f881cf-872f-4f65-d61c-6f738b0a4029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Notebook metadata cleaned. Try saving to GitHub again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V7XcFvhMlzQq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}